{"posts":[{"title":"MySql学习之路(十九)：数据库备份与恢复","content":"在任何数据库环境中，总会有不确定的意外情况发生，比如例外的停电、计算机系统中的各种软硬件故障、人为破坏、管理员误操作等是不可避免的，这些情况可能会导致数据的丢失、服务器瘫痪等严重的后果。存在多个服务器时，会出现主从服务器之间的数据同步问题。 为了有效防止数据丢失，并将损失降到最低，应定期对MySQL数据库服务器做备份。如果数据库中的数据丢失或者出现错误，可以使用备份的数据进行恢复。主从服务器之间的数据同步问题可以通过复制功能实现。 目录 一、物理备份与逻辑备份 二、mysqldump实现逻辑备份 三、mysql命令恢复数据 四、物理备份：直接复制整个数据库 五、物理恢复：直接复制到数据库目录 六、表的导出与导入 七、数据库迁移 八、删库跑路？ 一、物理备份与逻辑备份 物理备份：备份数据文件，转储数据库物理文件到某一目录。物理备份恢复速度比较快，但占用空间比较大，MySQL中可以用 xtrabackup 工具来进行物理备份。 逻辑备份：对数据库对象利用工具进行导出工作，汇总入备份文件内。逻辑备份恢复速度慢，但占用空间小，更灵活。MySQL 中常用的逻辑备份工具为 mysqldump 。逻辑备份就是 备份sql语句 ，在恢复的 时候执行备份的sql语句实现数据库数据的重现。 二、mysqldump实现逻辑备份 mysqldump是MySQL提供的一个非常有用的数据库备份工具。 ## 备份一个数据库 mysqldump –u 用户名称 –h 主机名称 –p密码 待备份的数据库名称[tbname, [tbname...]]&gt; 备份文件名称.sql mysqldump -uroot -p atguigudb1 &gt; /var/lib/mysql/atguigu.sql ## 备份全部数据库 mysqldump -uroot -pxxxxxx --all-databases &gt; all_database.sql mysqldump -uroot -pxxxxxx -A &gt; all_database.sql ## 备份部分数据库 mysqldump –u user –h host –p --databases [数据库的名称1 [数据库的名称2...]] &gt; 备份文件名称.sql mysqldump -uroot -p --databases atguigu atguigu12 &gt;two_database.sql ## 备份部分表 mysqldump –u user –h host –p 数据库的名称 [表名1 [表名2...]] &gt; 备份文件名称.sql mysqldump -uroot -p atguigu book account &gt; book.sql ## 备份单表的部分数据（有些时候一张表的数据量很大，我们只需要部分数据。这时就可以使用 --where 选项了） mysqldump -uroot -p atguigu student --where=&quot;id &lt; 10 &quot; &gt; student_part_id10_low_bak.sql ## 排除某些表的备份（如果我们想备份某个库，但是某些表数据量很大或者与业务关联不大，这个时候可以考虑排除掉这些表，同样的，选项 --ignore-table 可以完成这个功能） mysqldump -uroot -p atguigu --ignore-table=atguigu.student &gt; no_stu_bak.sql ## 只备份结构 mysqldump -uroot -p atguigu --no-data &gt; atguigu_no_data_bak.sql ## 只备份数据 mysqldump -uroot -p atguigu --no-create-info &gt; atguigu_no_create_info_bak.sql ## 备份中包含存储过程、函数、事件 ## mysqldump备份默认是不包含存储过程，自定义函数及事件的。可以使用 `--routines` 或 `-R` 选项来备份存储过程及函数，使用 `--events` 或 `-E` 参数来备份事件。 mysqldump -uroot -p -R -E --databases atguigu &gt; fun_atguigu_bak.sql mysqldump其他常用选项如下： --add-drop-database：在每个CREATE DATABASE语句前添加DROP DATABASE语句。 --add-drop-tables：在每个CREATE TABLE语句前添加DROP TABLE语句。 --add-locking：用LOCK TABLES和UNLOCK TABLES语句引用每个表转储。重载转储文件时插入得更快。 --all-database, -A：转储所有数据库中的所有表。与使用--database选项相同，在命令行中命名所有数据库。 --comment[=0|1]：如果设置为0，禁止转储文件中的其他信息，例如程序版本、服务器版本和主机。--skip\u0002comments与--comments=0的结果相同。默认值为1，即包括额外信息。 --compact：产生少量输出。该选项禁用注释并启用--skip-add-drop-tables、--no-set-names、--skip\u0002disable-keys和--skip-add-locking选项。 --compatible=name：产生与其他数据库系统或旧的MySQL服务器更兼容的输出，值可以为ansi、MySQL323、MySQL40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_table_options或者no_field_options。 --complete_insert, -c：使用包括列名的完整的INSERT语句。 --debug[=debug_options], -#[debug_options]：写调试日志。 --delete，-D：导入文本文件前清空表。 --default-character-set=charset：使用charsets默认字符集。如果没有指定，就使用utf8。 --delete--master-logs：在主复制服务器上，完成转储操作后删除二进制日志。该选项自动启用-master\u0002data。 --extended-insert，-e：使用包括几个VALUES列表的多行INSERT语法。这样使得转储文件更小，重载文件时可以加速插入。 --flush-logs，-F：开始转储前刷新MySQL服务器日志文件。该选项要求RELOAD权限。 --force，-f：在表转储过程中，即使出现SQL错误也继续。 --lock-all-tables，-x：对所有数据库中的所有表加锁。在整体转储过程中通过全局锁定来实现。该选项自动关闭--single-transaction和--lock-tables。 --lock-tables，-l：开始转储前锁定所有表。用READ LOCAL锁定表以允许并行插入MyISAM表。对于事务表（例如InnoDB和BDB），--single-transaction是一个更好的选项，因为它根本不需要锁定表。 --no-create-db，-n：该选项禁用CREATE DATABASE /*!32312 IF NOT EXIST*/db_name语句，如果给出--database或--all-database选项，就包含到输出中。 --no-create-info，-t：只导出数据，而不添加CREATE TABLE语句。 --no-data，-d：不写表的任何行信息，只转储表的结构。 --opt：该选项是速记，它可以快速进行转储操作并产生一个能很快装入MySQL服务器的转储文件。该选项默认开启，但可以用--skip-opt禁用。 --password[=password]，-p[password]：当连接服务器时使用的密码。 -port=port_num，-P port_num：用于连接的TCP/IP端口号。 --protocol={TCP|SOCKET|PIPE|MEMORY}：使用的连接协议。 --replace，-r –replace和--ignore：控制替换或复制唯一键值已有记录的输入记录的处理。如果指定--replace，新行替换有相同的唯一键值的已有行；如果指定--ignore，复制已有的唯一键值的输入行被跳过。如果不指定这两个选项，当发现一个复制键值时会出现一个错误，并且忽视文本文件的剩余部分。 --silent，-s：沉默模式。只有出现错误时才输出。 --socket=path，-S path：当连接localhost时使用的套接字文件（为默认主机）。 --user=user_name，-u user_name：当连接服务器时MySQL使用的用户名。 --verbose，-v：冗长模式，打印出程序操作的详细信息。 --xml，-X：产生XML输出。 三、mysql命令恢复数据 使用mysqldump命令将数据库中的数据备份成一个文本文件。需要恢复时，可以使用mysql命令来恢复备份的数据。 mysql命令可以执行备份文件中的CREATE语句和INSERT语句。通过CREATE语句来创建数据库和表。通过INSERT语句来插入备份的数据。 基本语法： mysql –u root –p [dbname] &lt; backup.sql 其中，dbname参数表示数据库名称。该参数是可选参数，可以指定数据库名，也可以不指定。指定数据库名时，表示还原该数据库下的表。此时需要确保MySQL服务器中已经创建了该名的数据库。不指定数据库名，表示还原文件中所有的数据库。此时sql文件中包含有CREATE DATABASE语句，不需要MySQL服务器中已存在的这些数据库。 四、物理备份：直接复制整个数据库 直接将MySQL中的数据库文件复制出来。这种方法最简单，速度也最快。MySQL的数据库目录位置不一 定相同： 在Windows平台下，MySQL 8.0存放数据库的目录通常默认为 “ C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data ”或者其他用户自定义目录； 在Linux平台下，数据库目录位置通常为/var/lib/mysql/； 在MAC OSX平台下，数据库目录位置通常为“/usr/local/mysql/data” 但为了保证备份的一致性。需要保证： 方式1：备份前，将服务器停止。 方式2：备份前，对相关表执行 FLUSH TABLES WITH READ LOCK 操作。这样当复制数据库目录中 的文件时，允许其他客户继续查询表。同时，FLUSH TABLES语句来确保开始备份前将所有激活的索 引页写入硬盘。 这种方式方便、快速，但不是最好的备份方法，因为实际情况可能 不允许停止MySQL服务器 或者 锁住表 ，而且这种方法 对InnoDB存储引擎 的表不适用。对于MyISAM存储引擎的表，这样备份和还原很方便，但是还原时最好是相同版本的MySQL数据库，否则可能会存在文件类型不同的情况。 注意，物理备份完毕后，执行 UNLOCK TABLES 来结算其他客户对表的修改行为。 说明： 在MySQL版本号中，第一个数字表示主版本号，主版本号相同的MySQL数据库文件格式相同。 此外，还可以考虑使用相关工具实现备份。比如， MySQLhotcopy 工具。MySQLhotcopy是一个Perl脚本，它使用LOCK TABLES、FLUSH TABLES和cp或scp来快速备份数据库。它是备份数据库或单个表最快的途径，但它只能运行在数据库目录所在的机器上，并且只能备份MyISAM类型的表。多用于mysql5.5之前。 五、物理恢复：直接复制到数据库目录 步骤：** 1）演示删除备份的数据库中指定表的数据 2）将备份的数据库数据拷贝到数据目录下，并重启MySQL服务器 3）查询相关表的数据是否恢复。需要使用下面的 chown 操作。 要求： 必须确保备份数据的数据库和待恢复的数据库服务器的主版本号相同。 因为只有MySQL数据库主版本号相同时，才能保证这两个MySQL数据库文件类型是相同的。 这种方式对 MyISAM类型的表比较有效 ，对于InnoDB类型的表则不可用。 因为InnoDB表的表空间不能直接复制。 在Linux操作系统下，复制到数据库目录后，一定要将数据库的用户和组变成mysql，命令如下： chown -R mysql.mysql /var/lib/mysql/dbname 其中，两个mysql分别表示组和用户；“-R”参数可以改变文件夹下的所有子文件的用户和组；“dbname”参数表示数据库目录。 提示 Linux操作系统下的权限设置非常严格。通常情况下，MySQL数据库只有root用户和mysql用户 组下的mysql用户才可以访问，因此将数据库目录复制到指定文件夹后，一定要使用chown命令将 文件夹的用户组变为mysql，将用户变为mysql。 六、表的导出与导入 6.1 表的导出 1. 使用SELECT…INTO OUTFILE导出文本文件 在MySQL中，可以使用SELECT…INTO OUTFILE语句将表的内容导出成一个文本文件。 **举例：**使用SELECT…INTO OUTFILE将atguigu数据库中account表中的记录导出到文本文件。 （1）选择数据库atguigu，并查询account表，执行结果如下所示。 use atguigu; select * from account; mysql&gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.01 sec) （2）mysql默认对导出的目录有权限限制，也就是说使用命令行进行导出的时候，需要指定目录进行操作。 查询secure_file_priv值： mysql&gt; SHOW GLOBAL VARIABLES LIKE '%secure%'; +--------------------------+-----------------------+ | Variable_name | Value | +--------------------------+-----------------------+ | require_secure_transport | OFF | | secure_file_priv | /var/lib/mysql-files/ | +--------------------------+-----------------------+ 2 rows in set (0.02 sec) （3）上面结果中显示，secure_file_priv变量的值为/var/lib/mysql-files/，导出目录设置为该目录，SQL语句如下。 SELECT * FROM account INTO OUTFILE &quot;/var/lib/mysql-files/account.txt&quot;; （4）查看 /var/lib/mysql-files/account.txt`文件。 1 张三 90 2 李四 100 3 王五 0 2. 使用mysqldump命令导出文本文件 **举例1：**使用mysqldump命令将将atguigu数据库中account表中的记录导出到文本文件： mysqldump -uroot -p -T &quot;/var/lib/mysql-files/&quot; atguigu account mysqldump命令执行完毕后，在指定的目录/var/lib/mysql-files/下生成了account.sql和account.txt文件。 打开account.sql文件，其内容包含创建account表的CREATE语句。 [root@node1 mysql-files]# cat account.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `account` -- DROP TABLE IF EXISTS `account`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `account` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `balance` int NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb3; /*!40101 SET character_set_client = @saved_cs_client */; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2022-01-07 23:19:27 打开account.txt文件，其内容只包含account表中的数据。 [root@node1 mysql-files]# cat account.txt 1 张三 90 2 李四 100 3 王五 0 **举例2：**使用mysqldump将atguigu数据库中的account表导出到文本文件，使用FIELDS选项，要求字段之 间使用逗号“，”间隔，所有字符类型字段值用双引号括起来： mysqldump -uroot -p -T &quot;/var/lib/mysql-files/&quot; atguigu account --fields-terminated\u0002by=',' --fields-optionally-enclosed-by='\\&quot;' 语句mysqldump语句执行成功之后，指定目录下会出现两个文件account.sql和account.txt。 打开account.sql文件，其内容包含创建account表的CREATE语句。 [root@node1 mysql-files]# cat account.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `account` -- DROP TABLE IF EXISTS `account`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `account` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `balance` int NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb3; /*!40101 SET character_set_client = @saved_cs_client */; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2022-01-07 23:36:39 打开account.txt文件，其内容包含创建account表的数据。从文件中可以看出，字段之间用逗号隔开，字 符类型的值被双引号括起来。 [root@node1 mysql-files]# cat account.txt 1,&quot;张三&quot;,90 2,&quot;李四&quot;,100 3,&quot;王五&quot;,0 3. 使用mysql命令导出文本文件 **举例1：**使用mysql语句导出atguigu数据中account表中的记录到文本文件： mysql -uroot -p --execute=&quot;SELECT * FROM account;&quot; atguigu&gt; &quot;/var/lib/mysql-files/account.txt&quot; 打开account.txt文件，其内容包含创建account表的数据。 [root@node1 mysql-files]# cat account.txt id name balance 1 张三 90 2 李四 100 3 王五 0 **举例2：**将atguigu数据库account表中的记录导出到文本文件，使用--veritcal参数将该条件记录分为多行显示： mysql -uroot -p --vertical --execute=&quot;SELECT * FROM account;&quot; atguigu &gt; &quot;/var/lib/mysql-files/account_1.txt&quot; 打开account_1.txt文件，其内容包含创建account表的数据。 [root@node1 mysql-files]# cat account_1.txt *************************** 1. row *************************** id: 1 name: 张三 balance: 90 *************************** 2. row *************************** id: 2 name: 李四 balance: 100 *************************** 3. row *************************** id: 3 name: 王五 balance: 0 **举例3：**将atguigu数据库account表中的记录导出到xml文件，使用--xml参数，具体语句如下。 mysql -uroot -p --xml --execute=&quot;SELECT * FROM account;&quot; atguigu&gt;&quot;/var/lib/mysql\u0002files/account_3.xml&quot; [root@node1 mysql-files]# cat account_3.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;resultset statement=&quot;SELECT * FROM account&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt; &lt;row&gt; &lt;field name=&quot;id&quot;&gt;1&lt;/field&gt; &lt;field name=&quot;name&quot;&gt;张三&lt;/field&gt; &lt;field name=&quot;balance&quot;&gt;90&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=&quot;id&quot;&gt;2&lt;/field&gt; &lt;field name=&quot;name&quot;&gt;李四&lt;/field&gt; &lt;field name=&quot;balance&quot;&gt;100&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=&quot;id&quot;&gt;3&lt;/field&gt; &lt;field name=&quot;name&quot;&gt;王五&lt;/field&gt; &lt;field name=&quot;balance&quot;&gt;0&lt;/field&gt; &lt;/row&gt; &lt;/resultset&gt; 说明：如果要将表数据导出到html文件中，可以使用 --html 选项。然后可以使用浏览器打开。 6.2 表的导入 1. 使用LOAD DATA INFILE方式导入文本文件 举例1： 使用SELECT...INTO OUTFILE将atguigu数据库中account表的记录导出到文本文件 SELECT * FROM atguigu.account INTO OUTFILE '/var/lib/mysql-files/account_0.txt'; 删除account表中的数据： DELETE FROM atguigu.account; 从文本文件account.txt中恢复数据： LOAD DATA INFILE '/var/lib/mysql-files/account_0.txt' INTO TABLE atguigu.account; 查询account表中的数据： mysql&gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 举例2： 选择数据库atguigu，使用SELECT…INTO OUTFILE将atguigu数据库account表中的记录导出到文本文件，使用FIELDS选项和LINES选项，要求字段之间使用逗号&quot;，&quot;间隔，所有字段值用双引号括起来： SELECT * FROM atguigu.account INTO OUTFILE '/var/lib/mysql-files/account_1.txt' FIELDS TERMINATED BY ',' ENCLOSED BY '\\&quot;'; 删除account表中的数据： DELETE FROM atguigu.account; 从/var/lib/mysql-files/account.txt中导入数据到account表中： LOAD DATA INFILE '/var/lib/mysql-files/account_1.txt' INTO TABLE atguigu.account FIELDS TERMINATED BY ',' ENCLOSED BY '\\&quot;'; 查询account表中的数据，具体SQL如下： select * from account; mysql&gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 2. 使用mysqlimport方式导入文本文件 举例： 导出文件account.txt，字段之间使用逗号&quot;，&quot;间隔，字段值用双引号括起来： SELECT * FROM atguigu.account INTO OUTFILE '/var/lib/mysql-files/account.txt' FIELDS TERMINATED BY ',' ENCLOSED BY '\\&quot;'; 删除account表中的数据： DELETE FROM atguigu.account; 使用mysqlimport命令将account.txt文件内容导入到数据库atguigu的account表中： mysqlimport -uroot -p atguigu '/var/lib/mysql-files/account.txt' --fields-terminated-by=',' --fields-optionally-enclosed-by='\\&quot;' 查询account表中的数据： select * from account; mysql&gt; select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 七、数据库迁移 ## 7.1 概述 数据迁移（data migration）是指选择、准备、提取和转换数据，并**将数据从一个计算机存储系统永久地传输到另一个计算机存储系统的过程**。此外，` 验证迁移数据的完整性` 和 `退役原来旧的数据存储` ，也被认为是整个数据迁移过程的一部分。 数据库迁移的原因是多样的，包括服务器或存储设备更换、维护或升级，应用程序迁移，网站集成，灾难恢复和数据中心迁移。 根据不同的需求可能要采取不同的迁移方案，但总体来讲，MySQL 数据迁移方案大致可以分为物理迁移和 逻辑迁移 两类。通常以尽可能 自动化 的方式执行，从而将人力资源从繁琐的任务中解放出来。 7.2 迁移方案 物理迁移 物理迁移适用于大数据量下的整体迁移。使用物理迁移方案的优点是比较快速，但需要停机迁移并且要 求 MySQL 版本及配置必须和原服务器相同，也可能引起未知问题。 物理迁移包括拷贝数据文件和使用 XtraBackup 备份工具两种。 不同服务器之间可以采用物理迁移，我们可以在新的服务器上安装好同版本的数据库软件，创建好相同目录，建议配置文件也要和原数据库相同，然后从原数据库方拷贝来数据文件及日志文件，配置好文件组权限，之后在新服务器这边使用 mysqld 命令启动数据库。 逻辑迁移 逻辑迁移适用范围更广，无论是 部分迁移 还是 全量迁移 ，都可以使用逻辑迁移。逻辑迁移中使用最多的就是通过 mysqldump 等备份工具。 7.3 迁移注意点 1. 相同版本的数据库之间迁移注意点 指的是在主版本号相同的MySQL数据库之间进行数据库移动。 方式1： 因为迁移前后MySQL数据库的 主版本号相同 ，所以可以通过复制数据库目录来实现数据库迁移，但是物理迁移方式只适用于MyISAM引擎的表。对于InnoDB表，不能用直接复制文件的方式备份数据库。 方式2： 最常见和最安全的方式是使用 mysqldump命令 导出数据，然后在目标数据库服务器中使用 MySQL命令导入。 举例： #host1的机器中备份所有数据库,并将数据库迁移到名为host2的机器上 mysqldump –h host1 –uroot –p –-all-databases| mysql –h host2 –uroot –p 在上述语句中，“|”符号表示管道，其作用是将mysqldump备份的文件给mysql命令；“--all-databases”表示要迁移所有的数据库。通过这种方式可以直接实现迁移。 2. 不同版本的数据库之间迁移注意点 例如，原来很多服务器使用5.7版本的MySQL数据库，在8.0版本推出来以后，改进了5.7版本的很多缺陷， 因此需要把数据库升级到8.0版本 旧版本与新版本的MySQL可能使用不同的默认字符集，例如有的旧版本中使用latin1作为默认字符集，而最新版本的MySQL默认字符集为utf8mb4。如果数据库中有中文数据，那么迁移过程中需要对 默认字符集 进行修改 ，不然可能无法正常显示数据。 高版本的MySQL数据库通常都会 兼容低版本 ，因此可以从低版本的MySQL数据库迁移到高版本的MySQL 数据库。 3. 不同数据库之间迁移注意点 不同数据库之间迁移是指从其他类型的数据库迁移到MySQL数据库，或者从MySQL数据库迁移到其他类 型的数据库。这种迁移没有普适的解决方法。 迁移之前，需要了解不同数据库的架构， 比较它们之间的差异 。不同数据库中定义相同类型的数据的 关键字可能会不同 。例如，MySQL中日期字段分为DATE和TIME两种，而ORACLE日期字段只有DATE；SQL Server数据库中有ntext、Image等数据类型，MySQL数据库没有这些数据类型；MySQL支持的ENUM和SET 类型，这些SQL Server数据库不支持。 另外，数据库厂商并没有完全按照SQL标准来设计数据库系统，导致不同的数据库系统的 SQL语句 有差别。例如，微软的SQL Server软件使用的是T-SQL语句，T-SQL中包含了非标准的SQL语句，不能和MySQL的SQL语句兼容。 不同类型数据库之间的差异造成了互相 迁移的困难 ，这些差异其实是商业公司故意造成的技术壁垒。但 是不同类型的数据库之间的迁移并 不是完全不可能 。例如，可以使用 MyODBC 实现MySQL和SQL Server之 间的迁移。MySQL官方提供的工具 MySQL Migration Toolkit 也可以在不同数据之间进行数据迁移。 MySQL迁移到Oracle时，需要使用mysqldump命令导出sql文件，然后， 手动更改 sql文件中的CREATE语句。 7.4 迁移小结 八、删库跑路？ 8.1 delete：误删行 8.2 truncate/drop ：误删库/表 8.3 预防使用truncate/drop误删库/表 ","link":"http://mofish.pily.life/post/mysql_learning_19/"},{"title":"MySql学习之路(十八)：数据库日志","content":"🤤很多看似奇怪的问题，答案往往就藏在日志里。很多情况下，只有通过查看日志才 能发现问题的原因，真正解决问题。所以，一定要学会查看日志，养成检查日志的习惯，对提升你的数 据库应用开发能力至关重要。 目录 一、MySQL支持的日志 1.1 日志类型 1.2 日志的弊端 二、慢查询日志(slow query log) 三、通用查询日志(general query log) 3.1 查看当前状态 3.2 启动日志 3.3 查看日志 3.4 删除/刷新日志 四、错误日志(error log) 4.1 启动日志 4.2 查看日志 4.3 删除/刷新日志 4.4 MySql 8.0 新特性 五、二进制日志(bin log) 5.1 binlog主要应用场景 5.2 查看默认情况 5.3 日志参数设置 5.4 查看日志 5.5 使用日志恢复数据 5.6 删除二进制日志 5.7 其它场景 六、再谈二进制日志(binlog) 6.1 写入机制 6.2 binlog与redolog对比 6.3 两阶段提交 七、中继日志(relay log) 7.1 介绍 7.2 查看中继日志 7.3 恢复的典型错误 一、MySQL支持的日志 1.1 日志类型 MySQL有不同类型的日志文件，用来存储不同类型的日志，分为二进制日志 、错误日志 、通用查询日志 、 慢查询日志 、中继日志和数据定义语句日志 。使用这些日志文件，可以查看MySQL内部发生的事情。 二进制日志 记录所有更改数据的语句，可以用于主从服务器之间的数据同步，以及服务器遇到故 障时数据的无损失恢复。 错误日志 记录MySQL服务的启动、运行或停止MySQL服务时出现的问题，方便我们了解服务器的 状态，从而对服务器进行维护。 通用查询日志 记录所有连接的起始时间和终止时间，以及连接发送给数据库服务器的所有指令， 对我们复原操作的实际场景、发现问题，甚至是对数据库操作的审计都有很大的帮助。 慢查询日志 记录所有执行时间超过long_query_time的所有查询，方便我们对查询进行优化。 中续日志 用于主从服务器架构中，从服务器用来存放主服务器二进制日志内容的一个中间文件。 从服务器通过读取中继日志的内容，来同步主服务器上的操作。 数据定义语句日志 录数据定义语句执行的元数据操作。 除二进制日志外，其他日志都是 文本文件 。默认情况下，所有日志创建于 MySQL数据目录 中。 1.2 日志的弊端 日志功能会 降低MySQL数据库的性能 。例如，在查询非常频繁的MySQL数据库系统中，如果开启了通用查询日志和慢查询日志，MySQL数据库会花费很多时间记录日志。 日志会 占用大量的磁盘空间 。对于用户量非常大，操作非常频繁的数据库，日志文件需要的存储空间设置比数据库文件需要的存储空间还要大。 二、慢查询日志(slow query log) 前面章节《MySql学习之路(十五)：慢查询日志》有记录过。 三、通用查询日志(general query log) 通用查询日志用来 记录用户的所有操作 ，包括启动和关闭MySQL服务、所有用户的连接开始时间和截止 时间、发给 MySQL 数据库服务器的所有 SQL 指令等。当我们的数据发生异常时，查看通用查询日志， 还原操作时的具体场景，可以帮助我们准确定位问题。 3.1 查看当前状态 mysql&gt; SHOW VARIABLES LIKE '%general%'; +------------------+------------------------------+ | Variable_name | Value | +------------------+------------------------------+ | general_log | OFF | #通用查询日志处于关闭状态 | general_log_file | /var/lib/mysql/atguigu01.log | #通用查询日志文件的名称是atguigu01.log +------------------+------------------------------+ 2 rows in set (0.03 sec) 系统变量general_log默认是关闭的，因为一旦开启记录通用查询日志，MySql会记录所有的连接起止和相关的SQL操作，极大的增加了资源的消耗和磁盘的空间，因此一般只要需要的时候再开启。 3.2 启动日志 方式1：永久性方式 修改my.cnf或者my.ini配置文件来设置。在[mysqld]组下加入log选项，并重启MySQL服务。格式如下： [mysqld] general_log=ON general_log_file=[path[filename]] #日志文件所在目录路径，filename为日志文件 如果不指定目录和文件名，通用查询日志将默认存储在MySQL数据目录中的hostname.log文件中， hostname表示主机名。 方式2：临时性方式 SET GLOBAL general_log=on; # 开启通用查询日志 SET GLOBAL general_log_file=’path/filename’; # 设置日志文件保存位置 对应的，关闭操作SQL命令如下： SET GLOBAL general_log=off; # 关闭通用查询日志 查看设置后情况： SHOW VARIABLES LIKE 'general_log%'; 3.3 查看日志 通用查询日志是以 文本文件 的形式存储在文件系统中的，可以使用 文本编辑器 直接打开日志文件。每台 MySQL服务器的通用查询日志内容是不同的。 在Windows操作系统中，使用文本文件查看器； 在Linux系统中，可以使用vi工具或者gedit工具查看； 在Mac OSX系统中，可以使用文本文件查看器或者vi等工具查看。 从 SHOW VARIABLES LIKE 'general_log%'; 结果中可以看到通用查询日志的位置。 /usr/sbin/mysqld, Version: 8.0.26 (MySQL Community Server - GPL). started with: Tcp port: 3306 Unix socket: /var/lib/mysql/mysql.sock Time Id Command Argument 2022-01-04T07:44:58.052890Z 10 Query SHOW VARIABLES LIKE '%general%' 2022-01-04T07:45:15.666672Z 10 Query SHOW VARIABLES LIKE 'general_log%' 2022-01-04T07:45:28.970765Z 10 Query select * from student 2022-01-04T07:47:38.706804Z 11 Connect root@localhost on using Socket 2022-01-04T07:47:38.707435Z 11 Query select @@version_comment limit 1 2022-01-04T07:48:21.384886Z 12 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:21.385253Z 12 Query SET NAMES utf8 2022-01-04T07:48:21.385640Z 12 Query USE `atguigu12` 2022-01-04T07:48:21.386179Z 12 Query SHOW FULL TABLES WHERE Table_Type != 'VIEW' 2022-01-04T07:48:23.901778Z 13 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:23.902128Z 13 Query SET NAMES utf8 2022-01-04T07:48:23.905179Z 13 Query USE `atguigu` 2022-01-04T07:48:23.905825Z 13 Query SHOW FULL TABLES WHERE Table_Type != 'VIEW' 2022-01-04T07:48:32.163833Z 14 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:32.164451Z 14 Query SET NAMES utf8 2022-01-04T07:48:32.164840Z 14 Query USE `atguigu` 2022-01-04T07:48:40.006687Z 14 Query select * from account 在通用查询日志里面，我们可以清楚地看到，什么时候开启了新的客户端登陆数据库，登录之后做了什么 SQL 操作，针对的是哪个数据表等信息。 3.4 删除/刷新日志 如果数据的使用非常频繁，那么通用查询日志会占用服务器非常大的磁盘空间。数据管理员可以删除很长时间之前的查询日志，以保证MySQL服务器上的硬盘空间。 手动删除文件 SHOW VARIABLES LIKE 'general_log%'; 可以看出，通用查询日志的目录默认为MySQL数据目录。在该目录下手动删除通用查询日志 atguigu01.log 使用如下命令重新生成查询日志文件，具体命令如下。刷新MySQL数据目录，发现创建了新的日志文 件。前提一定要开启通用日志。 mysqladmin -uroot -p flush-logs 如果希望备份旧的通用查询日志，就必须先将旧的日志文件复制出来或者改名，然后执行上面的mysqladmin命令。正确流程如下： cd mysql-data-directory # 输入自己的通用日志文件所在目录 mv mysql.general.log mysql.general.log.old # 指定旧的文件名 以及 新的文件名 mysqladmin -uroot -p flush-logs 四、错误日志(error log) 错误日志记录了MySql服务器启动、停止运行的时间，以及系统启动、运行和停止过程中的诊断信息，包括错误、警告和提示等。 通过错误日志可以查看系统的运行状态，便于及时发现故障、修复故障。如果MySql服务出现异常，错误日志是发现问题、解决故障的首选。 4.1 启动日志 在MySQL数据库中，错误日志功能是 默认开启 的。而且，错误日志 无法被禁止 。 默认情况下，错误日志存储在MySQL数据库的数据文件夹下，名称默认为 mysqld.log （Linux系统）或 hostname.err （mac系统）。如果需要制定文件名，则需要在my.cnf或者my.ini中做如下配置： [mysqld] log-error=[path/[filename]] #path为日志文件所在的目录路径，filename为日志文件名 修改配置项后，需要重启MySQL服务以生效。 4.2 查看日志 MySQL错误日志是以文本文件形式存储的，可以使用文本编辑器直接查看。 查询错误日志的存储路径： mysql&gt; SHOW VARIABLES LIKE 'log_err%'; +----------------------------+----------------------------------------+ | Variable_name | Value | +----------------------------+----------------------------------------+ | log_error | /alidata/server/mysql/log/mysqld.log | | log_error_verbosity | 3 | +----------------------------+----------------------------------------+ 2 rows in set (0.01 sec) 执行结果中可以看到错误日志文件是mysqld.log，位于MySQL默认的数据目录下。 2022-04-08T02:37:16.274693Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2022-04-08T02:37:16.274807Z 0 [Note] --secure-file-priv is set to NULL. Operations related to importing and exporting data are disabled 2022-04-08T02:37:16.274857Z 0 [Note] /alidata/server/mysql/bin/mysqld (mysqld 5.7.31) starting as process 8989 ... 2022-04-08T02:37:16.290510Z 0 [Note] InnoDB: PUNCH HOLE support available 2022-04-08T02:37:16.290544Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins 2022-04-08T02:37:16.290549Z 0 [Note] InnoDB: Uses event mutexes 2022-04-08T02:37:16.290553Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier 2022-04-08T02:37:16.290557Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.11 2022-04-08T02:37:16.291790Z 0 [Note] InnoDB: Number of pools: 1 2022-04-08T02:37:16.291914Z 0 [Note] InnoDB: Using CPU crc32 instructions 2022-04-08T02:37:16.293477Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M 2022-04-08T02:37:16.301686Z 0 [Note] InnoDB: Completed initialization of buffer pool 2022-04-08T02:37:16.304838Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority(). 2022-04-08T02:37:16.334902Z 0 [Note] InnoDB: Highest supported file format is Barracuda. 2022-04-08T02:37:16.567988Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables 我们在做初始化时候生成的数据库初始密码也是会记录在error.log中 4.3 删除/刷新日志 对于很久以前的错误日志，数据库管理员查看这些错误日志的可能性不大，可以将这些错误日志删除， 以保证MySQL服务器上的 硬盘空间 。MySQL的错误日志是以文本文件的形式存储在文件系统中的，可以 直接删除 。 第一步（方式1）：删除操作 rm -f /var/lib/mysql/mysqld.log 在运行状态下删除错误日志文件后，MySQL并不会自动创建日志文件。 第一步（方式2）：重命名文件 mv /var/log/mysqld.log /var/log/mysqld.log.old 第二步：重建日志 mysqladmin -uroot -p flush-logs 4.4 MySql 8.0 新特性 小结： 通常情况下，管理员不需要查看错误日志。但是，MySQL服务器发生异常时，管理员可以从错误日志中找到发生异常的时间、原因，然后根据这些信息来解决异常。 五、二进制日志(bin log) binlog可以说是MySQL中比较 重要 的日志了，在日常开发及运维过程中，经常会遇到。 binlog即binary log，二进制日志文件，也叫作变更日志（update log）。它记录了数据库所有执行的 DDL 和 DML 等数据库更新事件的语句，但是不包含没有修改任何数据的语句（如数据查询语句select、 show等）。 它以事件形式记录并保存在二进制文件中。通过这些信息，我们可以再现数据更新操作的全过程。 如果想要记录所有语句（例如，为了识别有问题的查询），需要使用通用查询日志。 binlog格式有如下3种： Statement 每一条会修改数据的sql都会记录在binlog中。 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。 Row 5.1.5版本的MySQL才开始支持row level 的复制，它不记录sql语句上下文相关信息，仅保存哪条记录被修改。 优点：row level 的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下 的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题。 Mixed 从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。 5.1 binlog主要应用场景 一是用于数据恢复，如果MySql数据库意外停止，可通过二进制日志文件来查看用户执行了哪些操作，对数据库服务器文件做了哪些修改，然后跟进二进制日志文件中的记录来恢复数据库服务器。 二是用户数据复制，由于日志的延续性和时效性，master把它的二进制日志传递给slaves来达到master-slave数据一致的目的。 可以说MySql数据库的数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性。 5.2 查看默认情况 查看记录二进制日志是否开启：在MySQL8中默认情况下，二进制文件是开启的。 mysql&gt; show variables like '%log_bin%'; +---------------------------------+----------------------------------+ | Variable_name | Value | +---------------------------------+----------------------------------+ | log_bin | ON | | log_bin_basename | /var/lib/mysql/binlog | | log_bin_index | /var/lib/mysql/binlog.index | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | | sql_log_bin | ON | +---------------------------------+----------------------------------+ 6 rows in set (0.00 sec) log_bin_basename：binlog日志的基本文件名，后面会追加标识来标识每一个文件 log_bin_index：是binlog文件的索引文件，这个文件管理了所有的binlog文件的目录 log_bin_trust_function_creators：限制存储过程，这是因为二进制日志的一个重要功能是用于主从复制，而存储函数可能导致主从的数据不一致，所以当开启二进制日志后，需要限制存储函数的创建、修改、调用 5.3 日志参数设置 修改MySQL的 my.cnf 或 my.ini 文件可以设置二进制日志的相关参数： [mysqld] #启用二进制日志 log-bin=mysql-bin # 日志名称，也可以填写自定义路劲 binlog_expire_logs_seconds=600 # 日志保留时长，单位：秒 max_binlog_size=100M # 控制单个二进制日志大小，超过时执行切换动作 注意：新建的文件夹需要使用mysql用户，使用下面的命令即可。 chown -R -v mysql:mysql binlog 5.4 查看日志 当MySQL创建二进制日志文件时，先创建一个以“filename”为名称、以“.index”为后缀的文件，再创建一 个以“filename”为名称、以“.000001”为后缀的文件。 MySQL服务 重新启动一次 ，以“.000001”为后缀的文件就会增加一个，并且后缀名按1递增。即日志文件的 个数与MySQL服务启动的次数相同；如果日志长度超过了 max_binlog_size 的上限（默认是1GB），就会创建一个新的日志文件。 查看当前的二进制日志文件列表及大小。指令如下： mysql&gt; SHOW BINARY LOGS; +------------------+-----------+-----------+ | Log_name | File_size | Encrypted | +------------------+-----------+-----------+ | mysql-bin.000001 | 179 | No | +------------------+-----------+-----------+ 1 rows in set (0.06 sec) 所有对数据库的修改都会记录在binlog中。但binlog是二进制文件，无法直接查看，想要更直观的观测它就要借助mysqlbinlog命令工具了。 mysqlbinlog &quot;/var/lib/mysql/mysql-bin.000001&quot; # 可查看参数帮助 mysqlbinlog --no-defaults --help # 查看最后100行 mysqlbinlog --no-defaults --base64-output=decode-rows -vv atguigu-bin.000002 |tail -100 # 根据position查找 mysqlbinlog --no-defaults --base64-output=decode-rows -vv atguigu-bin.000002 |grep -A 20 '4939002' 上面这种办法读取出binlog日志的全文内容比较多，不容易分辨查看到pos点信息，下面介绍一种更为方便的查询命令： mysql&gt; show binlog events [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count]; IN 'log_name' ：指定要查询的binlog文件名（不指定就是第一个binlog文件） FROM pos ：指定从哪个pos起始点开始查起（不指定就是从整个文件首个pos点开始算） LIMIT [offset] ：偏移量(不指定就是0) row_count :查询总条数（不指定就是所有行） 5.5 使用日志恢复数据 如果MySQL服务器启用了二进制日志，在数据库出现意外丢失数据时，可以使用MySQLbinlog工具从指定的时间点开始（例如，最后一次备份）直到现在或另一个指定的时间点的日志中回复数据。 mysqlbinlog恢复数据的语法如下： mysqlbinlog [option] filename|mysql –uuser -ppass; 这个命令可以这样理解：使用mysqlbinlog命令来读取filename中的内容，然后使用mysql命令将这些内容恢复到数据库中。 filename ：是日志文件名。 option ：可选项，比较重要的两对option参数是--start-date、--stop-date 和 --start-position、-- stop-position。 --start-date 和 --stop-date ：可以指定恢复数据库的起始时间点和结束时间点。 --start-position和--stop-position ：可以指定恢复数据的开始位置和结束位置。 注意：使用mysqlbinlog命令进行恢复操作时，必须是编号小的先恢复，例如atguigu-bin.000001必须在atguigu-bin.000002之前恢复。 5.6 删除二进制日志 MySQL的二进制文件可以配置自动删除，同时MySQL也提供了安全的手动删除二进制文件的方法。 PURGE MASTER LOGS 只删除指定部分的二进制日志文件， RESET MASTER 删除所有的二进制日志文 件。具体如下： 1. PURGE MASTER LOGS：删除指定日志文件 PURGE MASTER LOGS语法如下： PURGE {MASTER | BINARY} LOGS TO ‘指定日志文件名’ PURGE {MASTER | BINARY} LOGS BEFORE ‘指定日期’ # 示例一：如需要删除创建时间比binlog.000005早的所有日志 PURGE MASTER LOGS TO &quot;binlog.000005&quot;; # 示例二：删除2021年10月21号前创建的所有日志文件 PURGE MASTER LOGS BEFORE &quot;20211021&quot;; **2. RESET MASTER: 删除所有二进制日志文件 RESET MASTER; 5.7 其它场景 二进制日志可以通过数据库的 全量备份 和二进制日志中保存的 增量信息 ，完成数据库的 无损失恢复 。 但是，如果遇到数据量大、数据库和数据表很多（比如分库分表的应用）的场景，用二进制日志进行数据恢复，是很有挑战性的，因为起止位置不容易管理。 在这种情况下，一个有效的解决办法是 配置主从数据库服务器 ，甚至是 一主多从 的架构，把二进制日志文件的内容通过中继日志，同步到从数据库服务器中，这样就可以有效避免数据库故障导致的数据异常等问题。 六、再谈二进制日志(binlog) ## 6.1 写入机制 binlog的写入时机非常简单，事务执行过程中，先把日志写到`binlog cache`，事务提交的时候，再把`binlog cache`写到`binlog`文件中。 因为一个事务的binlog不能被拆开，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程非陪分配一块内存作为binlog cache。 我们可以通过binlog_cache_size参数控制单个线程 binlog cache 大小，如果存储内容超过了这个参数，就要暂存到磁盘（Swap）。binlog日志刷盘流程如下： 上图的write，是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快 上图的fsync，才是将数据持久化到磁盘的操作 write和fsync的时机，可以由参数 sync_binlog 控制，默认是 0 。为0的时候，表示每次提交事务都只 write，由系统自行判断什么时候执行fsync。虽然性能得到提升，但是机器宕机，page cache里面的 binglog 会丢失。如下图： 为了安全起见，可以设置为 1 ，表示每次提交事务都会执行fsync，就如同redo log 刷盘流程一样。 最后还有一种折中方式，可以设置为N(N&gt;1)，表示每次提交事务都write，但累积N个事务后才fsync。 在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。同样的，如果机器宕机，会丢失最近N个事务的binlog日志。 6.2 binlog与redolog对比 redo log 它是 物理日志 ，记录内容是“在某个数据页上做了什么修改”，属于 InnoDB 存储引擎层产生的。 而 binlog 是 逻辑日志 ，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于 MySQL Server 层。 虽然它们都属于持久化的保证，但是侧重点不同。 redo log让InnoDB存储引擎拥有了崩溃恢复能力。 binlog保证了MySQL集群架构的数据一致性。 6.3 两阶段提交 在执行更新语句的过程中，会记录redo log和binlog两块日志，以基本的事务为单位，redo log在事务执行过程中可以不断写入，而binlog只有在提交事务时才写入，所以redo log和binlog的写入时机不一样。 redo log与binlog两份日志之间的逻辑不一致，会出现什么问题？ 以update语句为例，假设id=2的记录，字段c值是0，把其更新为1，SQL语句为update T set c = 1 where id = 2。 假设执行过程中写完redo log之后，binlog日志写期间发生了异常，会出现什么情况？ 由于binlog没写完就异常，这时候binlog里面没有对应的修改记录。因此，之后用binlog日志恢复数据时，就会少了这一次更新，恢复出来的这一行c值为0，而主库因为redo log日志恢复，这一行c的值为1，最终导致主从数据不一致。 为了解决两份日志之间的逻辑一致问题，InnoDB存储引擎使用两阶段提交方案。原理很简单，将redo log的写入拆成了两个步骤prepare和commit，这就是两阶段提交。 使用两阶段提交后，写入binlog时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段，并且没有对应binlog日志，就会回滚该事务。 另一个场景，redo log设置commit阶段发生异常，那会不会回滚事务呢？ 并不会回滚事务，它会执行上图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以MySQL认为是完整的，就会提交事务恢复数据。 七、中继日志(relay log) 7.1 介绍 中继日志只在主从服务器架构的从服务器上存在。从服务器为了与主服务器保持一致，要从主服务器读取二进制日志的内容，并且把读取到的信息写入 本地的日志文件 中，这个从服务器本地的日志文件就叫 中继日志 。然后，从服务器读取中继日志，并根据中继日志的内容对从服务器的数据进行更新，完成主 从服务器的 数据同步 。 搭建好主从服务器之后，中继日志默认会保存在从服务器的数据目录下。 文件名的格式是： 从服务器名 -relay-bin.序号 。中继日志还有一个索引文件：从服务器名 -relaybin.index ，用来定位当前正在使用的中继日志。 7.2 查看中继日志 中继日志与二进制日志的格式相同，可以用 mysqlbinlog 工具进行查看。 7.3 恢复的典型错误 如果从服务器宕机，有的时候为了系统恢复，要重装操作系统，这样就可能会导致你的 服务器名称 与之前 不同 。而中继日志里是 包含从服务器名 的。在这种情况下，就可能导致你恢复从服务器的时候，无法 从宕机前的中继日志里读取数据，以为是日志文件损坏了，其实是名称不对了。 解决的方法也很简单，把从服务器的名称改回之前的名称。 ","link":"http://mofish.pily.life/post/mysql_learning_18/"},{"title":"MySql学习之路(十七)：数据库的一些调优策略","content":"🤗除了索引调优外，数据库还有一些别的调优策略，如分表、分库、读写分离、系统参数设置等等。 目录 一、数据库调优的措施 1.1 调优的目标 1.2 如何定位调优问题 1.3 调优的维度和步骤 二、优化MySql服务器 三、优化数据库结构 3.1 拆分表：冷热数据分离 3.2 增加中间表 3.3 增加冗余字段 3.4 优化数据类型 3.5 优化插入记录的速度 3.6 使用非空约束 四、大表优化 4.1 限定查询的范围 4.2 读/写分离 4.3 垂直拆分 4.4 水平拆分 五、其它调优策略 5.1 服务器语句超时处理 5.2 创建全局通用表空间 5.3 MySQL 8.0新特性：隐藏索引对调优的帮助 一、数据库调优的措施 👇👇 1.1 调优的目标 尽可能节省系统资源 ，以便系统可以提供更大负荷的服务。（吞吐量更大） 合理的结构设计和参数调整，以提高用户操作响应的速度 。（响应速度更快） 减少系统的瓶颈，提高MySQL数据库整体的性能。 1.2 如何定位调优问题 用户反馈 日志分析，通过查看数据库日志和操作系统日志等方式找出异常情况。 服务器资源监控，通过监控服务器的CPU、内存、I/O等使用情况，实时了解服务器的性能状况。 数据库内部状况监控，活动会话（Active Session）监控是一个重要指标，通过它可了解数据库当前是否处于繁忙状态，是否存在SQL堆积等。 其它的话，还可以对事务、锁等待等进行监控 1.3 调优的维度和步骤 我们需要调优的对象是整个数据库管理系统，它不仅包括 SQL 查询，还包括数据库的部署配置、架构 等。从这个角度来说，我们思考的维度就不仅仅局限在 SQL 优化上了。通过如下的步骤我们进行梳理： 第1步：选择适合的 DBMS 第2步：优化表设计 第3步：优化逻辑查询 第4步：优化物理查询 物理查询优化是在确定了逻辑查询优化之后，采用物理优化技术（比如索引等），通过计算代价模型对 各种可能的访问路径进行估算，从而找到执行方式中代价最小的作为执行计划。在这个部分中，我们需要掌握的重点是对索引的创建和使用。 第5步：使用 Redis 或 Memcached 作为缓存 除了可以对 SQL 本身进行优化以外，我们还可以请外援提升查询的效率。 因为数据都是存放到数据库中，我们需要从数据库层中取出数据放到内存中进行业务逻辑的操作，当用户量增大的时候，如果频繁地进行数据查询，会消耗数据库的很多资源。如果我们将常用的数据直接放到内存中，就会大幅提升查询的效率。 第6步：库级优化 但需要注意的是，分拆在提升数据库性能的同时，也会增加维护和使用成本。 二、优化MySql服务器 现在主流都是使用云服务器，因此硬件方面的优化就不过多描述，主要需要记录的是一些服务器参数的调优。 innodb_buffer_pool_size ：这个参数是Mysql数据库最重要的参数之一，表示InnoDB类型的 表 和索引的最大缓存 。它不仅仅缓存 索引数据 ，还会缓存 表的数据 。这个值越大，查询的速度就会越 快。但是这个值太大会影响操作系统的性能。 key_buffer_size ：表示 索引缓冲区的大小 。索引缓冲区是所有的 线程共享 。增加索引缓冲区可 以得到更好处理的索引（对所有读和多重写）。当然，这个值不是越大越好，它的大小取决于内存 的大小。如果这个值太大，就会导致操作系统频繁换页，也会降低系统性能。对于内存在 4GB 左右 的服务器该参数可设置为 256M 或 384M 。 table_cache ：表示 同时打开的表的个数 。这个值越大，能够同时打开的表的个数越多。物理内 存越大，设置就越大。默认为2402，调到512-1024最佳。这个值不是越大越好，因为同时打开的表 太多会影响操作系统的性能。 query_cache_size ：表示 查询缓冲区的大小 。可以通过在MySQL控制台观察，如果 Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况，就要增加Query_cache_size 的值；如果Qcache_hits的值非常大，则表明查询缓冲使用非常频繁，如果该值较小反而会影响效 率，那么可以考虑不用查询缓存；Qcache_free_blocks，如果该值非常大，则表明缓冲区中碎片很 多。MySQL8.0之后失效。该参数需要和query_cache_type配合使用。 query_cache_type 的值是0时，所有的查询都不使用查询缓存区。但是query_cache_type=0并不 会导致MySQL释放query_cache_size所配置的缓存区内存。 当query_cache_type=1时，所有的查询都将使用查询缓存区，除非在查询语句中指定 SQL_NO_CACHE ，如SELECT SQL_NO_CACHE * FROM tbl_name。 当query_cache_type=2时，只有在查询语句中使用 SQL_CACHE 关键字，查询才会使用查询缓 存区。使用查询缓存区可以提高查询的速度，这种方式只适用于修改操作少且经常执行相同的 查询操作的情况。 sort_buffer_size ：表示每个 需要进行排序的线程分配的缓冲区的大小 。增加这个参数的值可以 提高 ORDER BY 或 GROUP BY 操作的速度。默认数值是2 097 144字节（约2MB）。对于内存在4GB 左右的服务器推荐设置为6-8M，如果有100个连接，那么实际分配的总共排序缓冲区大小为100 × 6 ＝ 600MB。 join_buffer_size = 8M ：表示 联合查询操作所能使用的缓冲区大小 ，和sort_buffer_size一样， 该参数对应的分配内存也是每个连接独享。 read_buffer_size ：表示 每个线程连续扫描时为扫描的每个表分配的缓冲区的大小（字节） 。当线 程从表中连续读取记录时需要用到这个缓冲区。SET SESSION read_buffer_size=n可以临时设置该参 数的值。默认为64K，可以设置为4M。 innodb_flush_log_at_trx_commit ：表示 何时将缓冲区的数据写入日志文件 ，并且将日志文件 写入磁盘中。该参数对于innoDB引擎非常重要。该参数有3个值，分别为0、1和2。该参数的默认值 为1。 值为 0 时，表示 每秒1次 的频率将数据写入日志文件并将日志文件写入磁盘。每个事务的 commit并不会触发前面的任何操作。该模式速度最快，但不太安全，mysqld进程的崩溃会导 致上一秒钟所有事务数据的丢失。 值为 1 时，表示 每次提交事务时 将数据写入日志文件并将日志文件写入磁盘进行同步。该模 式是最安全的，但也是最慢的一种方式。因为每次事务提交或事务外的指令都需要把日志写入 （flush）硬盘。 值为 2 时，表示 每次提交事务时 将数据写入日志文件， 每隔1秒 将日志文件写入磁盘。该模 式速度较快，也比0安全，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数 据才可能丢失。 innodb_log_buffer_size ：这是 InnoDB 存储引擎的 事务日志所使用的缓冲区 。为了提高性能， 也是先将信息写入 Innodb Log Buffer 中，当满足 innodb_flush_log_trx_commit 参数所设置的相应条 件（或者日志缓冲区写满）之后，才会将日志写到文件（或者同步到磁盘）中。 max_connections ：表示 允许连接到MySQL数据库的最大数量 ，默认值是 151 。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接 数已达到允许最大值而失败，这是可以考虑增大max_connections 的值。在Linux 平台下，性能好的 服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。这个连接数 不是越大 越好 ，因为这些连接会浪费内存的资源。过多的连接可能会导致MySQL服务器僵死。 back_log ：用于 控制MySQL监听TCP端口时设置的积压请求栈大小 。如果MySql的连接数达到 max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即 back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默 认值为 50 ， 之后的版本默认为 50 + （max_connections / 5）， 对于Linux系统推荐设置为小于512 的整数，但最大不超过900。 如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。 thread_cache_size ： 线程池缓存线程数量的大小 ，当客户端断开连接后将当前线程缓存起来， 当在接到新的连接请求时快速响应无需创建新的线程 。这尤其对那些使用短连接的应用程序来说可 以极大的提高创建连接的效率。那么为了提高性能可以增大该参数的值。默认为60，可以设置为 120。 可以通过如下几个MySQL状态值来适当调整线程池的大小： mysql&gt; show global status like 'Thread%'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 2 | | Threads_connected | 1 | | Threads_created | 3 | | Threads_running | 2 | +-------------------+-------+ 4 rows in set (0.01 sec) 当 Threads_cached 越来越少，但 Threads_connected 始终不降，且 Threads_created 持续升高，可 适当增加 thread_cache_size 的大小。 wait_timeout ：指定 一个请求的最大连接时间 ，对于4GB左右内存的服务器可以设置为5-10。 interactive_timeout ：表示服务器在关闭连接前等待行动的秒数。 这里给出一份my.cnf的参考配置： [mysqld] port = 3306 serverid = 1 socket = /tmp/mysql.sock skip-locking #避免MySQL的外部锁定，减少出错几率增强稳定性。 skip-name-resolve #禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ back_log = 384 key_buffer_size = 256M max_allowed_packet = 4M thread_stack = 256K table_cache = 128K sort_buffer_size = 6M read_buffer_size = 4M read_rnd_buffer_size=16M join_buffer_size = 8M myisam_sort_buffer_size =64M table_cache = 512 thread_cache_size = 64 query_cache_size = 64M tmp_table_size = 256M max_connections = 768 max_connect_errors = 10000000 wait_timeout = 10 thread_concurrency = 8 #该参数取值为服务器逻辑CPU数量*2，在本例中，服务器有2颗物理CPU，而每颗物理CPU又支持H.T超线程，所以实际取值为4*2=8 skip-networking #开启该选项可以彻底关闭MySQL的TCP/IP连接方式，如果WEB服务器是以远程连接的方式访问MySQL数据库服务器则不要开启该选项！否则将无法正常连接！ table_cache=1024 innodb_additional_mem_pool_size=4M #默认为2M innodb_flush_log_at_trx_commit=1 innodb_log_buffer_size=2M #默认为1M innodb_thread_concurrency=8 #你的服务器CPU有几个就设置为几。建议用默认一般为8 tmp_table_size=64M #默认为16M，调到64-256最挂 thread_cache_size=120 query_cache_size=32M 三、优化数据库结构 一个好的数据库设计方案对于数据库的性能常常会起到事半功倍的效果。合理的数据库结构不仅可以使数据库占用更下的磁盘空间，而且能够查询速度更快。数据库结构的设计需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多个方面的内容。 3.1 拆分表：冷热数据分离 3.2 增加中间表 3.3 增加冗余字段 3.4 优化数据类型 3.5 优化插入记录的速度 针对nnoDB引擎的表： ① 禁用唯一性检查 插入数据之前执行set unique_checks=0来禁止对唯一索引的检查，数据导入完成之后再运行set unique_check=1。这个和MyISAM引擎的使用方法一样。 ② 禁用外键检查 ③ 禁止自动提交 3.6 使用非空约束 四、大表优化 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 4.1 限定查询的范围 禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制 在一个月的范围内； 4.2 读/写分离 经典的数据库拆分方案，主库负责写，从库负责读。 一主一从模式： 双主双从模式： 4.3 垂直拆分 当数据量级达到 千万级 以上时，有时候我们需要把一个数据库切成多份，放到不同的数据库服务器上， 减少对单一数据库服务器的访问压力。 如果数据库的数据表过多，可以采用垂直分库的方式，将关联的数据库部署在同一个数据库上。 如果数据库中的列过多，可以采用垂直分表的方式，将一张数据表分拆成多张数据表，把经常一起使用的列放在同一张表里。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起 JOIN 操作。此外，垂直拆分会让事务变得更加复杂。 4.4 水平拆分 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 五、其它调优策略 👇👇 5.1 服务器语句超时处理 在MySQL 8.0中可以设置 服务器语句超时的限制 ，单位可以达到 毫秒级别 。当中断的执行语句超过设置的 毫秒数后，服务器将终止查询影响不大的事务或连接，然后将错误报给客户端。 设置服务器语句超时的限制，可以通过设置系统变量 MAX_EXECUTION_TIME 来实现。默认情况下， MAX_EXECUTION_TIME的值为0，代表没有时间限制。 例如： SET GLOBAL MAX_EXECUTION_TIME=2000; SET SESSION MAX_EXECUTION_TIME=2000; #指定该会话中SELECT语句的超时时间 5.2 创建全局通用表空间 5.3 MySQL 8.0新特性：隐藏索引对调优的帮助 ","link":"http://mofish.pily.life/post/mysql_learning_17/"},{"title":"MySql学习之路(十六)：join语句的原理","content":"😏join方式连接多个表，本质就是各个表之间数据的循环匹配。 MySql5.5版本之前，只支持一种表间关联方式，就是嵌套循环（Nested Loop Join）。如果关联表的数据量较大，则join关联的执行时间会很长。而在MySql5.5版本之后，通过引入了BNLJ算法来优化嵌套执行。 1. 驱动表和被驱动表 驱动表就是主表，被驱动表是从表。 对于内连接来说： SELECT * FROM A JOIN B ON ... A一定是驱动表吗？不一定，优化器会根据你查询语句做优化，决定先查哪张表。先查询的那张表就是驱动表，反之就是被驱动表。通过explain关键字可以查看。 对于外连接来说： SELECT * FROM A LEFT JOIN B ON ... # 或 SELECT * FROM B RIGHT JOIN A ON ... 通常，大家会认为A就是驱动表，B就是被驱动表。但也未必。测试如下： CREATE TABLE a(f1 INT, f2 INT, INDEX(f1)) ENGINE=INNODB; CREATE TABLE b(f1 INT, f2 INT) ENGINE=INNODB; INSERT INTO a VALUES(1,1),(2,2),(3,3),(4,4),(5,5),(6,6); INSERT INTO b VALUES(3,3),(4,4),(5,5),(6,6),(7,7),(8,8); SELECT * FROM b; # 测试1 EXPLAIN SELECT * FROM a LEFT JOIN b ON(a.f1=b.f1) WHERE (a.f2=b.f2); +----+-------------+-------+------------+------+---------------+------+---------+------------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------------+------+----------+-------------+ | 1 | SIMPLE | b | NULL | ALL | NULL | NULL | NULL | NULL | 6 | 100.00 | Using where | | 1 | SIMPLE | a | NULL | ref | f1 | f1 | 5 | study.b.f1 | 1 | 16.67 | Using where | +----+-------------+-------+------------+------+---------------+------+---------+------------+------+----------+-------------+ 2 rows in set, 1 warning (0.00 sec) # 测试2 EXPLAIN SELECT * FROM a LEFT JOIN b ON(a.f1=b.f1) AND (a.f2=b.f2); +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------------------+ | 1 | SIMPLE | a | NULL | ALL | NULL | NULL | NULL | NULL | 6 | 100.00 | NULL | | 1 | SIMPLE | b | NULL | ALL | NULL | NULL | NULL | NULL | 6 | 100.00 | Using where; Using join buffer (hash join) | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+--------------------------------------------+ 2. Simple Nested-Loop Join (简单嵌套循环连接) 算法相当简单，从表A中取出一条数据1，遍历表B，将匹配到的数据放到result.. 以此类推，驱动表A中的每一条记录与被驱动表B的记录进行判断： 可以看到这种方式效率是非常低的，以上述表A数据100条，表B数据1000条计算，则A*B=10万次。开销统计如下: 当然mysql肯定不会这么粗暴的去进行表的连接，所以就出现了后面的两种对Nested-Loop Join优化算法。 3. Index Nested-Loop Join （索引嵌套循环连接） Index Nested-Loop Join其优化的思路主要是为了减少内存表数据的匹配次数，所以要求被驱动表上必须有索引才行。通过外层表匹配条件直接与内层表索引进行匹配，避免和内存表的每条记录去进行比较，这样极大的减少了对内存表的匹配次数。 驱动表中的每条记录通过被驱动表的索引进行访问，因为索引查询的成本是比较固定的，故mysql优化器都倾向于使用记录数少的表作为驱动表（外表）。 如果被驱动表加索引，效率是非常高的，但如果索引不是主键索引，所以还得进行一次回表查询。相比，被驱动表的索引是主键索引，效率会更高。 4. Block Nested-Loop Join（块嵌套循环连接） 如果存在索引，那么会使用index的方式进行join，如果join的列没有索引，被驱动表要扫描的次数太多了。每次访问被驱动表，其表中的记录都会被加载到内存中，然后再从驱动表中取一条与其匹配，匹配结束后清除内存，然后再从驱动表中加载一条记录，然后把被驱动表的记录在加载到内存匹配，这样周而复始，大大增加了IO的次数。 为了减少被驱动表的IO次数，就出现了Block Nested-Loop Join的方式。不再是逐条获取驱动表的数据，而是一块一块的获取，引入了join buffer缓冲区，将驱动表join相关的部分数据列（大小受join buffer的限制）缓存到join buffer中，然后全表扫描被驱动表，被驱动表的每一条记录一次性和join buffert中的所有驱动表记录进行匹配（内存中操作），将简单嵌套循环中的多次比较合并成一次，降低了被驱动表的访问频率。 举例来说，外层循环的结果集是100行，使用NLJ 算法需要扫描内部表100次，如果使用BNL算法，先把对Outer Loop表(外部表)每次读取的10行记录放到join buffer,然后在InnerLoop表(内部表)中直接匹配这10行数据，内存循环就可以一次与这10行进行比较, 这样只需要比较10次，对内部表的扫描减少了9/10。所以BNL算法就能够显著减少内层循环表扫描的次数. 注意： 这里缓存的不只是关联表的列，select后面的列也会缓存起来。 在一个有N个join关联的sql中会分配N-1个join buffer。所以查询的时候尽量减少不必要的字段，可以让join buffer中可以存放更多的列。 参数设置： block_nested_loop 通过show variables like '%optimizer_switch% 查看 block_nested_loop状态。默认是开启的。 join_buffer_size 驱动表能不能一次加载完，要看join buffer能不能存储所有的数据，默认情况下join_buffer_size=256k。 mysql&gt; show variables like '%join_buffer%'; join_buffer_size的最大值在32位操作系统可以申请4G，而在64位操作系统下可以申请大于4G的Join Buffer空间（64位Windows除外，其大值会被截断为4GB并发出警告）。 5. Join小结 整体效率比较：INLJ &gt; BNLJ &gt; SNLJ 永远用小结果集驱动大结果集（其本质就是减少外层循环的数据数量）（小的度量单位指的是表行数 * 每行大小） 为被驱动表匹配的条件增加索引(减少内存表的循环匹配次数) 增大join buffer size的大小（一次索引的数据越多，那么内层包的扫描次数就越少） 减少驱动表不必要的字段查询（字段越少，join buffer所缓存的数据就越多） 6. Hash Join 从MySQL的8.0.20版本开始将废弃BNLJ，因为从MySQL8.0.18版本开始就加入了hash join默认都会使用hash join Nested Loop: 对于被连接的数据子集较小的情况，Nested Loop是个较好的选择。 Hash Join是做大数据集连接时的常用方式，优化器使用两个表中较小（相对较小）的表利用Join Key在内存中建立散列表，然后扫描较大的表并探测散列表，找出与Hash表匹配的行。 这种方式适合于较小的表完全可以放于内存中的情况，这样总成本就是访问两个表的成本之和。 在表很大的情况下并不能完全放入内存，这时优化器会将它分割成若干不同的分区，不能放入内存的部分就把该分区写入磁盘的临时段，此时要求有较大的临时段从而尽量提高I/O的性能。 它能够很好的工作于没有索引的大表和并行查询的环境中，并提供最好的性能。大多数人都说它是Join的重型升降机。Hash Join只能应用于等值连接（如WHERE A.COL1 = B.COL2），这是由Hash的特点决定的。 ","link":"http://mofish.pily.life/post/mysql_learning_16/"},{"title":"MySql学习之路(十五)：慢查询日志","content":"❄️慢查询日志，是用来记录MySql中响应时间超过阈值的语句，也是我们优化MySql的一个重要手段，接下来let's look look！ 上面提到的响应时间超过的阈值，具体指☞运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中，其默认值为10。 它的主要作用是帮助我们发现哪些执行时间特别长的SQL查询，并且有针对性地优化，从而提高系统的整体效率。当我们的数据库服务器发生阻塞、运行变慢的时候，检查一下慢查询日志，结合一下explain进行全面分析。 一、开启慢查询日志参数 默认情况下，MySql是没有开启慢查询日志的，需要我们手动来设置这个参数。如果不是调优需要，一般不建议启动该参数，因为开启慢查询日志或多或少会带来一定的性能影响。 -- 开启慢查询日志，OFF-关闭，ON-开启 mysql&gt; set global slow_query_log='ON'; -- 检查是否开启慢查询日志及日志位置 mysql&gt; show variables like '%slow_query_log%'; +---------------------+---------------------------------+ | Variable_name | Value | +---------------------+---------------------------------+ | slow_query_log | ON | | slow_query_log_file | /www/server/data/mysql-slow.log | +---------------------+---------------------------------+ -- 设置慢查询时间阈值 #测试发现：设置global的方式对当前session的long_query_time失效。对新连接的客户端有效。所以可以一并 mysql &gt; set global long_query_time = 1; mysql&gt; show global variables like '%long_query_time%'; mysql&gt; set long_query_time=1; mysql&gt; show variables like '%long_query_time%'; -- 查询当前系统中有多少条慢查询记录 mysql&gt; SHOW GLOBAL STATUS LIKE '%Slow_queries%'; 除此之外，还可以直接设置my.cnf文件，然后重启MySql服务器使配置生效： [mysqld] slow_query_log=ON # 开启慢查询日志开关 slow_query_log_file=/var/lib/mysql/atguigu-low.log # 慢查询日志的目录和文件名信息 long_query_time=3 # 设置慢查询的阈值为3秒，超出此设定值的SQL即被记录到慢查询日志 log_output=FILE 二、案例演示 首先创建一下演示的数据表，然后填充一下数据。 -- 建表 CREATE TABLE `student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stuno` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `classId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; -- 设置参数 log_bin_trust_function_creators（This function has none of DETERMINISTIC......） set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 -- 随机产生字符串： DELIMITER // CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) #该函数会返回一个字符串 BEGIN DECLARE chars_str VARCHAR(100) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; DECLARE return_str VARCHAR(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; # 测试 SELECT rand_string(10); -- 产生随机数值： DELIMITER // CREATE FUNCTION rand_num (from_num INT ,to_num INT) RETURNS INT(11) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END // DELIMITER ; #测试： SELECT rand_num(10,100); -- 创建存储过程 DELIMITER // CREATE PROCEDURE insert_stu1( START INT , max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student (stuno, NAME ,age ,classId ) VALUES ((START+i),rand_string(6),rand_num(10,100),rand_num(10,1000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; -- 调用刚刚写好的函数, 8000000条记录,从100001号开始 CALL insert_stu1(100001,8000000); 三、测试及分析 mysql&gt; SELECT * FROM student WHERE stuno = 3455655; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 3355654 | 3455655 | wIYWKN | 70 | 58 | +---------+---------+--------+------+---------+ 1 row in set (2.16 sec) mysql&gt; SELECT * FROM student WHERE name = 'oQmLUr'; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 608599 | 708600 | OQMlUR | 54 | 916 | | 1320974 | 1420975 | OQMlUR | 56 | 57 | | 1646809 | 1746810 | Oqmlur | 57 | 596 | | 1733845 | 1833846 | Oqmlur | 53 | 382 | | 1878203 | 1978204 | oqMLuR | 11 | 546 | | 2209155 | 2309156 | Oqmlur | 56 | 531 | | 2408935 | 2508936 | oQmLUr | 96 | 782 | | 4425349 | 4525350 | Oqmlur | 50 | 252 | | 5053772 | 5153773 | oqMLuR | 10 | 494 | | 5065961 | 5165962 | oQmLUr | 96 | 780 | | 5074254 | 5174255 | oQmLUr | 98 | 866 | | 5980254 | 6080255 | oqMLuR | 11 | 551 | | 6114492 | 6214493 | oQmLUr | 10 | 30 | | 6909435 | 7009436 | oQmLUr | 98 | 853 | | 7073314 | 7173315 | Oqmlur | 51 | 309 | +---------+---------+--------+------+---------+ 15 rows in set (2.28 sec) 上面的查询结果已经超过long_query_time的阈值时间1s，那么应该当记录进慢查询日志中。 注意： 除了上述变量外，控制慢查询日志的还有一个系统变量：min_examined_row_limit，指的是扫描过的最少记录数。这个变量和查询执行时间，共同组成判断一个查询是否是慢查询的条件。 如果查询扫描过的记录数大于等于这个变量的值，并且查询执行时间超过long_query_time的值，那么，这个查询就被记录到慢查询日志中，反之则不会被记录下来。 -- 查询扫描最少记录数 mysql&gt; show variables like 'min%'; +------------------------+-------+ | Variable_name | Value | +------------------------+-------+ | min_examined_row_limit | 0 | +------------------------+-------+ -- 设置扫描最少记录📖 mysql&gt; set min_examined_row_limit=1; 这个值默认为0，与long_query_time=1组合在一起，表示只要查询的执行时间查过1秒，哪怕一个记录页没有扫描过，都要被记录到慢查询日志中。 四、慢查询分析工具：mysqldumpslow 在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具 mysqldumpslow 。 mysqldumpslow 命令的具体参数如下： -a: 不将数字抽象成N，字符串抽象成S -s: 是表示按照何种方式排序： c: 访问次数 l: 锁定时间 r: 返回记录 t: 查询时间 al:平均锁定时间 ar:平均返回记录数 at:平均查询时间 （默认方式） ac:平均查询次数 -t: 即为返回前面多少条的数据； -g: 后边搭配一个正则匹配模式，大小写不敏感的； 举例：我们想要按照查询时间排序，查看前五条 SQL 语句，这样写即可： mysqldumpslow -s t -t 5 /usr/local/mysql/data/moyuxingdeiMac-slow.log Reading mysql slow query log from /usr/local/mysql/data/moyuxingdeiMac-slow.log Count: 1 Time=2.26s (2s) Lock=0.00s (0s) Rows=15.0 (15), root[root]@localhost SELECT * FROM student WHERE name = 'S' Count: 1 Time=2.14s (2s) Lock=0.00s (0s) Rows=1.0 (1), root[root]@localhost SELECT * FROM student WHERE stuno = N Died at ./mysqldumpslow line 162, &lt;&gt; chunk 2. 工作常用参考： #得到返回记录集最多的10个SQL mysqldumpslow -s r -t 10 /var/lib/mysql/atguigu-slow.log #得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 /var/lib/mysql/atguigu-slow.log #得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s t -t 10 -g &quot;left join&quot; /var/lib/mysql/atguigu-slow.log #另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现爆屏情况 mysqldumpslow -s r -t 10 /var/lib/mysql/atguigu-slow.log | more # 删除慢查询日志 mysqladmin -uroot -p flush-logs slow ","link":"http://mofish.pily.life/post/mysql_learning_15/"},{"title":"MySql学习之路(十四)：性能分析工作的使用","content":"在数据库调优中，我们的目标是 响应时间更快, 吞吐量更大 。利用宏观的监控工具和微观的日志分析可以帮我们快速找到调优的思路和方式。 目录 一、数据库服务器的优化步骤 二、查看系统性能参数 三、统计SQL的查询成本：last_query_cost 四、定位查询慢的SQL：慢查询日志 五、分析查询语句：EXPLAIN 六、分析优化器执行计划：trace 七、MySql监控分析视图-sys schema 小结 一、数据库服务器的优化步骤 整个流程划分成了观察（Show status）和行动（Action）两个部分。字母S的部分代表观察（会使用对应的分析工具），字母A的部分代表行动（对应分析可以采取的行动）。 S1：我们需要观察服务器的状态是否存在周期性波动，如果存在周期性波动，有可能是周期性节点的原因，比如双十一、促销活动等。这样的话，我们可以通过A1这一步解决，就是加缓存或更改缓存失效策略。 S2：如果不是上述原因，那我们就需要进一步分析延迟和卡顿的原因，在S2这一步，我们需要开启慢查询。慢查询可以帮我们定位执行慢的SQL语句。我们可以通过设置long_query_time参数定义“慢”的阈值，当收集上来这些慢查询之后，我们就可以通过分析工作对慢查询日志进行分析。 S3：当我们知道了执行慢的SQL后，就可以使用EXPLAIN查看对应的SQL语句的执行计划，或者使用show profile查询SQL中每一个步骤的时间成本，这样我们就可以了解SQL查询慢是因为执行时间长还是等待时间长了。 A2：如果是SQL等待时间长，我们可以调优服务器的参数，比如适当增加数据库缓冲池等。 A3：如果是SQL执行时间长，首先考虑的是索引设计的问题，然后看看是否联表过多或者是表结构设计有缺陷等等。 A4：如果A2和A3都不能解决问题，我们需要考虑数据自身的性能是否已经达到了瓶颈，如果确认没有到达瓶颈那么久需要重新检查一下。如果已经到达了性能瓶颈，那么就需要考虑增加服务器，采用读写分离的架构，或者考虑对数据进行分库分表等操作了。 以上就是数据库调优的流程思路，我们可以通过观察了解数据库整体的运行状态，通过性能分析工具可以让我们了解执行慢的SQL都有哪些，查看具体的SQL执行计划，甚至是SQL执行中的每一步的成本代价，这样才能定位问题所在，找到了问题，再采取相应的行动。 这三种分析工具可以理解是SQL调优的三个步骤：慢查询、EXPLAIN和SHOW PROFILING。 二、查看系统性能参数 在MySql中，可以使用SHOW STATUS语句查询一些MySql数据库服务器的性能参数、执行频率。 SHOW [GLOBAL] STATUS LIKE '参数'; 一些性能参数如下： Connections：连接MySQL服务器的次数。 Uptime：MySQL服务器的上线时间。 Slow_queries：慢查询的次数。 Innodb_rows_read：Select查询返回的行数 Innodb_rows_inserted：执行INSERT操作插入的行数 Innodb_rows_updated：执行UPDATE操作更新的 行数 Innodb_rows_deleted：执行DELETE操作删除的行数 Com_select：查询操作的次数。 Com_insert：插入操作的次数。对于批量插入的 INSERT 操作，只累加一次。 Com_update：更新操作 的次数。 Com_delete：删除操作的次数。 三、统计SQL的查询成本：last_query_cost 一条SQL查询语句在执行前需要查询执行计划，如果存在多种执行计划的话，MySql回计算每个执行计划所需的成本，从中选择成本最小的一个作为最终执行的执行计划。 如果我们想要查看某条SQL语句的查询成本，可以在执行完这条SQL语句之后，通过查看当前会话中的last_query_cost变量值来得到当前查询的成本。它通常也是我们评价一个查询执行效率常用指标。这个查询成本对应的是SQL语句所需要读取的页的数量。 -- 构建数据表 CREATE TABLE `student_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `student_id` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `course_id` INT NOT NULL , `class_id` INT(11) DEFAULT NULL, `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; -- 查询一条记录 SELECT student_id, class_id, NAME, create_time FROM student_info WHERE id = 900001; SHOW STATUS LIKE 'last_query_cost'; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | Last_query_cost | 1.000000 | +-----------------+----------+ -- 查询多条记录 SELECT student_id, class_id, NAME, create_time FROM student_info WHERE id BETWEEN 900001 AND 900100; SHOW STATUS LIKE 'last_query_cost'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | Last_query_cost | 21.134453 | +-----------------+-----------+ 虽然看到页的数量是第一次查询的20倍，但是查询的效率并没有明显的变化，实际上这两个SQL的查询时间基本是一样的，就是因为采取了顺序读取得方式，讲页面一次性加载到缓冲池，然后再进行查找。虽然页的数量增加了，但是通过缓冲池的机制，并没有增加多少查询时间。 SQL查询时一个动态的过程，从页加载的角度来看，我们可以得到以下两点结论： 位置决定效率。如果页就在数据库的缓冲池中，那么效率最高，否则还需要从磁盘中读取数据并加载到内存中。 批量决定效率。如果我们从磁盘对单一页进行随机读，那么效率是低下的，而采用顺序读取的方式，批量对页进行读取，那么平均一页的读取效率就会提升很多，甚至快于单个页面在内存中的随机读取。 四、定位查询慢的SQL：慢查询日志 look this👉👉👉👉《MySql学习之路(十五)：慢查询日志》 五、分析查询语句：EXPLAIN look this👉👉👉👉《Mysql学习之路(六)：explain执行计划详解》 六、分析优化器执行计划：trace OPTIMIZER_TRACE是MySql5.6引入的一项跟踪功能，它可以跟踪优化器做出的各种决策（比如访问表的方法、各种开销计算、各种转换等），并将跟踪记录结果记录到INFORMATION_SCHEMA.OPTIMIZER_TRACE表中。 此功能默认关闭。开启trace，并设置格式未JSON，同事设置trace最大能够使用的内存大小，避免解析过程中因默认内存过小而不能够完整展示。 SET optimizer_trace=&quot;enabled=on&quot;,end_markers_in_json=on; SET optimizer_trace_max_mem_size=1000000; 开启后，可分析如下语句： SELECT INSERT REPLACE UPDATE DELETE EXPLAIN SET DECLARE CASE IF RETURN CALL -- 测试语句 select * from student where id &lt; 10; -- 查询 information_schema.optimizer_trace select * from information_schema.optimizer_trace\\G *************************** 1. row *************************** //第1部分：查询语句 QUERY: select * from student where id &lt; 10 //第2部分：QUERY字段对应语句的跟踪信息 TRACE: { &quot;steps&quot;: [ { &quot;join_preparation&quot;: { //预备工作 &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;expanded_query&quot;: &quot;/* select#1 */ select `student`.`id` AS `id`,`student`.`stuno` AS `stuno`,`student`.`name` AS `name`,`student`.`age` AS `age`,`student`.`classId` AS `classId` from `student` where (`student`.`id` &lt; 10)&quot; } ] /* steps */ } /* join_preparation */ }, { &quot;join_optimization&quot;: { //进行优化 &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;condition_processing&quot;: { //条件处理 &quot;condition&quot;: &quot;WHERE&quot;, &quot;original_condition&quot;: &quot;(`student`.`id` &lt; 10)&quot;, &quot;steps&quot;: [ { &quot;transformation&quot;: &quot;equality_propagation&quot;, &quot;resulting_condition&quot;: &quot;(`student`.`id` &lt; 10)&quot; }, { &quot;transformation&quot;: &quot;constant_propagation&quot;, &quot;resulting_condition&quot;: &quot;(`student`.`id` &lt; 10)&quot; }, { &quot;transformation&quot;: &quot;trivial_condition_removal&quot;, &quot;resulting_condition&quot;: &quot;(`student`.`id` &lt; 10)&quot; } ] /* steps */ } /* condition_processing */ }, { &quot;substitute_generated_columns&quot;: { //替换生成的列 } /* substitute_generated_columns */ }, { &quot;table_dependencies&quot;: [ //表的依赖关系 { &quot;table&quot;: &quot;`student`&quot;, &quot;row_may_be_null&quot;: false, &quot;map_bit&quot;: 0, &quot;depends_on_map_bits&quot;: [ ] /* depends_on_map_bits */ } ] /* table_dependencies */ }, { &quot;ref_optimizer_key_uses&quot;: [ //使用键 ] /* ref_optimizer_key_uses */ }, { &quot;rows_estimation&quot;: [ //行判断 { &quot;table&quot;: &quot;`student`&quot;, &quot;range_analysis&quot;: { &quot;table_scan&quot;: { &quot;rows&quot;: 3973767, &quot;cost&quot;: 408558 } /* table_scan */, //扫描表 &quot;potential_range_indexes&quot;: [ //潜在的范围索引 { &quot;index&quot;: &quot;PRIMARY&quot;, &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;id&quot; ] /* key_parts */ } ] /* potential_range_indexes */, &quot;setup_range_conditions&quot;: [ //设置范围条件 ] /* setup_range_conditions */, &quot;group_index_range&quot;: { &quot;chosen&quot;: false, &quot;cause&quot;: &quot;not_group_by_or_distinct&quot; } /* group_index_range */, &quot;skip_scan_range&quot;: { &quot;potential_skip_scan_indexes&quot;: [ { &quot;index&quot;: &quot;PRIMARY&quot;, &quot;usable&quot;: false, &quot;cause&quot;: &quot;query_references_nonkey_column&quot; } ] /* potential_skip_scan_indexes */ } /* skip_scan_range */, &quot;analyzing_range_alternatives&quot;: { //分析范围选项 &quot;range_scan_alternatives&quot;: [ { &quot;index&quot;: &quot;PRIMARY&quot;, &quot;ranges&quot;: [ &quot;id &lt; 10&quot; ] /* ranges */, &quot;index_dives_for_eq_ranges&quot;: true, &quot;rowid_ordered&quot;: true, &quot;using_mrr&quot;: false, &quot;index_only&quot;: false, &quot;rows&quot;: 9, &quot;cost&quot;: 1.91986, &quot;chosen&quot;: true } ] /* range_scan_alternatives */, &quot;analyzing_roworder_intersect&quot;: { &quot;usable&quot;: false, &quot;cause&quot;: &quot;too_few_roworder_scans&quot; } /* analyzing_roworder_intersect */ } /* analyzing_range_alternatives */, &quot;chosen_range_access_summary&quot;: { //选择范围访问摘要 &quot;range_access_plan&quot;: { &quot;type&quot;: &quot;range_scan&quot;, &quot;index&quot;: &quot;PRIMARY&quot;, &quot;rows&quot;: 9, &quot;ranges&quot;: [ &quot;id &lt; 10&quot; ] /* ranges */ } /* range_access_plan */, &quot;rows_for_plan&quot;: 9, &quot;cost_for_plan&quot;: 1.91986, &quot;chosen&quot;: true } /* chosen_range_access_summary */ } /* range_analysis */ } ] /* rows_estimation */ }, { &quot;considered_execution_plans&quot;: [ //考虑执行计划 { &quot;plan_prefix&quot;: [ ] /* plan_prefix */, &quot;table&quot;: &quot;`student`&quot;, &quot;best_access_path&quot;: { //最佳访问路径 &quot;considered_access_paths&quot;: [ { &quot;rows_to_scan&quot;: 9, &quot;access_type&quot;: &quot;range&quot;, &quot;range_details&quot;: { &quot;used_index&quot;: &quot;PRIMARY&quot; } /* range_details */, &quot;resulting_rows&quot;: 9, &quot;cost&quot;: 2.81986, &quot;chosen&quot;: true } ] /* considered_access_paths */ } /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, //行过滤百分比 &quot;rows_for_plan&quot;: 9, &quot;cost_for_plan&quot;: 2.81986, &quot;chosen&quot;: true } ] /* considered_execution_plans */ }, { &quot;attaching_conditions_to_tables&quot;: { //将条件附加到表上 &quot;original_condition&quot;: &quot;(`student`.`id` &lt; 10)&quot;, &quot;attached_conditions_computation&quot;: [ ] /* attached_conditions_computation */, &quot;attached_conditions_summary&quot;: [ //附加条件概要 { &quot;table&quot;: &quot;`student`&quot;, &quot;attached&quot;: &quot;(`student`.`id` &lt; 10)&quot; } ] /* attached_conditions_summary */ } /* attaching_conditions_to_tables */ }, { &quot;finalizing_table_conditions&quot;: [ { &quot;table&quot;: &quot;`student`&quot;, &quot;original_table_condition&quot;: &quot;(`student`.`id` &lt; 10)&quot;, &quot;final_table_condition &quot;: &quot;(`student`.`id` &lt; 10)&quot; } ] /* finalizing_table_conditions */ }, { &quot;refine_plan&quot;: [ //精简计划 { &quot;table&quot;: &quot;`student`&quot; } ] /* refine_plan */ } ] /* steps */ } /* join_optimization */ }, { &quot;join_execution&quot;: { //执行 &quot;select#&quot;: 1, &quot;steps&quot;: [ ] /* steps */ } /* join_execution */ } ] /* steps */ } //第3部分：跟踪信息过长时，被截断的跟踪信息的字节数。 MISSING_BYTES_BEYOND_MAX_MEM_SIZE: 0 //丢失的超出最大容量的字节 //第4部分：执行跟踪语句的用户是否有查看对象的权限。当不具有权限时，该列信息为1且TRACE字段为空，一般在 调用带有SQL SECURITY DEFINER的视图或者是存储过程的情况下，会出现此问题。 INSUFFICIENT_PRIVILEGES: 0 //缺失权限 1 row in set (0.00 sec) 七、MySql监控分析视图-sys schema 关于MySql的性能监控和问题诊断，我们一般都从performance_schema中去获取想要的数据，在MySql5.7版本中新增sys schema，它讲performance_schema和information_schema中的数据以更容易理解的方式总结归纳为“视图”，其目的就是为了降低查询performation_schema的复杂度，让DBA能够快速的定位问题。 Sys schema视图摘要： 主机相关：以host_summary开头，主要汇总了IO延迟的信息。 Innodb相关：以innodb开头，汇总了innodb buffer信息和事务等待innodb锁的信息。 I/o相关：以io开头，汇总了等待I/O、I/O使用量情况。 内存使用情况：以memory开头，从主机、线程、事件等角度展示内存的使用情况 连接与会话信息：processlist和session相关视图，总结了会话相关信息。 表相关：以schema_table开头的视图，展示了表的统计信息。 索引信息：统计了索引的使用情况，包含冗余索引和未使用的索引情况。 语句相关：以statement开头，包含执行全表扫描、使用临时表、排序等的语句信息。 用户相关：以user开头的视图，统计了用户使用的文件I/O、执行语句统计信息。 等待事件相关信息：以wait开头，展示等待事件的延迟情况。 Sys schema视图摘要： -- 索引情况 #1. 查询冗余索引 select * from sys.schema_redundant_indexes; #2. 查询未使用过的索引 select * from sys.schema_unused_indexes; #3. 查询索引的使用情况 select index_name,rows_selected,rows_inserted,rows_updated,rows_deleted from sys.schema_index_statistics where table_schema='dbname'; -- 表相关 # 1. 查询表的访问量 select table_schema,table_name,sum(io_read_requests+io_write_requests) as io from sys.schema_table_statistics group by table_schema,table_name order by io desc; # 2. 查询占用bufferpool较多的表 select object_schema,object_name,allocated,data from sys.innodb_buffer_stats_by_table order by allocated limit 10; # 3. 查看表的全表扫描情况 select * from sys.statements_with_full_table_scans where db='dbname'; -- 语句相关 #1. 监控SQL执行的频率 select db,exec_count,query from sys.statement_analysis order by exec_count desc; #2. 监控使用了排序的SQL select db,exec_count,first_seen,last_seen,query from sys.statements_with_sorting limit 1; #3. 监控使用了临时表或者磁盘临时表的SQL select db,exec_count,tmp_tables,tmp_disk_tables,query from sys.statement_analysis where tmp_tables&gt;0 or tmp_disk_tables &gt;0 order by (tmp_tables+tmp_disk_tables) desc; -- IO相关 #1. 查看消耗磁盘IO的文件 select file,avg_read,avg_write,avg_read+avg_write as avg_io from sys.io_global_by_file_by_bytes order by avg_read limit 10; -- Innodb 相关 #1. 行锁阻塞情况 select * from sys.innodb_lock_waits; 风险提示： 通过sys库去查询时，MySql会消耗大量资源区收集相关信息，严重的可能会导致业务请求被阻塞，从而引起故障。建议生产上不要频繁的区查询sys或者performance_schema、information_schema来完成监控、巡检等工作。 小结 查询是数据库中最频繁的操作，提高查询速度可以有效地提高MySql数据库的性能。通过对查询语句的分析可以了解查询语句的执行情况，找出查询语句执行的瓶颈，从而优化查询语句。 ","link":"http://mofish.pily.life/post/mysql_learning_14/"},{"title":"MySql学习之路(十三)：InnoDB数据存储结构","content":"🤡索引结构给我们提供了高效数据索引方式，不过索引信息都是存储在文件上的，确切的说是存储在页结构中。 👉由于如今InnoDB是 MySql的默认存储引擎，所以就学习记录一下InnoDB存储引擎的数据存储结构！ 目录 一、数据库的存储结构：页 1.1 磁盘与内存交互的基本单位：页 1.2 页结构概述 1.3 页的大小 1.4 页的上层结构 二、InnoDB数据页结构 2.1 页结构 第一部分：File Header和File Trailer 第二部分：User Records、Free Space和Infimum + Supremum 第三部分：Page Directory和Page Header 2.2 从数据库页的角度看B+树如何查询 三、InnoDB行格式 3.1 COMPACT行格式 记录的额外信息 记录的真实数据 行溢出数据 3.2 Dynamic、Compressed行格式 四、区、段与碎片区 4.1 为什么要有区？ 4.2 为什么要有段？ 4.3 为什么要有碎片区？ 4.4 区的分类 五、表空间 5.1 独立表空间 5.2 系统表空间 一、数据库的存储结构：页 在InnoDB中，数据会存储在磁盘上，在真正处理数据时需要先将数据加载到内存，表中读取某些记录时，InnoDB存储引擎不需要一条一条的把记录从磁盘中读取出来，接下来细细讲解！💃💃 1.1 磁盘与内存交互的基本单位：页👈 InnnoDB将数据划分为若干个页，以页作为磁盘和内存之间数据交互的基本单位，大小默认为16K！ 在数据库中，无论是读取一行还是多行，都是将这行数据所在的页进行加载。也就是说，数据库管理存储空间的基本单位是页(Page)，数据库 I/O 操作的最小单位是页。 记录是按照行来存储的，一个页可以存储多行数据，而数据库的读取方式也是以页为单位，减少 I/O 次数。 1.2 页结构概述👈 页a、页b、页c...页n 这些页可以不在物理结构上相连，只要通过双向链表串联起来即可； 每个数据页中的记录会按照主键升序组成一个单向链表； 每个数据页都会为存储在它里面的记录生成一个页目录，再通过主键查找某条记录时，可以在页目录中使用二分法快速定位到对应的槽位，然后再遍历该槽位对应分组中的记录即可快速找到指定的记录。 1.3 页的大小 ## InnoDB存储引擎的默认页大小时 16KB show variables like '%innodb_page_size%' 1.4 页的上层结构 在数据库中，除了页（Page）之外，还存在着区（Extent）、段（Segment）和表空间（TableSpace）的概念，它们的关系如下： 区（Extent） 是比页大一级的存储结构，在 InnoDB 存储引擎中，一个区会分配64个连续的页，即一个区的大小是 64*16KB = 1MB。 段（Segment） 由一个或多个区组成，段中不要求区与区之间是相邻的。段是数据库中的分配单位，不同类型的数据库对象以不同的段形式存在。比如创建一张表时会创建一个表段，创建一个索引时会创建一个索引段。 表空间（TableSpace） 是一个逻辑容器，表空间存储的对象是一个或多个段，而且一个段只能属于一个表空间。数据库由一个或多个表空间组成，表空间从管理上可划分为系统表空间、用户表空间、撤销表空间、临时表空间等。 二、InnoDB数据页结构 页如果按照类型划分的话，常见的有数据页、系统表、Undo页和事务数据页等。数据页是我们最常使用的页。 2.1 页结构 数据页的16KB大小存储空间被划分为7个部分，分别是： 第一部分：File Header和File Trailer File Header 大小：38字节 作用：描述各种页的通用信息，如页的编号、上一页和下一页是谁等 名称 占用空间 描述 FIL_PAGE_SPACE_OR_CHKSUM 4字节 页的校验和，检查页的数据是否完整 FIL_PAGE_OFFSET 4字节 每一页都有一个独立的页号，InnoDB通过页号可以唯一定位一个页 FIL_PAGE_PREV 4字节 上一页的页号，确保页之间逻辑上是连续的 FIL_PAGE_NEXT 4字节 下一页的页号，确保页之间逻辑上是连续的 FIL_PAGE_LSN 8字节 页面被最后修改时对应的日志序列位置 FIL_PAGE_TYPE 2字节 标记当前页的类型，如Undo日志页、系统页、索引页等等 FIL_PAGE_FILE_FLUSH_LSN 8字节 仅在系统表空间的一个页中定义，代表文件至少被刷新到了对应的LSN值 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 4字节 页属于哪个表空间 File Trailer 大小：8字节 作用：校验页的完整性 名称 占用空间 描述 FIL_PAGE_SPACE_OR_CHKSUM 4字节 这部分与File Header中的校验和相对应 FIL_PAGE_FILE_FLUSH_LSN 4字节 也是为了校验页的完整性，如果首位的LSN值校验不成功，说明同步过程出现了问题，需要重新同步页数据 第二部分：User Records、Free Space和Infimum + Supremum User Records 大小： 不确定 作用： User Records中的这些记录按照指定的行格式一条一条摆在User Records部分，相互之间形成 单链表。 Free Space 大小：不确定 作用：一开始生成页的时候是没有User Records这个部分的，每当我们插入一条记录，都会从Free Space部分，也就是尚未使用的存储空间申请一个记录大小的空间划分到User Records部分。当Free Space的空间全部用完后，就需要去申请新的页。 Infimum + Supremum 大小：26字节 作用：InnoDB规定的最小记录与最大记录这两条记录的构造十分简单，都是由5字节大小的记录头信息和8字节大小的一个固定的部分组成的（图一）。 而且这两条记录不是我们自己定义的记录 ，所以它们并不存放在页的User Records部分，他们被单独放在一个称为Infimum + Supremum的部分（图二）。 第三部分：Page Directory和Page Header Page Directory（重点) 大小：不确定 作用：在页中，记录是以单向链表的形式进行存储的，虽然单向链表插入、删除非常方便，但是检索的效率不高，最差的情况需要遍历整个链表。因此在页页结构中专门设计了页目录，专门给记录做一个目录，通过二分查找法的方法是进行检索，提升效率。 问：页目录分组数是如何确定的？ 答： InnoDB规定对于最小记录所在的分组只能有1条记录，最大记录所在的分组拥有的记录条数只能在1~8条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间。 其步骤如下： 初始情况下一个数据页里只有最小记录和最大记录两条记录，它们分属于两个分组。 之后每插入一条记录，都会从页目录中找到主键值比本记录的主键值大并且差值最小的槽，然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8个。 在一个组中的记录数等于8个后再插入一条记录时，会将组中的记录拆分成两个组，一个组中4条记录，另一个5条记录。这个过程会在页目录中新增一个槽来记录这个新增分组中最大的那条记录的偏移量。 问：页目录结构下如何快速查找记录？ 答：在一个数据页中查找指定主键值的记录的过程分为两步 通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录。 Page Header 大小：56字节 作用：为了能得到一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等等，特意在页中定义了一个叫Page Header的部分，这个部分占用固定的56个字节，专门存储各种状态信息。 名称 占用空间 描述 PAGE_N_DIR_SLOTS 2字节 在页目录中的槽数量 PAGE_HEAP_TOP 2字节 还未使用的地址最小空间，也就是说该地址之后就是Free Space PAGE_N_HEAP 2字节 本页中的记录的数量（包括最大、最小和标记为已删除的记录） PAGE_FREE 2字节 第一个已经标记为删除的记录地址（各个已删除的记录通过next_record也会组成一个单链表，这个单链表中的记录可以被重新利用） PAGE_GARBAGE 2字节 已删除记录占用的字节数 PAGE_LAST_INSERT 2字节 最后插入记录的位置 PAGE_DIRECTION 2字节 记录插入方向 PAGE_N_DIRECTION 2字节 一个方向连续插入的记录数量 PAGE_N_RECS 2字节 本页中的记录的数量（不包括最大、最小和标记为已删除的记录） PAGE_MAX_TRX_ID 8字节 修改当前页的最大事务ID，该值在二级索引中定义 PAGE_LEVEL 2字节 当前页在B+树中所处的层数 PAGE_INDEX_ID 8字节 索引ID，表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10字节 B+树叶子段的头部信息，仅在B+树的Root页定义 PAGE_BTR_SEG_TOP 10字节 B+树非叶子段的头部信息，仅在B+树的Root页定义 2.2 从数据库页的角度看B+树如何查询 一颗B+树按照节点类型可以分为两部分： 叶子结点，B+树最底层的节点，存储具体行记录； 非叶子节点，存储索引键和页面指针，并不存储行记录本身。 当我们从页结构来理解 B+ 树的结构的时候，可以帮我们理解一些通过索引进行检索的原理： 问：B+树是如何进行记录检索的？ 答：从B+树的根开始，逐层检索，知道找到叶子节点，也就是找到对应的数据页为止，将数据页加载到内存中，页目录中的槽（slot）采用二分查找先找到一个粗略的记录分组，然后再在分组中通过链表遍历的方式查找记录。 问：普通索引和唯一索引在查询效率上有什么不同？ 答：唯一索引就是在普通索引上增加了约束性，也就是关键字唯一，找到了关键字就停止检索。而普通索引会存在关键字相同的行记录，由于根据页结构的原理，磁盘把数据加载到内存是一页一页的加载的，而不是找到一行加载一行，而一页中可能包含上千条记录。因此在普通索引的字段上查找也就是在内存中多几次“判断下一条记录”的操作，对CPU来说这些操作的消耗可以忽略不计，因为这两种索引在查询效率上基本什么差别。 三、InnoDB行格式 一行记录可以以不同的格式存在InnoDB中，行格式分别是： Redundant（5.0版本之前的默认行格式） Compact（5.1版本的默认行格式） Dynamic（5.7版本之后的默认行格式） Compressed（与Dynamic类似，不过会对存储在其中的行数据会以zlib的算法进行压缩） 我们可以在创建或修改表的语句中指定行格式： CREATE TABLE (列的信息) ROW_FORMAT= 行格式名称 ALTER TABLE ROW_FORMAT= 行格式名称 3.1 COMPACT行格式 在MySQL 5.1版本中，默认设置为Compact行格式。一条完整的记录其实可以被分为两大部分： 记录的额外信息 变长字段长度列表 NULL值列表 记录头信息 记录的真实数据 变长字段长度列表 MySQL支持一些变长的数据类型，比如VARCHAR(M)、VARBINARY(M)、TEXT类型，BLOB类型，这些数据类型修饰列称为变长字段 ，变长字段中存储多少字节的数据不是固定的，所以我们在存储真实数据的时候需要顺便把这些数据占用的字节数也存起来。 在Compact行格式中，把所有变长字段的真实数据占用的字节长度都存放在记录的开头部位，从而形成一个变长字段长度列表。 注意：这里面存储的变长长度和字段 顺序是反过来 的。比如两个varchar字段在表结构的顺序是a(10)，b(15)。那么在变长字段长度列表中存储的长度顺序就是15，10，是反过来的。 列名 存储内容 内容长度（十进制） 内容长度（十六进制） col1 'zhangsan' 8 0x08 col2 'lisi' 4 0x04 col3 'songhk' 6 0x06 把这个字节串组成的变长字段长度列表填入上边的示意图中的效果就是： NULL值列表 Compact行格式会把可以为NULL的列统一管理起来，存在一个标记为NULL值列表中。如果表中没有允许存储 NULL 的列，则 NULL值列表也不存在了。 二进制位的值为1时，代表该列的值为NULL 二进制位的值位0时，代表该列的值不为NULL 问：为什么要定义NULL值列表？ 答：之所以要存储NULL是因为数据都是需要对齐的，如果没有标注出来NULL值的位置，那么就有可能在查询数据的时候出现混乱。而且如果是用一个特定的符号放到相应的数据为表示NULL的话，虽然达到效果，但是每个NULL值位都需要存在这个特定的符号，有点浪费空间，所以就直接在行数据的头部开辟出一块空间专门用来记录该行数据中哪些是非空数据。 例如：字段col1、col2、col3、col4四个字段，其中col2是IS NOT NULL，其中col1=1、col3=null、col4=null。 因为col2是非NULL，所以NULL值列表的数据就会跳过col2 记录头信息 这些记录头信息中各个属性如下： 名称 大小（bit） 描述 预留位1 1 没有使用 预留位2 1 没有使用 delete_mask 1 标记该记录是否被删除，0未删除，1已删除 min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记 n_owned 4 页目录Page Directory中每个组中最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字段。 heap_no 13 表示当前记录在记录堆中的位置 record_type 13 表示当前记录的类型，0-普通记录，1-B+树非叶节点记录，2-最小记录，3-最大记录 next_record 16 表示下一条记录的相对位置 -- 插入数据： IINSERT INTO page_demo VALUES (1, 100, 'song'), (2, 200, 'tong'), (3, 300, 'zhan'), (4, 400, 'lisi'); 问：delete_mask中，被删除的记录为什么还在页中存储呢？ 答：你以为它删除了，可它还在真实的磁盘上。这些被删除的记录之所以不立即从磁盘上移除，是因为移除它们之后其他的记录在磁盘上需要重新排列，导致性能消耗 。所以只是打一个删除标记而已，所有被删除掉的记录都会组成一个所谓的垃圾链表 ，在这个链表中的记录占用的空间称之为可重用空间 ，之后如果有新记录插入到表中的话，可能把这些被删除的记录占用的存储空间覆盖掉。 问：怎么不见heap_no值为0和1的记录呢？ 答：MySQL会自动给每个页里加了两个记录，由于这两个记录并不是我们自己插入的，所以有时候也称为伪记录或者虚拟记录 。这两个伪记录一个代表最小记录，一个代表最大记录。最小记录和最大记录的heap_no值分别是0和1，也就是说它们的位置最靠前。 记录的真实数据 记录的真实数据除了我们自己定义的列的数据以外，还会有三个隐藏列： 列名 是否必须 占用空间（字节） 描述 DB_ROW_ID 否 6 行ID，唯一标识一条记录 DB_TRX_ID 是 6 事务ID DB_ROLL_PTR 是 7 回滚指针 一个表没有手动定义主键，则会选取一个Unique键作为主键，如果连Unique键都没有定义的话，则会为表默认添加一个名为row_id的隐藏列作为主键。所以row_id是在没有自定义主键以及Unique键的情况下才会存在的。 行溢出数据 一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65533个字节，这样就可能出现一个页存放不了一条记录，这种现象成为行溢出。 在Compact和Reduntant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会储存该列的一部分数据，把剩余的数据分散存储在几个其它的页中进行分页存储，然后记录的真实数据处用20个字节存储指向这些页的地址，从而可以找到剩余数据所在的页，这称为页的扩展。 3.2 Dynamic、Compressed行格式 在MySQL5.7和8.0中，默认行格式就是Dynamic， Dynamic、Compressed行格式 和Compact行格式挺像，只不过在处理行溢出数据时有分歧： Compressed和Dynamic两种记录格式对于存放在BLOB中的数据采用了完全的行溢出的方式。如图，在数据页中只存放20个字节的指针（溢出页的地址），实际的数据都存放在Off Page（溢出页）中。 Compressed行记录格式的另一个功能就是，存储在其中的行数据会以zlib的算法进行压缩，因此对于BLOB、TEXT、VARCHAR这类大长度类型的数据能够进行非常有效的存储。 四、区、段与碎片区 👇👇👇👇👇👇👇 4.1 为什么要有区？ B+树的每一层中的页都会形成一个双向链表，如果是以页为单位来分配存储空间的话，双向链表相邻的两个页之间的物理位置可能离得非常远，可能会导致所谓的随机I/O。 而磁盘的速度和内存的速度差了好几个数量级，随机I/O是非常慢的，所以我们应该尽量让链表中相邻的页的物理位置页相邻，这样进行范围查询的时候就可以使用所谓的顺序I/O。 引入区的概念，一个区就是再物理的位置上连续的64页。因为InnoDB中的页大小默认是16KB，所以一个区的大小是16*64=1MB。 在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区为单位分配，甚至可以一次性分配多个连续的区。虽然可能造成一点点空间的浪费（数据不足以填充满整个区），但是从性能的角度来看，可以消除很多的随机I/O，功大于过！ 4.2 为什么要有段？ 对于范围查询，其实是对B+树叶子节点中的记录进行顺序扫描，而如果不区分叶子节点和非叶子节点，通通把节点代表的页放到区中的话，进行范围扫描的效果就大打折扣了。 所以InnoDB对B+树的叶子节点和非叶子节点进行了区分，也就是说叶子节点有自己独有的区，非叶子节点亦是。而存放叶子几点的区的集合就算是一个段，存放非叶子节点的区的集合也是一个段，也就是说一个索引会生成两个段，一个叶子节点段，一个非叶子节点段。 除此之外，InnoDB中还有常见的数据段、索引段、回滚段。数据段即为B+树的叶子节点，索引段即为B+树的非叶子节点。 在InnoDB存储引擎中，段其实不对应表空间中某一个连续的物理区域，而是一个逻辑上的概念，由若干个零散的页面以及一些完整的区组成。 4.3 为什么要有碎片区？ InnoDB存储引擎中，一个表只有一个聚簇索引，因此会生成两个段，而段是以区为单位申请的存储中间，一个区默认占用1MB，所以默认情况下一个只存了几条记录的小表页占据了2M的存储空间了。那么以后每次添加一个索引都要多申请2M的存储空间么？这必定是空间的浪费！ 为了考虑以完整的区为单位分配给某个段对于数据来较小的表太浪费存储空间的这种情况，InnoDB提出了一个碎片区的概念：在一个碎片区中，并不是所有的页都是为了存储同一个段的数据而存在的，而是碎片区中的页可以用于不同的目的，比如有些页数据段A，有些页属于段B，有些也甚至哪个段都不属于。 碎片区直属于表空间，并不属于任何一个段。 所以段的分配存储空间的策略是这样的： 在刚开始向表中插入数据时，段时从某个碎片区以页为单位来分配存储空间的 当某个段已经占用了32个碎片区页面之后，就会申请以完整的区为单位来分配存储空间 所以现在段不能仅定义为某些区的集合，更准确来说应该时某些零散的页面以及一个完整的区的集合。 4.4 区的分类 空闲的区（FREE）：现在还没有用到这个区中的任何页面 有剩余空间的碎片区（FREE_FRAG）：表示碎片区中还有可用的页面 没有剩余空间的碎片区（FULL_FRAG）：表示碎片区中的所有页面都被使用，没有空闲页面 附属于某个段的区（FSEG）：每一个索引都可以分为叶子节点段和非叶子节点段 处于FREE、FREE_FRAG 以及 FULL_FRAG 这三种状态的区都是独立的，直属于表空间。而处于 FSEG 状态的区是附属于某个段的。 五、表空间 表空间可以看做事InnoDB存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。 表空间是一个逻辑容器，其存储的对象是段，在一个表空间中可以有一个或多个段，但是一个段只能属于一个表空间。 表空间从管理上可以划分为系统表空间、独立表空间、撤销表空间、临时表空间等。 5.1 独立表空间 独立表空间，即每张表有一个独立的表空间，也就是数据和索引信息都会保存在自己的表空间中。独立的表空间 (即：单表) 可以在不同的数据库之间进行 迁移。 空间可以回收 (DROP TABLE 操作可自动回收表空间；其他情况，表空间不能自己回收) 。如果对于统计分析或是日志表，删除大量数据后可以通过：alter table TableName engine=innodb; 回收不用的空间。对于使用独立表空间的表，不管怎么删除，表空间的碎片不会太严重的影响性能，而且还有机会处理。 5.2 系统表空间 系统表空间的结构和独立表空间基本类似，只不过由于整个MySQL进程只有一个系统表空间，在系统表空间中会额外记录一些有关整个系统信息的页面，这部分是独立表空间中没有的。 每当我们向一个表中插入一条记录时，MySQL都要校验相关信息是否符合插入条件： 先要校验一下插入语句对应的表存不存在，插入的列和表中的列是否符合，如果语法没有问题的话，还需要知道该表的聚簇索引和所有二级索引对应的根页面是哪个表空间的哪个页面，然后把记录插入对应索引的B+树中。所以说，MySQL除了保存着我们插入的用户数据之外，还需要保存许多额外的信息，比方说： 某个表属于哪个表空间，表里边有多少列 表对应的每一个列的类型是什么 该表有多少索引，每个索引对应哪儿个字段，该索引对应的根页面在哪个表空间的哪个页面 该表有哪些外键，外键对应哪个表的哪些列 某个表空间对应文件系统上文件路径是什么 ... 实际上是为了更好的管理我们这些用户数据而不得以引入的一些额外数据，这些数据页称为 元数据。InnoDB 存储引擎特意定义了一些列的 内部系统表 (internal system table) 来记录这些元数据： | 表名 | 描述 | | SYS_TABLES | 整个InnoDB存储引擎中所有的表的信息 | | SYS_COLUMNS | 整个InnoDB存储引擎中所有的列的信息 | | SYS_INDEXS | 整个InnoDB存储引擎中所有的索引的信息 | | SYS_FIELDS | 整个InnoDB存储引擎中所有的索引对应的列的信息 | | SYS_FOREIGN | 整个InnoDB存储引擎中所有的外键的信息 | | SYS_FOREIGN_COLS | 整个InnoDB存储引擎中所有的外键对应的列的信息 | | SYS_TABLESPACES | 整个InnoDB存储引擎中所有表空间信息 | | SYS_DATAFILES | 整个InnoDB存储引擎中所有表空间对应文件系统的文件路径信息 | | SYS_VIRTUAL | 整个InnoDB存储引擎中所有的虚拟生成列的信息 | 这些系统表也称为 数据字典，它们都是以 B+ 树的形式保存在系统表空间的某个页面中。其中 SYS_TABLES、SYS_COLUMNS、SYS_INDEXES、SYS_FIELDS 这四个表尤其重要，称之为基本系统表 (basic system tables) ： ","link":"http://mofish.pily.life/post/mysql_learning_13/"},{"title":"讲解laravel框架中的控制反转（IoC）和依赖注入（DI）","content":"👏 之前已经讲过了 IoC 和 DI 的概念，这一章节虽然也是讲这两个，但是主要是结合 laravel框架来讲解，看看 laravel 中的 IoC 和 DI 是如何使用的。 目录 一、什么是IoC 和 DI？ 二、目的 三、原生代码实现 3.1 传统写法 3.2 采用IoC和DI的思想来实现 四、对IoC和DI的本质分析 五、Laravel框架IoC核心源码 5.1 绑定 5.2 解析 一、什么是IoC 和 DI？ IoC（Inversion of Control）控制反转 IoC 是一个理论，一个指导思想。指导开发人员如何使用对象去管理对象。把对象的创建、属性赋值、对象的生命周期都交给代码之外的容器管理。 IoC分为控制和反转： 控制：对象创建、属性赋值、对象生命周期的管理。 正转：开发人员在代码中，使用new构造方法创建对象，开发人员掌握了对象的创建、属性赋值、对象从开始到销毁的全部过程，开发人员对对象全部控制。 反转：把开发人员管理对象的权限转移给代码之外的容器来实现，由容器来完成对对象的管理。 简单的说，就是在使用对象时，由主动 new 产生对象转换成从外部提供对象，在这个过程中，对象的创建控制权由程序转移道外部。 DI（Dependency Injection）依赖注入 DI 是 IoC 的一种技术实现。程序只需要提供要使用的对象名称即可，对象如何创建，如何从容器中查找、获取都由容器内部自己实现。 通俗解释 小明有钱了，想自己建个房子，但是自己建的话又很麻烦很乱，于是就找到房产中介（IoC容器）买了房子（DI依赖注入），因此小明不用自己建，直接找中介就得到了想要的房子。 小明依赖房子，但是他从自己盖房子（自己控制房子）到找中介买房子（让中介控制房子），这就叫做控制反转，而中介根据小明的要求，直接把房子提供给小明，这就叫做依赖注入。 当然，这个房子不是中介建设的，而是开发商建设的，这个开发商就是服务提供者，中介只是负责把房子挂牌出售，便于提供给需要的人。 二、目的 采用IOC思想和DI设计模式，主要目的是：解耦，简化代码、便于管理。 不用每个方法里面都 new new new 那样子，要修改的时候就需要到处搜索修改，不仅麻烦，还容易改漏。 三、原生代码实现 3.1 传统写法 &lt;?php /** * 购房者 */ class User { private $userName; public function __construct($userName) { $this-&gt;userName = $userName; } public function buyHouse() { $house = new House('0001', '三室一厅', '120平方米'); echo '我是'.$this-&gt;userName.&quot;\\r\\n&quot;; echo '我买了'. $house-&gt;getHouseType(). '的房子了'.&quot;\\r\\n&quot;; echo '我买了'. $house-&gt;getHouseArea(). '的房子了'.&quot;\\r\\n&quot;; } } /** * 商品房 */ class House { private $houseNo; private $houseType; private $houseArea; public function __construct($houseNo, $houseType, $houseArea) { $this-&gt;houseNo = $houseNo; $this-&gt;houseType = $houseType; $this-&gt;houseArea = $houseArea; } public function getHouseType() { return $this-&gt;houseType; } public function getHouseArea() { return $this-&gt;houseArea; } } $user = new User('小明'); $user-&gt;buyHouse(); &gt; 以上代码输出结果为： 我是小明 我买了三室一厅的房子了 我买了120平方米的房子了 3.2采用IoC和DI的思想来实现 /** * Class 购房者 */ class User { private $userName; public function __construct($userName) { $this-&gt;userName = $userName; } public function buyHouse(House $house) { echo '我是'.$this-&gt;userName.&quot;\\r\\n&quot;; echo '我买了'. $house-&gt;getHouseType(). '的房子了'.&quot;\\r\\n&quot;; echo '我买了'. $house-&gt;getHouseArea(). '的房子了'.&quot;\\r\\n&quot;; } } /** * 商品房 */ class House { private $houseNo; private $houseType; private $houseArea; public function __construct($houseNo, $houseType, $houseArea) { $this-&gt;houseNo = $houseNo; $this-&gt;houseType = $houseType; $this-&gt;houseArea = $houseArea; } public function getHouseType() { return $this-&gt;houseType; } public function getHouseArea() { return $this-&gt;houseArea; } } /** * 房产中介，就是我们讲的ioc * Class 房产中介 */ class 房产中介 { private $在售房源 = [];//这个类似于laravel的Container对象中的$bindings private $认筹房源 = [];//类似于laravel的Container对象中的$resolved private $公租房 = [];//类似于laravel的Container对象中的$instances private $网红房源 = [];//类似于laravel的Container对象中的$aliases / $abstractAliases private $意向购房群体 = []; public function 预售登记($户型, $详细信息) { $this-&gt;在售房源[$户型] = $详细信息; } public function 获取在售房源($户型) { return ($this-&gt;在售房源[$户型])();//因为是闭包，所以，要增加()来执行闭包函数 } public function 意向登记($意向人, $个人信息) { $this-&gt;意向购房群体[$意向人] = $个人信息; } public function 获取意向人信息($意向人) { return ($this-&gt;意向购房群体[$意向人])();//因为是闭包，所以，要增加()来执行闭包函数 } } $app = new 房产中介(); $app-&gt;预售登记('三室一厅', function(){ return new House('1001', '三室一厅', '100平方米'); }); $app-&gt;预售登记('四室两厅', function(){ return new House('1002', '四室两厅', '150平方米'); }); $app-&gt;意向登记('小明', function(){ return new User('小明'); }); $app-&gt;意向登记('张三', function(){ return new User('张三'); }); //echo $app-&gt;获取意向人信息('小明')-&gt;买房($app-&gt;获取在售房源('四室两厅')); $user = $app-&gt;获取意向人信息('小明'); $house = $app-&gt;获取在售房源('四室两厅'); $user-&gt;buyHouse($house); 以上代码输出结果为： 我是小明 我买了三室一厅的房子了 我买了120平方米的房子了 四、对IoC和DI的本质分析 从上面的代码我们可以看到，房产中介作为IoC，其实本质就是一个数组（一维或者多维）。 其实，在laravel框架中，Container对象中的属性$bindings 、$resolved 、$instances 、$aliases、$abstractAliases 其实也是从不同维度来管理注册到容器中的对象的。 上面的例子中，如果从业务逻辑角度来讲，无非就是购房者买房，主要的类有：购房者、商品房。如果按照传统代码来实现的话，那么购房者对象对商品房对象的依赖是强依赖，因为在购房者类中需要new 商品房类()。 而如果采用 IoC和DI 的思想来实现的话，增加了房产中介这个 IoC 容器，商品房首先在房产中介那边进行一下预售登记，购房者也在房产中介那边进行一下意向登记，购房者对象需要依赖商品房对象，采用依赖注入。 即：让房产中介直接把实例化后到商品房对象注入到购房者对象，购房者对象无需关注怎么实例化，只管拿过来用就行。 五、Laravel框架IoC核心源码 5.1 绑定 我们来简单看下laravel框架的核心容器的绑定是怎么实现的，以及三种绑定模式的区别。 以下代码都是在Illuminate\\Container\\Container类中 bind绑定 /** * Register a binding with the container. * * @param string $abstract * @param \\Closure|string|null $concrete * @param bool $shared * @return void */ public function bind($abstract, $concrete = null, $shared = false) { //移除旧的实例 $this-&gt;dropStaleInstances($abstract); // If no concrete type was given, we will simply set the concrete type to the // abstract type. After that, the concrete type to be registered as shared // without being forced to state their classes in both of the parameters. if (is_null($concrete)) { $concrete = $abstract; } // If the factory is not a Closure, it means it is just a class name which is // bound into this container to the abstract type and we will just wrap it // up inside its own Closure to give us more convenience when extending. if (! $concrete instanceof Closure) { $concrete = $this-&gt;getClosure($abstract, $concrete); } // 创建一个包含变量与其值的数组。 //对每个参数，compact() 在当前的符号表中查找该变量名并将它添加到输出的数组中，变量名成为键名而变量的内容成为该键的值。简单说，它做的事和 extract() 正好相反。返回将所有变量添加进去后的数组。 $this-&gt;bindings[$abstract] = compact('concrete', 'shared'); // If the abstract type was already resolved in this container we'll fire the // rebound listener so that any objects which have already gotten resolved // can have their copy of the object updated via the listener callbacks. //检查是否解析过 或则 已经存在所要绑定对象对应的实例 if ($this-&gt;resolved($abstract)) { $this-&gt;rebound($abstract); } } 如何使用bind方法来将对象注册绑定到容器中呢？如下，bind方法是将闭包绑定到容器当中 $this-&gt;app-&gt;bind('User\\API', function ($app) { return new User\\API($app-&gt;make('UserLogin')); }); 同样，bind方法会先删除旧的实例，然后再新的实例放入闭包中，再绑定到容器中。如果第二个参数不是闭包，会通过getClosure方法将类名封装到闭包中，然后在闭包中通过make方法或build方法解析绑定的类。绑定时会将闭包和是否是shared放入$this-&gt;bind[]数组中，解析时调用。 singleton绑定 /** * Register a shared binding in the container. * 绑定到容器的对象只会被解析一次，之后的调用都返回相同的实例： * @param string|array $abstract * @param \\Closure|string|null $concrete * @return void */ public function singleton($abstract, $concrete = null) { $this-&gt;bind($abstract, $concrete, true); } 从官方的代码可以看出，singleton方法，最终还是调用了$this-&gt;bind()方法，只是通过singleton()方法绑定到容器的对象只会被解析一次，之后的调用都返回相同的实例，这就是所谓的单例。 instance绑定 /** * Register an existing instance as shared in the container. * * @param string $abstract * @param mixed $instance * @return mixed */ public function instance($abstract, $instance) { //移除已经存在的抽象的别名 $this-&gt;removeAbstractAlias($abstract); //检查是否已经存在该别名的绑定的对象 $isBound = $this-&gt;bound($abstract); unset($this-&gt;aliases[$abstract]); // We'll check to determine if this type has been bound before, and if it has // we will fire the rebound callbacks registered with the container and it // can be updated with consuming classes that have gotten resolved here. $this-&gt;instances[$abstract] = $instance; // 判断是否已经绑定 if ($isBound) { $this-&gt;rebound($abstract); } return $instance; } 可以看到在instance方法中，首先移除已经存在的相同的别名，然后将对象存入$this-&gt;instance 数组中。然后完成了绑定。 使用该方法注册绑定到容器中的对象实例是共享的。 结论 bind：不提前占用内存, 待到调用的时候调用才会进行实例化, 每次都new一个新实例。 singletion：需要提前占用内存, 也就是需要先提前实例化出一个对象, 之后的每次调用都是调用这个实例。 instance：不提前占用内存, 待到调用的时候, 待到调用的时候调用才会进行实例化, 每次返回的都是同一个实例。 5.2 解析 /** * Resolve the given type from the container. * * @param string $abstract * @param array $parameters * @return mixed * * @throws \\Illuminate\\Contracts\\Container\\BindingResolutionException */ public function make($abstract, array $parameters = []) { return $this-&gt;resolve($abstract, $parameters); } /** * Resolve the given type from the container. * * @param string $abstract * @param array $parameters * @param bool $raiseEvents * @return mixed * * @throws \\Illuminate\\Contracts\\Container\\BindingResolutionException */ protected function resolve($abstract, $parameters = [], $raiseEvents = true) { $abstract = $this-&gt;getAlias($abstract); $needsContextualBuild = ! empty($parameters) || ! is_null( $this-&gt;getContextualConcrete($abstract) ); // If an instance of the type is currently being managed as a singleton we'll // just return an existing instance instead of instantiating new instances // so the developer can keep using the same objects instance every time. if (isset($this-&gt;instances[$abstract]) &amp;&amp; ! $needsContextualBuild) { return $this-&gt;instances[$abstract]; } $this-&gt;with[] = $parameters; $concrete = $this-&gt;getConcrete($abstract); // We're ready to instantiate an instance of the concrete type registered for // the binding. This will instantiate the types, as well as resolve any of // its &quot;nested&quot; dependencies recursively until all have gotten resolved. if ($this-&gt;isBuildable($concrete, $abstract)) { $object = $this-&gt;build($concrete); } else { $object = $this-&gt;make($concrete); } // If we defined any extenders for this type, we'll need to spin through them // and apply them to the object being built. This allows for the extension // of services, such as changing configuration or decorating the object. foreach ($this-&gt;getExtenders($abstract) as $extender) { $object = $extender($object, $this); } // If the requested type is registered as a singleton we'll want to cache off // the instances in &quot;memory&quot; so we can return it later without creating an // entirely new instance of an object on each subsequent request for it. if ($this-&gt;isShared($abstract) &amp;&amp; ! $needsContextualBuild) { $this-&gt;instances[$abstract] = $object; } if ($raiseEvents) { $this-&gt;fireResolvingCallbacks($abstract, $object); } // Before returning, we will also set the resolved flag to &quot;true&quot; and pop off // the parameter overrides for this build. After those two things are done // we will be ready to return back the fully constructed class instance. $this-&gt;resolved[$abstract] = true; array_pop($this-&gt;with); return $object; } /** * Instantiate a concrete instance of the given type. * * @param string $concrete * @return mixed * * @throws \\Illuminate\\Contracts\\Container\\BindingResolutionException */ public function build($concrete) { // If the concrete type is actually a Closure, we will just execute it and // hand back the results of the functions, which allows functions to be // used as resolvers for more fine-tuned resolution of these objects. if ($concrete instanceof Closure) { return $concrete($this, $this-&gt;getLastParameterOverride()); } try { $reflector = new ReflectionClass($concrete); } catch (ReflectionException $e) { throw new BindingResolutionException(&quot;Target class [$concrete] does not exist.&quot;, 0, $e); } // If the type is not instantiable, the developer is attempting to resolve // an abstract type such as an Interface or Abstract Class and there is // no binding registered for the abstractions so we need to bail out. if (! $reflector-&gt;isInstantiable()) { return $this-&gt;notInstantiable($concrete); } $this-&gt;buildStack[] = $concrete; $constructor = $reflector-&gt;getConstructor(); // If there are no constructors, that means there are no dependencies then // we can just resolve the instances of the objects right away, without // resolving any other types or dependencies out of these containers. if (is_null($constructor)) { array_pop($this-&gt;buildStack); return new $concrete; } $dependencies = $constructor-&gt;getParameters(); // Once we have all the constructor's parameters we can create each of the // dependency instances and then use the reflection instances to make a // new instance of this class, injecting the created dependencies in. try { $instances = $this-&gt;resolveDependencies($dependencies); } catch (BindingResolutionException $e) { array_pop($this-&gt;buildStack); throw $e; } array_pop($this-&gt;buildStack); return $reflector-&gt;newInstanceArgs($instances); } 从上面的第三段代码build()方法中可以看出，解析时，如果绑定注册的是闭包函数，那么就是直接返回闭包函数的执行，关键代码如下 if ($concrete instanceof Closure) { return $concrete($this, $this-&gt;getLastParameterOverride()); } 如果绑定注册的是类名，那么就利用php的反射（ReflectionClass）来实例化对象，并返回给调用者。bind()方法剩下的代码干的就是这个工作。 最后，在resolve()方法中的 $this-&gt;resolved[$abstract] = true; 代表已经解析成功了。 ","link":"http://mofish.pily.life/post/jiang-jie-laravel-kuang-jia-zhong-de-ioc-he-di/"},{"title":"讲讲AOP与OOP有什么区别？（已迁移）","content":"🤡 面试常见问题了，因此记录一下AOP 和 OOP的相关知识。 一、AOP概念 AOP（Aspect Oriented Programming），意为面向切面编程，通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。 AOP 是 OOP 的延续，是函数式编程的一种衍生泛型。利用 AOP 可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各个部分之间耦合度降低，提高程序的可重用性，提高开发效率。 二、AOP 和 OOP 的关系 OOP（面向对象编程），是针对业务处理过程中的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。 而 OOP 有一个明显的缺点就是关注点聚焦时，面向对象无法解决这个问题，一个关注点时面向所有而不是单一的类，不受类的边界的约束，因为OOP无法将关注点聚焦来解决，只能分散到各个类中。 AOP（面向切面编程）则是针对业务处理过程中的切面进行提取，它所面对的是处理过程中的某个步骤或阶段，以获得逻辑过程中各部分之间低耦合性的隔离效果。 这两种设计思想在目标上有着本质的差异。AOP并不是与OOP对立的，而是为了弥补OOP的不足。OOP解决了竖向的问题，AOP解决了横向的问题。 简单的讲，AOP是一种在不改变原来代码的基础上，通过”动态注入“代码，来改变原来执行结果的技术。 OOP 抽象的是对象，AOP 抽象的是场景 三、AOP的主要应用场景 日志记录、性能统计、安全控制、事务处理、异常处理等等。 四、主要目标 将日志记录，性能统计，安全控制，事务处理，异常处理等代码从业务逻辑代码中划分出来，通过对这些行为的分离，我们希望可以将它们独立到非指导业务逻辑的方法中，进而改变这些行为的时候不影响业务逻辑的代码。 如Laravel中路由中间件的使用。 五、对比 AOP（面向切面编程） OOP（面向对象编程） 定义 通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。 针对业务处理过程的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。 注重方面不同 AOP偏重业务处理过程的某个步骤或阶段。 OOP注重业务逻辑单元的划分。 使用方式 AOP在运行时动态地将代码切入到类的指定方法、指定位置上，从而提高了代码的实用性。 OOP面向对象的三大特征：封装,继承,多态。将功能分散到不同的对象中去，让不同的类设计不同的方法，降低代码的复杂程度，提高类的复用性，但也增加了代码的重复性。 面向目标不同 AOP希望能够将通用需求功能从不相关的类当中分离出来，能够使得很多类共享一个行为，一旦发生变化，不必修改很多类，而只需要修改这个行为即可。 OOP关注将需求功能划分为不同的并且相对独立，封装良好的类，并让它们有着属于自己的行为，依靠继承和多态等来定义彼此的关系。 总结 封装的是业务，将主业务和通用业务区分开，将通用业务划分为切面，切面又划分为通知和切入点。 封装的是方法和属性，以对象为最小操作单元，属性和方法都要通过对象才能调用。 ","link":"http://mofish.pily.life/post/jiang-jiang-aop-yu-oop-you-shi-me-qu-bie/"},{"title":"一文读懂CGI、FastCGI、php-cgi、php-fpm的区别（已迁移）","content":"😣 经典面试问题了，所以简单记录一下。 前言 在CGI诞生之前Web服务器负责静态文件的存储、查找及响应,此时的服务器还不能处理php或asp此类文件。 随着人们对于网站的要求越来越高，出现了动态技术，此时服务器依然不能直接运行php文件，虽然自己没有办法识别，缺可以将识别的过程交给别的程序完成。 对于服务器与这个程序之间，我们需要一些规则来进行约定，这个约定便是 CGI协议。 CGI协议是Web服务器与CGI程序之间传递信息的接口标准，当Web服务器获取到客户端提交的数据后，通过CGI接口转交给CGI程序处理后再返回给客户端。 CGI CGI 全称是“公共网关接口”（Common GateWay Interface），是一种通用网关接口规范，该规范详细描述了 Web Server（如：nginx）和后端应用服务器（如：php-cgi）在获取及返回数据过程中传输数据的标准。 简单的说就是 Web Server 与 Web Application 之间数据交换的一种协议。 PHP-CGI PHP-CGI 支持 CGI协议 的 php解释器，是一个 CGI程序，即是 PHP （Web Application）对 Web Server 提供的 CGI 协议的接口程序。 工作原理： 当用户请求Web服务器的动态脚本 Web服务器fork出一个新的进程启动CGI程序[启动的过程中需要加载配置、扩展等],将动态脚本交给CGI程序处理 CGI程序启动后解析动态脚本 将结果返回Web服务器 Web服务器将结果返回客户端，fork的进程关闭 缺点： 每次有了动态脚本处理的请求，都需要fork新进程，处理完毕后也要进行关闭，效率非常低下。 FastCGI FastCGI（Fast Common Gateway Interface）快速通用网关接口，是 CGI 的增强版本。 它的诞生就是为了减轻Web服务器与CGI程序的交互负载，使得服务器可以同时处理更多的请求。 PHP-FPM PHP-FPM 支持 FastCGI协议 的 php解释器，是一个 FastCGI程序，即是 PHP （Web Application）对 Web Server 提供的 FastCGI 协议的接口程序，而且还额外提供了一些相对智能的任务管理功能。 众所周知，CGI 程序的反复加载是 CGI 性能低下的主要原因，如果CGI进程保持在内存中，并接受 FastCGI 进程管理器调度，则可以提供良好的性能、伸缩性、Fail-Over 特性等等。 FastCGI 像是一个常驻(long-live)型的 CGI，它可以一直执行着，只要激活后，不会每次都要花费时间去 fork 一次。 工作原理： 客户端请求Web服务器的动态脚本 服务器将之交给 FastCGI master 进程 FastCGI master 进程 安排空闲 worker进程 解析脚本 随后处理结果返回服务器 服务器返回客户 上面的 worker进程 并不会关闭，而是继续等待 master进程 分配任务 PHP-FPM对进程的管理： 首先，先启一个master，解析配置文件，初始化执行环境，然后再启动多个worker。 当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。 而且当worker不够用时，master可以根据配置预先启动几个worker等着；当然空闲worker太多时，也会停掉一些，这样就提高了性能，也节约了资源。 这就是php-fpm的对进程的管理 CGI 与 FastCGI 的区别 CGI 模式中，直接杀死 php-cgi 进程，php就不能运行了。FastCGI 模式就没有这个问题，守护进程会平滑重新生成新的 php-cgi 子进程。 对于 CGI 来说，每一个 Web 请求 PHP 都必须重新解析 php.ini、重新载入全部扩展，并重新初始化全部数据结构。而使用 FastCGI，所有这些都只在进程启动时发生一次。一个额外的好处是，持续数据库连接（Persistent database connection）可以工作。 修改 php.ini 之后，php-cgi 进程无法平滑重启。php-fpm 对此的处理机制是新的 worker 用新的配置，已经存在的 worker 处理完手上的活就不再接受新的请求并准备退出，通过这种机制来平滑过度。 由于 FastCGI 是多进程，所以比 CGI 消耗更多的服务器内存。 PHP-CGI和PHP-FPM的关系（特别注意） php5.4 前后分别是两种不同的关系 php5.4之前，php-fpm(第三方编译)是管理器，php-cgi是解析器 php5.4之后，php-fpm(官方自带)，master 与 pool 模式。此时的php-fpm 和 php-cgi 没有关系了。php-fpm即是解析器，又是管理器。 ","link":"http://mofish.pily.life/post/yi-wen-du-dong-cgifastcgiphp-cgiphp-fpm-de-qu-bie/"},{"title":"PHP中的opcache是什么，opcache是用来干嘛的？（已迁移）","content":"PHP项目中，尤其是在高并发大流量的场景中，如何提升PHP的响应时间，是一项十分重要的工作。而opcache又是优化PHP性能不可缺失的组件，尤其是应用了PHP框架的项目中，作用更是明显🤑 opcache从字面意思，肯定是缓存这一块的，那么它的工作原理是什么呢😲 目录 一、概述 1.1、PHP-FPM + Nginx 的工作机制 1.2、PHP脚本解释执行的机制 二、opcache 介绍 三、opcache 原理 四、opcache 缓存解读 五、opcache 更新策略 六、opcache 配置 一、概述 在理解 opcache 功能之前，我们有必要先理解PHP-FPM + Nginx 的工作机制，以及PHP脚本解释执行的机制。 1.1、PHP-FPM + Nginx 的工作机制 请求从 Web浏览器 到 Nginx，再到 PHP处理 完成，一共要经历如下五个步骤： 第一步：启动服务 启动PHP-FPM。PHP-FPM 支持两种通信模式：TCP socket和Unix socket PHP-FPM 会启动两种类型的进程：master 进程 和 worker 进程，前者负责监控端口、分配任务、管理worker进程；后者就是PHP的cgi程序，负责解释编译执行PHP脚本 启动 Nginx。首先会载入 ngx_http_fastcgi_module 模块，初始化 FastCGI 执行环境，实现 FastCGI协议 请求代理 这里注意：FastCGI 的 worker 进程(cgi进程)，是由 PHP-FPM 来管理的，不是 Nginx 负责，Nginx只是代理。 第二步：Nginx 接收请求 Nginx 接收到请求后，基于 location 配置选择一个合适的 handler（这里就是代理 PHP 的handler） 第三步：Nginx 转发请求到 PHP-FPM Nginx 把请求翻译成 fastcgi 请求 通过 TCP socket / Unix socket 发送给 PHP-FPM 的 master 进程 第四步：PHP-FPM master 分配任务到 worker PHP-FPM master进程接收到请求 分配任务到 worker进程执行 PHP脚本，如果没有空闲的 worker，返回 502 错误 worker（cgi程序）进程执行 PHP脚本，如果超时，返回 504 错误 处理结束，返回结果 第五步：PHP-FPM worker 返回处理结果 PHP-FPM worker 进程返回处理结果，并关闭连接，等待下一个请求 PHP-FPM Master 进程通过 socket 返回处理结果 Nginx 再把处理结果返回给客户端 1.2、PHP脚本解释执行的机制 了解了PHP + Nginx 整体的处理流程后，我们接下来看一下PHP脚本具体执行流： PHP 初始化执行环节，启动Zend引擎，加载注册的扩展模块 初始化后读取脚本文件，Zend引擎 对脚本进行词法和语法分析，生成语法树 Zend引擎 编译语法树，生成opcode操作码 Zend 引擎执行opcode，返回执行结果 在PHP cli模式下，每次执行PHP脚本，四个步骤都会依次执行一遍； 在PHP-FPM模式下，步骤①)在PHP-FPM启动时执行一次，后续的请求中不再执行；步骤②~④每个请求都要执行一遍； 其实步骤②和③生成的语法树和opcode，同一个PHP脚本每次运行的结果都是一样的，在PHP-FPM模式下，每次请求都要处理一遍，是对系统资源极大的浪费，而opcache就是一个有效的优化方法！ 二、opcache 介绍 OPCache 是Zend官方出品的，开放自由的 opcode 缓存扩展，还具有代码优化功能，省去了每次加载和解析 PHP 脚本的开销。 PHP 5.5.0 及后续版本中已经绑定了 OPcache 扩展。 缓存两类内容： opcode interned string，如注释、变量名等 三、opcache 原理 opcache 缓存的机制主要是将编译好的操作码opcode放入共享内存中，提供给其它进程访问。 依据PHP字节码缓存的场景，opcache的内存管理设计非常简单，快速读写，不释放内存，过期数据置为wasted。 当 wasted 内存大于设定值时，自动重启 opcache 机制，清空并重新生成缓存。 四、opcache 缓存解读 opcache 是官方的 opcode 缓存解决方案，在PHP5.5版本之后，已经打包到PHP源码中一起发布。 它将PHP编译产生的字节码以及数据缓存到共享内存中, 在每次请求，从缓存中直接读取编译后的opcode，进行执行。通过节省脚本的编译过程，提高PHP的运行效率。 4.1 opcode 缓存 opcache 会缓存 opcode以及如下内容： PHP脚本涉及到的函数 PHP脚本中定义的Class PHP脚本文件路径 PHP脚本oparray（这个OPArray中就包含了所有的 opcode，Zend引擎就是从这个数组中取出opcode，一个接一个地执行） PHP脚本自身结构/内容 4.2 interned string 缓存 在PHP5.4的时候, 引入了Interned String机制, 用于优化PHP对字符串的存储和处理。尤其是处理大块的字符串，比如PHP doces时，Interned String 可以优化内存。 Interned String 缓存的内容包括：变量名称、类名、方法名、字符串、注释等。 在PHP-FPM模式中，Interned String 缓存字符，仅限于Worker 进程内部。而缓存到OPCache中，那么Worker进程之间可以使用 Interned String 缓存的字符串，节省内存。 我们需要注意一个事情，在PHP开发中，一般会有大段的注释，也会被缓存到OPCache中。可以通过php.ini的配置，关闭注释的缓存。 五、opcache更新策略 只要是缓存，都会存在过期以及更新策略。而 opcache 的更新策略非常简单，到期数据设为 wasted ,当 wasted 达到设定值时，清空缓存，重建缓存。 注意： 在高流量的场景下，重建缓存是一件非常消耗资源的事，opcache 在创建缓存时并不会组织进程读取，这会导致大量进程反复新建缓存。 所以，尽量不要设置 opcache 过期时间！ 每次发布新代码时，都会出现反复新建缓存的情况，如何避免？ 不要再高峰期发布代码 代码预热，比如使用脚本批量掉PHP访问URL，提前缓存 六、opcache 配置 6.1 内存配置 opcache.preferred_memory_model=&quot;mmap&quot; 。opcache 首选的内存模块。如果留空，opcache 会选择适用的模块， 通常情况下，自动选择就可以满足需求。可选值包括：mmap，shm, posix 以及 win32。 opcache.memory_consumption=64 opcache 的共享内存大小，以兆字节为单位，默认64M opcache.interned_strings_buffer=4 用来存储临时字符串的内存大小，以兆字节为单位，默认4M opcache.max_wasted_percentage=5 浪费内存的上限，以百分比计。如果达到此上限，那么 opcache 将产生重新启动续发事件。默认5 6.2 允许缓存的文件数量以及大小 opcache.max_accelerated_files=2000 OPcache 哈希表中可存储的脚本文件数量上限。 opcache.max_file_size=0 以字节为单位的缓存的文件大小上限。设置为 0 表示缓存全部文件。默认值0 6.3 注释相关的缓存 opcache.load_commentsboolean 如果禁用，则即使文件中包含注释，也不会加载这些注释内容。本选项可以和 opcache.save_comments 一起使用，以实现按需加载注释内容。 opcache.fast_shutdown boolean 如果启用，则会使用快速停止续发事件。所谓快速停止续发事件是指依赖 Zend 引擎的内存管理模块 一次释放全部请求变量的内存，而不是依次释放每一个已分配的内存块。 6.4 二级缓存的设置 opcache.file_cache 配置二级缓存目录并启用二级缓存。启用二级缓存可以在 SHM 内存满了、服务器重启或者重置 SHM 的时候提高性能。默认值为空字符串 &quot;&quot;，表示禁用基于文件的缓存。 opcache.file_cache_onlyboolean 启用或禁用在共享内存中的 opcode 缓存。 opcache.file_cache_consistency_checksboolean 当从文件缓存中加载脚本的时候，是否对文件的校验和进行验证。 opcache.file_cache_fallbackboolean 在 Windows 平台上，当一个进程无法附加到共享内存的时候， 使用基于文件的缓存，也即：opcache.file_cache_only=1。需要显示的启用文件缓存。 ","link":"http://mofish.pily.life/post/php-zhong-de-opcache-shi-shi-me-opcache-shi-yong-lai-gan-ma-de/"},{"title":"使用 Yield 生成器来善待你的内存（已迁移）","content":"很多PHP开发者或许都不知道生成器这个功能，可能是因为生成器是PHP 5.5.0才引入的功能，也可以是生成器作用不是很明显，但是生成器功能的确非常有用，可以很有效的节省内存。 https://mp.weixin.qq.com/s/sxCgLJyGms9nztTDicglQQ https://mp.weixin.qq.com/s/QBgZKIoIotA_QD0flkaWHg 一、 什么是 &quot;yield&quot;？ 生成器函数看起来和普通函数一样，只是生成器不返回值，而是生成所需的值。 看看以下示例： &lt;? function getValues() { yield 'value'; } // 输出字符串 &quot;value&quot; echo getValues(); 当然， 这不是它生效的方式， 前面的例子会给你一个致命的错误： 类生成器的对象不能被转换成字符串。 二、yield 解决什么问题？ 解决运行内存的瓶颈！ 用过的地方大概就是 excel 文件导出的时候了，因为要遍历大量处理，并且要 foreach 去处理每一项处理，如果等全部处理完再 return，那么就是弹出内存不足的提示： Fatal Error: Allowed memory size of xxxxxx bytes 虽然可以通过设置最大运行内存，但是当文件很大时，内存还是吃不消。 ini_set('memory_limit', '500M') 因为可以使用 yield 对数据一行一行的处理、写入。 三、&quot;yield&quot; &amp; &quot;return&quot; 的区别？ 前面的错误意味着 getValues() 方法不会如预期返回一个字符串，让我们检查一下他的类型： &lt;? function getValues() { return 'value'; } var_dump(getValues()); // string(5) &quot;value&quot; function getValues() { yield 'value'; } var_dump(getValues()); // class Generator#1 (0) {} 生成器 类实现了 生成器 接口， 这意味着你必须遍历 getValue() 方法来取值： &lt;? $values = getValues(); foreach ($values as $value) { echo $value; } 但这不是唯一的区别！ 生成器允许您编写使用 foreach 的代码对一组数据进行迭代的代码，而无需在内存中构建数组，那样可能会导致超出内存限制。 在下面的例子里我们创建一个有 800,000 元素的数字同时从 getValues() 方法中返回他，同时在此期间，我们将使用函数 memory_get_usage() 来获取分配给次脚本的内存， 我们将会每增加 200,000 个元素来获取一下内存使用量，这意味着我们将会提出四个检查点： &lt;?php function getValues() { $valuesArray = []; // 获取初始内存使用量 echo round(memory_get_usage() / 1024 / 1024, 2) . ' MB' . PHP_EOL; for ($i = 1; $i &lt; 800000; $i++) { $valuesArray[] = $i; // 为了让我们能进行分析，所以我们测量一下内存使用量 if (($i % 200000) == 0) { // 来 MB 为单位获取内存使用量 echo round(memory_get_usage() / 1024 / 1024, 2) . ' MB'. PHP_EOL; } } return $valuesArray; } $myValues = getValues(); // 一旦我们调用函数将会在这里创建数组 foreach ($myValues as $value) {} ?&gt; // 输出结果为内存消耗，脚本的输出结果如下 0.34 MB 8.35 MB 16.35 MB 32.35 MB 这意味着我们这几行脚本消耗了超过 30M 的内存，每次我们向 $valuesArray 数组添加元素，都会增加它在内存中的大小: 举个使用 yield 的例子： &lt;?php function getValues() { // 获取初始内存使用 echo round(memory_get_usage() / 1024 / 1024, 2) . ' MB' . PHP_EOL; for ($i = 1; $i &lt; 800000; $i++) { yield $i; // 开始进行分析 测量内存占用 if (($i % 200000) == 0) { // get memory usage in megabytes echo round(memory_get_usage() / 1024 / 1024, 2) . ' MB'. PHP_EOL; } } } $myValues = getValues(); // 遍历完毕`values`之前不进行任何操作 foreach ($myValues as $value) {} // 此处开始获取`values` // 输出结果为内存消耗，脚本的输出结果如下 0.34 MB 0.34 MB 0.34 MB 0.34 MB 因此，如果你在应用中创建会导致服务器上内存出问题的巨大数组，则 yield 更加适合你的情况。 四、什么是 &quot;yield&quot; 选项？ 这里有很多 yield 的选项， 我将强调他们中的几个： 4.1 使用 yield， 你也可以使用 return &lt;? function getValues() { yield 'value'; return 'returnValue'; } $values = getValues(); foreach ($values as $value) {} echo $values-&gt;getReturn(); // 'returnValue' &gt; 4.2 返回键值对 &lt;? function getValues() { yield 'key' =&gt; 'value'; } $values = getValues(); foreach ($values as $key =&gt; $value) { echo $key . ' =&gt; ' . $value; } &gt; ","link":"http://mofish.pily.life/post/shi-yong-yield-sheng-cheng-qi-lai-shan-dai-ni-de-nei-cun/"},{"title":"一文搞懂PHP的垃圾回收机制（已迁移）","content":"垃圾回收是一个多数编程语言中都带有的内存管理机制。与非托管性语言相反：C, C++ 和 Objective C，用户需要手动收集内存，带有 GC 机制的语言：Java, javaScript 和 PHP 可以自动管理内存。 一、概念 垃圾回收机制，即我们常说得gc，就是废物利用的意思，是一种动态存储分配的方案。它会自动释放程序不再需要的已分配的内存块。 垃圾回收机制可以让程序员不必过分关心程序的内存分配，从而将更多的精力投入到业务逻辑中。 在现在的流行各种语言当中，垃圾回收机制是新一代语言所共有的特征，如Python、PHP、C#、Ruby等都使用了垃圾回收机制。 二、垃圾回收机制的发展 在PHP5.3版本之前，使用的垃圾回收机制是单纯的“引用计数”，即： 每个内存对象都分配一个计数器，当内存对象被变量引用时，计数器+1； 当变量引用撤掉后（执行 unset() 后），计数器-1； 当计数器 = 0时，表明内存对象没有被使用，该内存对象则进行销毁，垃圾回收完成。 准确的说，5.3版本之前的垃圾回收机制是没有专门的垃圾回收器的，只是简单的判断了一下变量的zval的refcount是否未0，是就释放否则不释放直至进程结束。 但是当两个或多个对象互相引用形成环状后，内存对象的计数器不会消减为0，这时候，这一组内存对象已经没用了，但是不能回收，从而导致内存泄露的现象。 随着PHP的发展，PHP开发者的增加以及其所承载的业务范围的扩大，在PHP5.3中引入了更加完善的垃圾回收机制，新的垃圾回收机制解决了无法处理循环的引用内存泄漏问题。 每个PHP的变量都存在于一个叫做zval的容器中，一个zval容器，除了包含变量名和值，还包括两个字节的额外信息： 一个叫做'is_ref'，是个布尔值，用来表示这个变量是否属于引用集合,通过这个字节，我们php才能把普通变量和引用变量区分开来。 第二个额外字节就是'refcount'，用来表示指向这个容器的变量的个数。 三、PHP5和PHP7的垃圾回收不同点（重点） PHP5标量数据类型会计数，PHP7标量数据类型不再计数，不需要单独分配内存 PHP7的zval 需要的内存不再是单独从堆上分配，不再自己存储引用计数 PHP7的复杂数据类型（比如数组和对象）的引用计数由其自身来存储 四、变量在zval的变量容器中结构 zval中，除了存储变量的类型和值之外，还有is_ref字段和refcount字段 1、is_ref：是个bool值，用来区分变量是否属于引用集合。 2、refcount：计数器，表示指向这个zval变量容器的变量个数。 五、标量在zval容器的例子 5.1 PHP5.3标量在zval容器例子 当将一个变量复制给另外一个变量时，不会立即为新变量分配内存空间，而是在原变量的 zval 中给 refcount 加1。 只有当原变量或者新变量发生改变时，才会为新变量分配新的内存空间，同时原变量的 refcount 减1，这就是我们常说的引用计数和写时拷贝（copy on write）。 当然，如果是 unset 原变量的话，新变量会直接使用原变量的 zval ，而不是重新分配。 对于 &amp; 引用赋值时，原变量的 is_ref 加1，除此之外还有一点要注意的是，如果给一个变量 &amp;赋值，那么之前普通赋值的变量会分配新的空间，避免修改影响。 &lt;?php $a = 1; xdebug_debug_zval('a'); echo PHP_EOL; $b = $a; xdebug_debug_zval('a'); echo PHP_EOL; $c = &amp;$a; xdebug_debug_zval('a'); echo PHP_EOL; xdebug_debug_zval('b'); echo PHP_EOL; 结果如下： a:(refcount=1, is_ref=0),int 1 a:(refcount=2, is_ref=0),int 1 a:(refcount=2, is_ref=1),int 1 b:(refcount=1, is_ref=0),int 1 5.2 PHP7.X标量在zval容器例子 &lt;?php $a = 1; xdebug_debug_zval('a'); echo PHP_EOL; $b = $a; xdebug_debug_zval('a'); 结果如下：可以看到标量（布尔，字符串，整形，浮点型）不再计数了 a:(refcount=0, is_ref=0),int 1 a:(refcount=0, is_ref=0),int 1 六、复合类型数组和对象在zval容器例子 6.1 PHP5.3复合类型数组和对象在zval容器例子 &lt;?php // 数组类型 $a = array( 'meaning' =&gt; 'life', 'number' =&gt; 42 ); xdebug_debug_zval( 'a' ); // 结果如下：数组用了比数组长度多1个zval存储。数组分配了三个zval容器：a meaning number a:(refcount=1, is_ref=0), array 'meaning' =&gt; (refcount=1, is_ref=0) string 'life' (length=4) 'number' =&gt; (refcount=1, is_ref=0) int 42 // 对象类型 class Test{ public $a = 1; public $b = 2; function handle(){ echo 'hehe'; } } $test = new Test(); xdebug_debug_zval('test'); // 结果如下： test:(refcount=1, is_ref=0), object(Test)[1] public 'a' =&gt; (refcount=2, is_ref=0), int public 'b' =&gt; (refcount=2, is_ref=0), int 6.2 PHP7.X复合类型数组和对象在zval容器例子 &lt;?php $a = array( 'meaning' =&gt; 'life', 'number' =&gt; 42 ); xdebug_debug_zval( 'a' ); // 结果如下：可以明显的看到数组a的refcount=2，后经测试发现数组的refcount都是从2开始的 a:(refcount=2, is_ref=0), array 'meaning' =&gt; (refcount=1, is_ref=0) string 'life' (length=4) 'number' =&gt; (refcount=1, is_ref=0) int 42 // 对象 class Test{ public $a = 1; public $b = 2; function handle(){ echo 'hehe'; } } $test = new Test(); xdebug_debug_zval('test'); // 结果如下： test:(refcount=1, is_ref=0), object(Test)[1] public 'a' =&gt; (refcount=0, is_ref=0) int 1 public 'b' =&gt; (refcount=0, is_ref=0) int 2 七、循环引用问题 &lt;?php $a = array('life'); xdebug_debug_zval( 'a' ); echo PHP_EOL; $a[] = &amp;$a; xdebug_debug_zval('a'); 说明： 在5.2及更早版本的PHP中，没有专门的垃圾回收器GC（Garbage Collection），引擎在判断一个变量空间是否能够被释放的时候是依据这个变量的zval的refcount的值。 如果refcount为0，那么变量的空间可以被释放，否则就不释放，这是一种非常简单的GC实现。 现在unset ($a),那么array的refcount减1变为1.现在无任何变量指向这个zval，而且这个zval的计数器为1，不会回收。 结果： 尽管不再有某个作用域中的任何符号指向这个结构(就是变量容器)，由于子元素“1”仍然指向数组本身，所以这个容器不能被清除。 因为没有另外的符号指向它，用户没有办法清除这个结构，结果就会导致内存泄漏。 在php5.3的GC中，针对的垃圾做了如下说明： 1：如果一个zval的refcount增加，那么此zval还在使用，肯定不是垃圾，不会进入缓冲区。 2：如果一个zval的refcount减少到0， 那么zval会被立即释放掉，不属于GC要处理的垃圾对象，不会进入缓冲区。 3：如果一个zval的refcount减少之后大于0，那么此zval还不能被释放，此zval可能成为一个垃圾，将其放入缓冲区。PHP5.3中的GC针对的就是这种zval进行的处理。 开启/关闭：垃圾回收机制可以通过修改php配置实现，也可以在程序中使用gc_enable() 和 gc_disable()开启和关闭。 八、垃圾回收算法 优化后的垃圾回收算法呢，还是以引用计数为基础，但是不再是使用简单计数作为回收准则，而是使用了一种同步回收算法。 首先PHP会分配一个固定大小的“根缓冲区”，这个缓冲区用于存放固定数量的zval，这个数量默认是10,000，如果需要修改则需要修改源代码Zend/zend_gc.c中的常量GC_ROOT_BUFFER_MAX_ENTRIES然后重新编译。之后当根缓冲区满额时，PHP就会执行垃圾回收。 回收时候的具体算法操作如下： 它首先对每个根缓冲区中的根zval按照深度优先遍历算法遍历所有能遍历到的zval，并将每个zval的refcount减1，同时为了避免对同一zval多次减1(因为可能不同的根能遍历到同一个zval)，每次对某个zval减1后就对其标记为“已减”； 再次对每个缓冲区中的根zval深度优先遍历，如果某个zval的refcount不为0，则对其加1，否则保持其为0； 清空根缓冲区中的所有根(注意是把这些zval从缓冲区中清除而不是销毁它们)，然后销毁所有refcount为0的zval，并收回其内存。 如果不能完全理解也没有关系，只需记住PHP5.3的垃圾回收算法有以下几点特性： 并不是每次refcount减少时都进入回收周期，只有根缓冲区满额后在开始垃圾回收。 可以解决循环引用问题。 可以总将内存泄露保持在一个阈值以下。 九、性能影响 1、内存占用空间的节省 首先，实现垃圾回收机制的整个原因是为了一旦先决条件满足，通过清理循环引用的变量来节省内存占用。在PHP执行中，一旦根缓冲区满了或者调用gc_collect_cycles() 函数时，就会执行垃圾回收。 2、执行时间增加 垃圾回收影响性能的第二个领域是它释放已泄漏的内存耗费的时间。 通常，PHP中的垃圾回收机制，仅仅在循环回收算法确实运行时会有时间消耗上的增加。但是在平常的(更小的)脚本中应根本就没有性能影响。 3、在平常脚本中有循环回收机制运行的情况下，内存的节省将允许更多这种脚本同时运行在你的服务器上。因为总共使用的内存没达到上限。 这种好处在长时间运行脚本中尤其明显，诸如长时间的测试套件或者daemon脚本此类。同时，对通常比Web脚本运行时间长的脚本应用程序，新的垃圾回收机制，应该会大大改变一直以来认为内存泄漏问题难以解决的看法。 ","link":"http://mofish.pily.life/post/php_learning_la_ji_hui_shou/"},{"title":"docker搭建tars环境","content":"一、部署Lnmp环境 1、创建网段 # 创建一个名为tars的桥接(bridge)虚拟网络，网关172.25.0.1，网段为172.25.0.0 docker network create -d bridge --subnet=172.25.0.0/16 --gateway=172.25.0.1 tars 2、拉取lnmp镜像 地址：https://hub.docker.com/r/2233466866/lnmp docker pull 2233466866/lnmp:1.13-nosql 3、创建映射文件并拷贝映射文件 ①随便run一下生成一个容器 docker run -dit -p 80:80 -p 443:443 -p 3306:3306 -p 9000:9000 -p 6379:6379 --privileged=true --name=test --net=tars --ip=&quot;172.25.0.2&quot; 2233466866/lnmp:1.13-nosql ②创建映射文件夹（自行创建需要的） ocker container cp containeID:/usr/local/nginx/conf . docker复制文件，把容器中的文件复制到本地当前位置，防止映射时覆盖为空 注意不要漏了那个 . ③启动容器 docker run -dit -p 80:80 -p 443:443 -p 3306:3306 -p 9000:9000 -p 6379:6379 -v /home/dockerServer/www:/www -v /home/dockerServer/mysql/data:/data/mysql -v /home/dockerServer/php/etc:/usr/local/php7/etc -v /home/dockerServer/nginx/conf:/usr/local/nginx/conf -v /home/dockerServer/nginx/logs:/usr/local/nginx/logs --privileged=true --name=lnmp --net=tars --ip=&quot;172.25.0.2&quot; 2233466866/lnmp:1.13-nosql ④mysql配置 1.修改配置文件 --- vim /etc/my.cnf 在 [mysqld] 的段中加上一句：skip-grant-tables 2.重启mysql --- systemctl restart mysqld 3.修改密码 --- SHOW VARIABLES LIKE 'validate_password%'; --- set global validate_password_policy=LOW; --- set global validate_password_length=6; --- alter user user() identified by '941002'; --- use mysql; --- update user set authentication_string=password('941002') where user = 'root'; --- grant all privileges on *.* to root@&quot;%&quot; identified by '941002'; --- grant all privileges on *.* to root@'%' identified by '941002' with grant option; --- flush privileges; 4.还原配置文件 --- vim /etc/my.cnf 二、tar-framework安装 docker pull tarscloud/framework:v3.0.10 # 挂载的/etc/localtime是用来设置容器时区的，若没有可以去掉 # 3000端口为web程序端口 # 3001端口为web授权相关服务端口(docker&gt;=v2.4.7可以不暴露该端口) docker run -d \\ --name=tars-framework \\ --net=tars \\ -e MYSQL_HOST=&quot;172.25.0.2&quot; \\ -e MYSQL_ROOT_PASSWORD=&quot;941002&quot; \\ -e MYSQL_USER=root \\ -e MYSQL_PORT=3306 \\ -e REBUILD=false \\ -e INET=eth0 \\ -e SLAVE=false \\ --ip=&quot;172.25.0.3&quot; \\ -v /home/dockerServer/tars/framework:/data/tars \\ -v /etc/localtime:/etc/localtime \\ -p 3000:3000 \\ -p 3001:3001 \\ tarscloud/framework:v3.0.10 检查mysql连接是否正常 docker exec -it tars-framework /bin/bash cd /usr/local/tars/cpp/deploy/ ./mysql-tool --host=172.25.0.2 --user=&quot;root&quot; --pass=&quot;123456&quot; --port=3306 --check 三、部署 tars 应用节点 docker pull tarscloud/tars-node:latest docker run -d --name=tars-node --net=tars -e INET=eth0 -e WEB_HOST=&quot;http://172.25.0.3:3000&quot; --ip=&quot;172.25.0.5&quot; -v /data/tars:/data/tars -v /etc/localtime:/etc/localtime -p 9010-9020:9010-9020 tarscloud/tars-node:latest 四、清理日志文件 sudo du -h -m --max-depth=1 /usr/local/app/tars/app_log find /usr/local/app/tars/app_log/hlyun/ -name '*.log' | xargs rm yum 修改源 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo yum clean all &amp;&amp; yum makecache git git config --global credential.helper store ","link":"http://mofish.pily.life/post/docker_to_tars/"},{"title":"wsl2环境搭建及常用命令（已迁移）","content":"一、wsl2安装 ①检验是否开启虚拟化 「Ctrl+Alt+Del」调出「安全选项」界面，选择启动「任务管理器」。「性能」选项卡显示「虚拟化：已启用」即可。 ②开启开发者模式 「开发者选项」→「开发人员模式」，打开开关。 ③开启Windows功能 需要先启用「适用于 Linux 的 Windows 子系统」可选功能，然后才能在 Windows 上安装 Linux 分发。 找到「控制面板」-「程序/功能」-「启用或关闭Windows功能」，选中「适用于Linux的Windows子系统」，然后点击确定。 ④升级WSL内核 适用于 x64 计算机的 WSL2 Linux 内核更新包 ⑤启用虚拟机功能 找到「控制面板」-「程序/功能」-「启用或关闭Windows功能」，选中「虚拟机平台」，然后点击确定。 ⑥设置默认WSL 将 WSL 2 设置为默认版本，打开 PowerShell，然后在安装新的 Linux 发行版时运行以下命令，将 WSL 2 设置为默认版本： wsl --set-default-version 2 参考：(34条消息) 适用于 Linux 的 Windows 10/11 子系统（WSL2）安装指南_江城撅嘴的川羌的博客-CSDN博客_wsl2 二、centos安装 chocolatey Chocolatey 是基于 NuGet 的一个软件包管理器，就像 Linux 中的 yum 或 apt 一样，在 Windows10 中也可以用命令行安装程序了。 Set-ExecutionPolicy Bypass -Scope Process -Force; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1')) lxRunOffline LxRunOffline 是非常实用的 WSL 管理软件，可以备份、创建、恢复、导出WSL子系统，也可以安装适配 WSL 的任何 Linux 发行版，可以将 WSL 子系统安装到任意目录中。 choco install lxrunoffline -y ### centos（ubuntu同理） 首先需要自己下载对应的包 安装到指定的硬盘： LxRunOffline.exe install -n centos -d D:\\centos -f D:\\centos-7.8.2003-x86_64-docker.tar.xz 设为wsl2方式： wsl --set-version centos 2 三、系统导出导入 导出系统： wsl —export docker-desktop d:\\docker\\docker-desktop.tar 注销系统： wsl —unregister docker-desktop 导入系统： wsl —import docker-desktop d:\\docker\\docker-desktop d:\\docker\\docker-desktop.tar —version 2 Ps：主要用于把系统移动到其它盘，防止占用C盘空间 四、端口和ip固定启动设置 固定wsl2访问： wsl -d centos -u root ip addr add 192.168.193.168/24 broadcast 192.168.207.255 dev eth0 label eth0:1 netsh interface ip add address &quot;vEthernet (WSL)&quot; 192.168.193.169 255.255.255.240 查看开放的端口列表： netsh interface portproxy show all 设置开放端口： netsh interface portproxy add v4tov4 listenport=外部访问端口 listenaddress=0.0.0.0 connectport=内网访问端口 connectaddress=wsl2内部固定ip 删除开放端口： netsh interface portproxy delete v4tov4 listenport=3000 listenaddress=0.0.0.0 Ps：使得外部能访问本机端口 五、代理设置 cat /etc/resolv.conf|grep nameserver|awk '{print $2}' export ALL_PROXY=&quot;http://172.24.144.1:41092&quot; unset http_proxy unset https_proxy ","link":"http://mofish.pily.life/post/wsl2_configure/"},{"title":"MySql学习之路(十二)：什么是分布式事务？","content":"随着微服务架构的普及，一个大型业务系统往往由若干个子系统构成，这些子系统又拥有各自独立的数据库。往往一个业务流程需要由多个子系统共同完成，而且这些操作可能需要在一个事务中完成。在微服务系统中，这些业务场景是普遍存在的。此时，我们就需要在数据库之上通过某种手段，实现支持跨数据库的事务支持，这也就是大家常说的“分布式事务”。 https://juejin.cn/post/6844903734753886216 https://juejin.cn/post/6844903647197806605#heading-15 https://juejin.cn/post/6844903573667446797#heading-11 https://blog.csdn.net/weixin_44792186/article/details/122898611 一、分布式事务的基础👏 1.1 CAP理论👇 定义： CAP又被称作布鲁尔定理，其定义为在一个分布式系统中，当设计读写操作时，只能保证一致性(Consistence)、可用性(Availability)和分区容错性(PartitionTolerance)三者中的两个，另外一个必须被牺牲。 CAP： C - Consistence 一致性： 对某个指定的客户端来说，读操作能返回最新的写操作结果。 对于数据分布在不同节点上的数据来说，如果在某个节点更新了数据，那么在其它节点如果能读取到这个最新的数据，那么就称为强一致性。如果某个节点没有读取到，那就是分布式不一致。 A - Availibility 可用性 非故障的节点在合理的时间内返回合理的响应（不是错误或超时的响应）。 这里强调的是合理的响应，不能超时，不能出错。注意并没有说正确的结果，例如，应该返回 100 但实际上返回了 90，肯定是不正确的结果，但可以是一个合理的结果。 P - PartitionTolerance 分区容忍性 当出现网络分区后，系统能够继续履行职责，即这个集群仍然可以继续工作。 网络分区指的是，一个分布式系统里面，节点组成的网络本来是连通的，然而可能因为一些故障导致部分节点之间断开了，整个网络就分成了几块区域，数据就散布在这些不连通的区域中。 如何选择： 虽然CAP理论定义是三个要素只能选择其中两个，但是在分布式系统下，我们会发P(分区容忍性)是必须要选择的要素，因为网络本身无法做到100%可靠，有可能会出现故障或者短暂的网络分区，因此分区是一个必然现象，所以是不可能选择CA架构的。 CP (一致性+分区容忍性) 对于 CP 来说，放弃了可用性，追求强一致性和分区容忍性，确认不同节点的数据是强一致的。 AP （可用性+分区容忍性） 放弃一致性(这里说的一致性是强一致性)，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的BASE也是根据AP来扩展。 另外，只能选择CP或者AP是指系统发生分区现象时无法同时保证C（一致性）和A（可用性），但不是意味着什么都不做，当分区故障解决后，系统还是要保持保证CA。 1.2 BASE理论👇 定义： BASE 是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency），核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性，是对CAP中AP的一个扩展。 BASE： BA - Basic Available 基本可用 分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。 S - Soft State 柔性状态 同一数据的不同副本的状态，可以不需要实时一致，这里指的是CAP中的不一致。 E - Eventual Consisstency 最终一致性 最终一致性 同一数据的不同副本的状态，可以不需要实时一致，但一定要保证经过一定时间后最终是一致的。 1.3 平衡👇 ACID能够保证事务的强一致性，即数据是实时一致的。这在本地事务中是没有问题的，在分布式事务中，强一致性会极大影响分布式系统的性能，因此分布式系统中遵循BASE理论即可。 但分布式系统的不同业务场景对一致性的要求也不同。 如交易场景下，就要求强一致性，此时就需要遵循ACID理论，而在注册成功后发送短信验证码等场景下，并不需要实时一致，因此遵循BASE理论即可。因此要根据具体业务场景，在ACID和BASE之间寻求平衡。 二、分布式事务解决方案 Mq+本地任务表（确保最终一致性） Seata XA TCC AT SAGA ","link":"http://mofish.pily.life/post/mysql_learning_12/"},{"title":"MySql学习之路(十一)：死锁的原因及其解决方法","content":"https://blog.csdn.net/qq_52253798/article/details/121526110 https://juejin.cn/post/7088473265072504846 https://juejin.cn/post/7070386492324970532 https://juejin.cn/post/6844903923459817479 ","link":"http://mofish.pily.life/post/mysql_learning_11/"},{"title":"MySql学习之路(十)：各种锁详解","content":"😻 MySql在服务器层或存储引擎层实现了各种各样的锁，用于并发访问同一共享资源时的同步机制，保证了数据访问的一致性与有效性，本章节就记录一下MySql中各种锁的功能和原理。 前言 这么多锁，头都看晕了，现在分门别类一下，然后按照类别一一整理吧。 目录 一、全局锁 二、表级锁 表锁 元数据锁 意向锁 三、页级锁 四、行级锁 4.1 按属性分类 共享锁（S锁、读锁） 排它锁（X锁、写锁） 4.2 按模式分类 乐观锁 悲观锁 4.3 按算法分类 记录锁 间隙锁 临键锁 一、全局锁 全局锁就是对整个数据库实例加锁，加锁后整个实例就处于只读的状态，后续的DML写的语句，DDL语句，已经更新的事务提交语句都将被阻塞。 其典型的使用场景是做全库的备份，对所用表进行锁定，从而获取一致性的视图，保证数据的完整性。 应用场景 全库逻辑备份（mysqldump） 实现方式 MySql提供了一个加全局读锁的方法，命令如下： #加锁 flush tables with read lock; #做备份 mysqldump -uroot -p密码 数据库名&gt;备份到的文件; mysqldump -uroot -p1234 itcast&gt;itcast.sql #解锁 unlock tables; 当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞： 数据的增删改 数据定义语句（建表，修改表结构等） 更新类事务的提交语句 全库逻辑备份风险点： 如果在主库上备份，备份期间都不能执行更新，那么业务基本处理停止状态，只能读了； 如果在从库上备份，那么备份期间从库不能执行主库同步过来的binlog文件，导致主从延迟。 解决方案： 执行mysqldump时增加 --single-transaction，启动一个事务，确保拿到一致性视图，由于MVCC的支持，这个过程中数据是可以正常更新。 二、表级锁 表级锁，每次操作锁住整张表。锁定粒度大，发生锁冲突的概率最高，并发度最低。应用在MyISAM、InnoDB、BDB等存储引擎中。 其中包含： 表锁：表共享读锁、表排它写锁； 元数据锁（meta data lock，MDL）； 意向锁：意向共享锁、意向排它锁。 2.1 表锁 # 当加了读锁之后客户端都可以读，但是都不可以改。 lock tables table_name read; # 当加了写锁之后，加锁的那个客户端既可以读也可以写，但是其他客户端都不能进行读写。 lock tables table_name write; #释放锁 unlock tables 2.2 元数据锁（meta data lock，MDL） MDL加锁的过程是系统自动控制，无需显式使用，在访问一张表的时候会自动加上。MDL锁的主要作用是维护表元数据的数据一致性，在表上有活动事务的时候，不可以对元数据进行写入操作。 为了避免DML和DDL冲突，保证数据读写的正确性。 在MySQL5.5中引入MDL，当对一张表进行增删改查的时候，加MDL读锁（共享）；当对表结构进行变更操作的时候，加MDL写锁（排他）。 所谓元数据，就是表示数据的数据。只要不是我们存储到数据库里的数据，大多数可以理解为元数据。因此，列名、数据库名、用户名、版本名以及从show,information_schema数据库中的表内容都是元数据。 2.3 意向锁 意向共享锁和意向排它锁总称为意向锁，属于表锁的一种。意向锁的出现是为了支持Innodb，协调行锁和表锁的关系，支持多粒度（表锁与行锁）的锁并存。 作用 当有事务A有行锁时，MySQL会自动为该表添加意向锁，事务B如果想申请整个表的写锁，那么不需要遍历每一行判断是否存在行锁，而直接判断是否存在意向锁，增强性能。 实现方式 意向共享锁（IS） 事务想要在获得表中某些记录的共享锁(行锁)，需要在表上先加意向共享锁， 因此执行select…lock in share mode时，会给表加上意向共享锁。 意向共享锁与表共享读锁兼容，表排它锁互斥。 即改客户端加了意向共享锁后，别的客户端可以加表共享锁，但是不能加表排它锁。 意向排它锁（IX） 事务想要在获得表中某些记录的排它锁(行锁)，需要在表上先加意向排它锁，因此执行insert、update、delete、select…for update时，会给表加上意向排它锁。 意向排它锁与表共享读锁和排它锁都互斥。 即改客户端加了意向排它锁后，别的客户端可以不能加表共享锁，也不能加表排它锁。 三、页级锁 页级锁是 MySql 中比较独特的一种锁定级别，在其他数据库管理软件中并不常见。 页级锁的颗粒度介于行级锁与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力同样也是介于上面二者之间。另外，页级锁和行级锁一样，会发生死锁。 页级锁主要应用于 BDB 存储引擎，用的比较少。 四、行级锁 行级锁是粒度最低的锁，发生锁冲突的概率也最低、并发度最高。但是加锁慢、开销大，容易发生死锁现象。 随着锁定资源颗粒度的减小，应用程序的访问请求遇到锁等待的可能性也会随之降低，系统整体并发度也会随之提升，从而提高需要高并发应用系统的整体性能。 但是也因为由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也就更多，带来的消耗自然也就更大。此外，行级锁也最容易发生死锁。所以说行级锁最大程度地支持并发处理的同时，也带来了最大的锁开销。 MySql中只有InnoDB支持行级锁，基本分为共享锁和排它锁两种，其它什么悲观锁、间隙锁都是通过不用的使用方式演变而来的。 按属性分类：共享锁(S锁)、排它锁(X锁) 按模式分类：乐观锁(业务实现)、悲观锁(即排它锁) 按算法分类：记录锁、间隙锁、临键锁 行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。在UPDATE、DELETE操作时，MySQL不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的间隙锁（next-key locking）。 4.1 按属性分类 共享锁（S锁、读锁） 共享锁，也称读锁、S锁，当事务A对数据加上读锁后，其它事务只能对该数据加读锁，并不能做任何修改操作，只有当事务A的读锁被释放后，其它事务才能修改数据或者添加写锁。 应用场景： 共享锁主要是为了支持并发的读取数据而出现的，读取数据时，不允许其他事务对当前数据进行修改操作，从而避免”不可重读”的问题的出现。 实现方式： select ... lock in share mode; # 或 select ... for share; # 8.0新增语法 事务A持有共享锁，事务B可以正常读数据，也可以加共享锁，但是 update 操作会一直被阻塞，知道超时或者事务A释放锁。 共享锁也属于悲观锁的一种，即也会对外界的修改持保守态度，因此会将数据处于锁定状态，只允许外界读，但是不允许修改。 排它锁（X锁、写锁） 排它锁，也称写锁、X锁，当事务A对数据加上排它锁后，其它事务对该数据即不能修改，也不能对该数据进行加锁，因为排它锁与其它锁是互斥的，只有当锁被释放后，其它事务才能修改数据或者添加锁。 应用场景： 写锁主要是为了解决在修改数据时，不允许其他事务对当前数据进行修改和加锁操作，从而可以有效避免”脏读”问题的产生。 实现方式： InnoDB引擎默认update、delete、insert都会自动给涉及到的数据加上排它锁，select语句默认不会加任何锁类型，但是可以通过添加for update来加锁。 select ... for update; 事务A对数据加上排它锁后，事务B不可以修改该数据，操作会一直阻塞直至超时或者那行数据锁被释放。 事务A对数据加上排它锁后，事务B可以正常读取该行数据，但是不能再次通过加排它锁的方式来读取，因为排它锁是互斥的。 MySql8.0 新特性： 在5.7及之前的版本，SELECT ... FOR UPDATE，如果获取不到锁，会一直等待，直到innodb_lock_wait_timeout超时。 在8.0版本中，SELECT ... FOR UPDATE，SELECT ... FOR SHARE添加NOWAIT、SKIP LOCKED语法，跳过锁等待，或者跳过锁定。 NORMAL：立即返回报错 SKIP LOCKED：立即返回，只是返回结果不包含被锁定的行 4.2 按模式分类 乐观锁 乐观锁不是数据库自带的锁，是需要我们根据业务逻辑去实现的。 乐观锁是假设数据一般情况下不会造成冲突，是乐观的，只有当数据进行提交时才会对数据的冲突进行检测，如果发现有冲突，则返回错误信息，交由用户去决定如何进行下一步。 应用场景： 适用于读多写少，因为如果出现大量的写操作，写冲突的可能性就会增大，业务层需要不断重试，会大大降低系统性能。 实现方式： 一般我们采用在数据表中添加version字段，每操作一次就给该行数据的版本号version进行加1，然后在操作更新数据时，先查询数据及其version，更新数据时再判断此刻数据的verison是否和刚刚查询出来的是一致的，如果是则更新，不是则说明这段时间有其它程序更新了该数据，则不进行更新操作。 -- 先获取数据，假设现在获取到的version = 1 select * from student where id = 1; -- 更新数据时判断version是否一致 -- 如果是一致，则更新成功 -- 如果不一致，则更新失败 update student set name = '小光', version = 2 where id = 1 and version = 1; 悲观锁 悲观锁，具有强烈的独占和排它性，每次获取或修改数据时，都认为别人会修改，因此在整个数据处理过程中都会将数据处于一个锁定的状态。 应用场景： 适用于并发量不大、写入操作比较频繁、数据一致性比较高的场景。 实现方式： 在MySQL中使用悲观锁，必须关闭MySQL的自动提交，set autocommit=0。 共享锁和排它锁是悲观锁的不同的实现，它俩都属于悲观锁的范畴。 -- 开启事务并关闭自动提交 setautocommit=0; -- 查询学生信息 select * from student where id=1 for update; -- 修改学生信息 update student set age = 27 where id = 1; -- 提交事务 commit; select...for update 是 MySql 提供的实现悲观锁的方式，属于排它锁。在上述事务中，id为1的数据会被锁定，其它事务要操作该条数据时必须等待本次事务提交之后才能执行，这样我们就可以保证当前数据不会被其它事务修改。 此时MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条sql语句用不到索引则会把所有数据住。 4.3 按算法分类 记录锁、间隙锁、临键锁都是排它锁，而记录锁的使用方法跟排它锁介绍一致。 记录锁 记录锁，即某行记录加锁，所以也可以叫行锁，其使用方式和排它锁一致。 -- 通过for update 获取数据 select * from student where id = 1 for update; -- 更新和删除数据 update student set age = 99 where id = 1; id 为 1的记录行会被锁住，其它事务不允许对该行数据进行操作或加锁。 需要注意的是： id列必须是主键列(primary key)或唯一索引列(unique key)，否则上述语句加的锁就会变成临键锁。 同时查询语句必须为精确匹配：=，不能是&gt;、&lt;、like、between等，否则也会退化成临键锁。 间隙锁（Gap Locks、gap锁） 间隙锁 是 InnoDB 在 RR(可重复读) 隔离级别下为了解决幻读问题时引入的锁机制，而间隙锁也是 InnoDB 中行锁的一种。 间隙锁是基于非唯一索引的，使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据。 SELECT * FROM table WHERE sort BETWEN 1 AND 10 FOR UPDATE; 即所有在（1，10）区间内的记录行都会被锁住，所有 sort 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。 除了手动加锁外，在执行完某些 SQL 后，InnoDB 也会自动加间隙锁，这个我们在下面会提到。 临键锁（Next-Key Locks） 临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间，其主要目的，也是为了避免幻读(Phantom Read)。 每个非唯一索引列都会有一个临键锁，当唯一索引进行范围查找时也会退化为临键锁。 简单的话，间隙锁锁住的是所选数据区间范围，临键锁锁住的是所选数据及相邻数据之间的范围。 RC级别没有临键锁和间隙锁。 -- 主键：id -- 唯一索引：student_no -- 普通索引：sort -- 情况1：主键或唯一索引的等值查询 -- 锁情况：只会锁住当前记录行，即只有一个记录锁 select * from student where id = 4 for update； select * from student where student_no = 2022042123 for update； -- 情况2：普通索引等值查询、唯一索引和普通索引的范围查询 -- 锁情况：临建锁，锁定的是范围区间 select * from student where sort = 9 for update； select * from student where id between 5 and 14 for update； -- 这里为什么update sort = 25 的时候也会失败？因为sort = 25的数据id = 14，而sort = 12 的数据有一条 id = 16，所以可能落到了范围区间内，导致锁等待。 select * from student where sort between 9 and 12 for update； update student set sex = 1 where sort = 25; -- 这里1-6的数据sort索引排序正常，唯一索引id和student_no也是排序正常的，所以锁住的范围也是(-∞，9)这个区间。 select sort from student where sort between 1 and 6 for update; 重点： 很多时候发现锁住的范围跟预想的范围不是特别一样，比如锁住了1-9这个索引区间，这是一个实际的范围，那么插入一条 1 的索引时候，可能插入到区间内（被阻塞），也可能插入到区间外（插入成功）！ 这个是会收到一些其它索引排序策略的影响的，所以到底是谁开谁闭，实际上是会收到排列顺序的影响的。 ","link":"http://mofish.pily.life/post/mysql_learning_10/"},{"title":"MySql学习之路(九)：事务及其四个隔离级别之MVCC","content":"上一章节我们讲了MySql的事务，这章节我们就说说RC和RR级别下能够解决可重复读和幻读的原理是什么！ 一、概述📑 MVCC，中文叫多版本并发控制，它是通过读取历史版本的数据，来降低并发事务冲突，从而提高并发性能的一种机制。它的实现依赖于隐式字段、undo日志、快照读&amp;当前读、Read View。 1.1 MVCC带来的好处是？🤤 多版本并发控制（MVCC）是一种用来解决读-写冲突的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能同时还可以解决脏读，不可重复读和幻读等事务隔离问题，但不能解决更新丢失问题 MVCC是采用无锁的形式解决读-写冲突问题。这里的读是指的快照读。即MVCC实现的快照读！！！ 1.2 MVCC和间隙锁😵 MVCC无锁解决了读-写冲突的问题，并且解决了不可重复读和部分解决了幻读问题，从而实现了RC和RR两个隔离级别。 **而间隙锁本质上依旧是锁，会阻塞两个并发事务的执行。**在一定程度上可以解决幻读的问题，也是为了解决binlog日志中statement模式带来的bug。 mysql数据库的主从复制依靠的是binlog。而在mysql5.0之前，binlog模式只有statement格式。这种模式的特点：binlog的记录顺序是按照数据库事务commit顺序为顺序的。 当不存在间隙锁的情况下，会有如下场景： 事务a先删除订购日期 &lt; 2021-01-01 的数据； 事务b把其中一条数据 id = 6 的订购日期设为 2020-12-12日，并且commit； 事务a进行commit。 此时由于statement日志是按照commit顺序记录的，导致事务b先执行，然后事务a执行的时候，把事务把更新的那条数据也删除了。 这就导致了主库是有id = 6的这条数据的，但是从库的日志是先更新后删除的，就没有id = 6这边数据了，导致主从不一样。 所以为了解决这个bug，才在RR级别引入了间隙锁，使得更新id = 6时被锁住，需要等事务a提交后，才能更新成功。 二、实现原理⚙️ MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决读写冲突，它的实现原理主要是依赖记录中的快照读&amp;当前读、 3个隐式字段、 undo日志、Read View 来实现的，undo日志保存了历史快照，而Read View规则帮我们判断当前版本的数据是否可见。 2.1 快照读&amp;当前读🖨 当前读👇👇 以下的操作都属于当前读，那什么是当前读？那就是它读取的是记录的最新版本，读取和操作时还需要保证并发事务不会同步操作该记录，会对该行记录进行加锁。 -- 共享锁 select ... lock in share mode; -- 排它锁 select ... for update; -- 更新、插入删除 update ....; insert ...; delete ...; 快照读👇👇 普通不加锁的select操作就是快照读，即不加锁的非阻塞读。 快照读的前提是隔离级别不是串行级别，串行级别下都是属于当前读。 之所以出现快照读，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制实现的，即MVCC。因此快照读读到的并不一定是数据的最新版本，而有可能是之前的历史版本。 总结👇👇 说白了MVCC就是为了实现读-写冲突不加锁，而这个读指的就是快照读, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现 2.2 隐式字段🛡 每行记录处理记录我们定义的字段外，数据库还隐式定义了row_id、roll_pointer、trx_id等字段。 row_id（6 byte）：当不存在主键和非Null唯一索引时，InooDB会自动以row_id生成一个聚簇索引。 roll_pointer（7 byte）：回滚指针，指向这条记录的上一个版本，每次对记录进行改动时，都会把旧的版本写入到undo日志中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 trx_id（6 byte）：存储这条记录最后一次修改/插入事务id 2.3 undo log📃 InnoDB把这些为了事务回滚而记录的数据称之为undo log，其主要分为3种： Insert undo log：插入记录时，至少把记录的主键值记录下来，便于回滚时直接根据主键删除即可； Update undo log：更新记录时，至少把记录修改前的旧值都记录下来，便于回滚时把记录恢复为旧值； Delete undo log：删除记录时，至少把记录内容记录下来，便于回滚时把记录重新插会表中 删除操作其实只是设置以下记录中的一个隐藏字段：deleted_bit，并不是真正的记录删除 为了节省磁盘空间，InnoDB有专门的purge线程来清理 deleted_bit 为true的记录。为了不影响MVCC的正常工作，purge线程自己也会维护一个read view，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。 对MVCC有帮助的实质是undo log，其实际上就是存在rollback segment回滚段中旧记录链。 比如有一个事务插入了一条新纪录，其隐式主键为1，事务ID和回滚指针为null： 现在来了第二个事务B需要修改这行记录的name。 ①对改行数据加排它锁 ②把该行数据的原始值记录到undo log，作为回滚的旧记录； ③记录完成后，修改行记录的name；修改隐藏字段trx_id为当前事务的id，假设默认从1开始，后续递增；修改隐藏字段roll_pointer指向undo log中的副本记录，表示当前记录的上一个版本是它； ④事务提交，释放行锁。 不同事务或者相同事务在对同一条记录进行修改时，会导致该记录的undo log成为一条记录版本线性表，即链表。 链首就是最新的旧记录，链尾就是最早的旧纪录。此外还要主要的是，如果事务提交之后，那么undo log种这条记录就会被清理掉。 2.4 Read View（读视图）📊 Read View就是事务进行快照读操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)。 为了方面理解Read View的可见性规则，先定义几个变量： m_ids：当前系统种那些活跃的读写事务ID，数据结构为一个list； min_limit_id：m_ids事务列表中，最小的事务id； max_limit_id：m_ids事务列表中，最大的事务id。 如果 trx_id &lt; min_limit_id，表明生成该版本的事务在生成ReadView前已经提交(因为事务ID是递增的)，所以该版本可以被当前事务访问。 如果 trx_id &gt; m_ids列表中最大的事务id，表明生成该版本的事务在生成ReadView后才生成，所以该版本不可以被当前事务访问。 如果 min_limit_id =&lt; trx_id &lt;= max_limit_id,需要判断m_ids.contains(DB_TRX_ID) 如果在，则代表Read View生成时刻，这个事务还在活跃，还没有Commit，你修改的数据，当前事务也是看不见的，所以该版本不可以被当前事务访问。； 如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，修改的结果，当前事务是能看见的，所以该版本可以被当前事务访问。。 注意：RR跟RC隔离级别，最大的区别就是：RC每次读取数据前都生成一个ReadView，而RR只在第一次读取数据时生成一个ReadView。 2.5 MVCC整体操作流程📈 了解了这些概念之后，我们来看下当查询一条记录的时候，系统如何通过MVCC找到它： 首先获取事务自己的版本号，也就是事务 ID； 获取 ReadView； 查询得到的数据，然后与 ReadView 中的事务版本号进行比较； 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照； 最后返回符合规则的数据。 MVCC相关问题 RC，RR级别下的InnoDB快照读有什么不同？RR是如何在RC级的基础上解决不可重复读的？ 当前读和快照读在RR级别下的区别： 表1： 表2： 所以我们知道事务中快照读的结果是非常依赖该事务首次出现快照读的地方，即某个事务中首次出现快照读的地方非常关键，它有决定该事务后续快照读结果的能力 我们这里测试的是更新，同时删除和更新也是一样的，如果事务B的快照读是在事务A操作之后进行的，事务B的快照读也是能读取到最新的数据的。 正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同： RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见，此后在调用快照读的时候，还是使用的是同一个Read View。 在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因 总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。 如何解决幻读 已提交读存在幻读分析 RC隔离级别下，加锁的select, update, delete等语句，使用的是记录锁，其他事务的插入依然可以执行，因此会存在幻读 可重复读存在幻读分析 RR隔离级别下，加锁的select, update, delete等语句，会使用间隙锁+ 临键锁，锁住索引记录之间的范围，避免范围间插入记录，以避免产生幻读行记录。 ","link":"http://mofish.pily.life/post/mysql_learning_08_02/"},{"title":"MySql学习之路(八)：事务及其四个隔离级别之基础知识","content":"👉事务，由一个有限的数据库操作序列构成，这些操作要么全部执行,要么全部不执行，是一个不可分割的工作单位。 目录 一、事务 1.1 什么是事务？ 1.2 为什么要用事务？ 1.3 事务的四大特性 二、如何使用事务 2.1 显示提交事务 2.2 隐式提交事务 2.3 隐式提交数据的情况 2.4 使用举例1：提交与回滚 2.5 使用举例2：测试不支持事务的engine 2.6 使用举例3：SAVEPOINT 三、事务并发存在的问题 3.1 脏读（dirty read） 3.2 不可重复读（unrepeatable read） 3.3 幻读（phantom problem） 四、事务的四大隔离级别 4.1 读未提交（Read Uncommitted） 4.2 读已提交（Read Committed） 4.3 可重复读（Repeatable Read） 4.4 串行化（Serializable）、 五、为什么大厂使用RC不使用RR？ 一、事务 在学习之前我们先建立一个数据表先： CREATE TABLE user_demo ( id INT PRIMARY KEY, name VARCHAR(32)， money INT DEFAULT 0 NOT NULL ) Engine=InnoDB CHARSET=utf8mb64; 1.1 什么是事务？ 事务，由一个有限的数据库操作序列构成，这些操作要么全部执行,要么全部不执行，是一个不可分割的工作单位。 假如A转账给B 100 元，先从A的账户里扣除 100 元，再在 B 的账户上加上 100 元。如果扣完A的100元后，还没来得及给B加上，银行系统异常了，最后导致A的余额减少了，B的余额却没有增加。所以就需要事务，将A的钱回滚回去，就是这么简单。 1.2 为什么要用事务？ 上面的例子我们可以简单的看到，如果一个业务操作涉及多个处理逻辑，当处理到一般时，服务器宕机了，那么导致这个业务操作只有一半完成，另外一半就的操作就丢失了，从而导致了脏数据，大量的脏数据会导致系统数据的不可靠性大大增加，业务也会变得不可靠。 因此使用事务，把一个事务看作MySql操作的一个单位，事务中的操作要么全部执行，要么失败后回滚都不执行。 1.3 事务的四大特性 事务的四大特性，包括原子性、一致性、隔离性、持久性，即我们常说的ACID。 原子性：事务作为一个整体被执行，事务中所有的操作要么全部执行，要么都不执行； 一致性：指在事务前后数据不会被破坏，例如A给B转账10块钱，不管事务成功与否，总金额是不会变得； 隔离性：多个事务并发访问时，事务之间是隔离得，一个事务不应该被其它事务干扰； 持久性：当事务完成提交成功后，该事务对数据的修改将持久地保存在数据库之中。 二、如何使用事务 使用事务有两种方式，分别为 显式事务 和 隐式事务 。 2.1 显示提交事务 步骤1： START TRANSACTION 或者 BEGIN ，作用是显式开启一个事务。 mysql&gt; BEGIN; #或者 mysql&gt; START TRANSACTION; START TRANSACTION 语句相较于 BEGIN 特别之处在于，后边能跟随几个 修饰符 ： ① READ ONLY ：标识当前事务是一个 只读事务 ，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。 补充：只读事务中只是不允许修改那些其他事务也能访问到的表中的数据，对于临时表来说（我们使用 CREATE TMEPORARY TABLE 创建的表），由于它们只能再当前会话中可见，所有只读事务其实也是可以对临时表进行增、删、改操作的。 ② READ WRITE ：标识当前事务是一个 读写事务 ，也就是属于该事务的数据库操作既可以读取数据， 也可以修改数据。 ③ WITH CONSISTENT SNAPSHOT ：启动一致性读。 START TRANSACTION 和 START TRANSACTION WITH CONSISTENT SNAPSHOT有什么区别： START TRANSACTION 时，是第一条语句的执行时间点，就是事务开始的时间点，第一条select语句建立一致性读的snapshot； START TRANSACTION WITH CONSISTENT SNAPSHOT时，则是立即建立本事务的一致性读snapshot，当然也开始事务了。 比如： START TRANSACTION READ ONLY; # 开启一个只读事务 START TRANSACTION READ ONLY, WITH CONSISTENT SNAPSHOT # 开启只读事务和一致性读 START TRANSACTION READ WRITE, WITH CONSISTENT SNAPSHOT # 开启读写事务和一致性读 注意： 一个事务的访问模式不能同时即设置为只读的也设置为读写的，所以不能同时把READ ONLY和READ WRITE放到START TRANSACTION语句后边。 如果我们不显式指定事务的访问模式，那么该事务的访问模式就是读写模式 步骤2： 一系列事务中的操作（主要是DML，不含DDL） 步骤3： 提交事务 或 中止事务（即回滚事务） # 提交事务。当提交事务后，对数据库的修改是永久性的。 mysql&gt; COMMIT; # 回滚事务。即撤销正在进行的所有没有提交的修改 mysql&gt; ROLLBACK; # 将事务回滚到某个保存点。 mysql&gt; ROLLBACK TO [SAVEPOINT] 其中关于SAVEPOINT相关操作有： # 在事务中创建保存点，方便后续针对保存点进行回滚。一个事务中可以存在多个保存点。 SAVEPOINT 保存点名称; # 删除某个保存点 RELEASE SAVEPOINT 保存点名称; 2.2 隐式提交事务 MySQL中有一个系统变量 autocommit ： mysql&gt; SHOW VARIABLES LIKE 'autocommit'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | autocommit | ON | +---------------+-------+ 1 row in set (0.01 sec) 当然，如果我们想关闭这种 自动提交 的功能，可以使用下边两种方法之一： 显式的的使用 START TRANSACTION 或者 BEGIN 语句开启一个事务。这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。 把系统变量 autocommit 的值设置为 OFF ，就像这样： SET autocommit = OFF; #或 SET autocommit = 0; 2.3 隐式提交数据的情况 数据定义语言（Data definition language，缩写为：DDL） 数据库对象，指的就是数据库、表、视图、存储过程等结构。当我们CREATE、ALTER、DROP等语句去修改数据库对象时，就会隐式的提交前边语句所属于的事物。即： BEGIN; SELECT ... # 事务中的一条语句 UPDATE ... # 事务中的一条语句 ... # 事务中的其他语句 CREATE TABLE ... # 此语句会隐式的提交前边语句所属于的事务 隐式使用或修改mysql数据库中的表 当我们使用ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD等语句时也会隐式的提交前边语句所属于的事务。 事务控制或关于锁定的语句 ① 当我们在一个事务还没提交或者回滚时就又使用 START TRANSACTION 或者 BEGIN 语句开启了另一个事务时，会隐式的提交上一个事务。即： BEGIN; SELECT ... # 事务中的一条语句 UPDATE ... # 事务中的一条语句 ... # 事务中的其他语句 BEGIN; # 此语句会隐式的提交前边语句所属于的事务 ② 当前的 autocommit 系统变量的值为 OFF ，我们手动把它调为 ON 时，也会 隐式的提交前边语句所属的事务。 ③ 使用 LOCK TABLES 、 UNLOCK TABLES 等关于锁定的语句也会 隐式的提交 前边语句所属的事务。 加载数据的语句 使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。 关于MySQL复制的一些语句 使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句会隐式的提交前边语句所属的事务 其他的一些语句 使用ANALYZE TABLE、CACHE INDEX、CAECK TABLE、FLUSH、LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前边语句所属的事务。 2.4 使用举例1：提交与回滚 情况1： CREATE TABLE user (name varchar(20), PRIMARY KEY (name)) ENGINE=InnoDB; BEGIN; INSERT INTO user SELECT '张三'; COMMIT; INSERT INTO user SELECT '李四'; INSERT INTO user SELECT '李四'; ROLLBACK; 运行结果（2 行数据）： mysql&gt; SELECT * FROM user; +--------+ | name | +--------+ | 张三 | | 李四 | +--------+ 2 行于数据集 (0.01 秒) 情况2： CREATE TABLE user(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB; SET @@completion_type = 1; BEGIN; INSERT INTO user SELECT '张三'; COMMIT; INSERT INTO user SELECT '李四'; INSERT INTO user SELECT '李四'; ROLLBACK; SELECT * FROM user; 运行结果（1 行数据）： mysql&gt; SELECT * FROM user; +--------+ | name | +--------+ | 张三 | +--------+ 1 行于数据集 (0.01 秒) 当我们设置 autocommit=0 时，不论是否采用 START TRANSACTION 或者 BEGIN 的方式来开启事 务，都需要用 COMMIT 进行提交，让事务生效，使用 ROLLBACK 对事务进行回滚。 当我们设置 autocommit=1 时，每条 SQL 语句都会自动进行提交。 不过这时，如果你采用 START TRANSACTION 或者 BEGIN 的方式来显式地开启事务，那么这个事务只有在 COMMIT 时才会生效， 在 ROLLBACK 时才会回滚。 2.5 使用举例2：测试不支持事务的engine CREATE TABLE test1(i INT) ENGINE=InnoDB; CREATE TABLE test2(i INT) ENGINE=MYISAM; 针对于InnoDB表 BEGIN; INSERT INTO test1 VALUES(1); ROLLBACK; SELECT * FROM test1; 结果：没有数据 针对于MYISAM表： BEGIN; INSERT INTO test1 VALUES(1); ROLLBACK; SELECT * FROM test2; 结果：有一条数据 2.6 使用举例3：SAVEPOINT 创建表并添加数据： CREATE TABLE account( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(15), balance DECIMAL(10,2) ); INSERT INTO account(NAME,balance) VALUES ('张三',1000), ('李四',1000); BEGIN; UPDATE account SET balance = balance - 100 WHERE NAME = '张三'; UPDATE account SET balance = balance - 100 WHERE NAME = '张三'; SAVEPOINT s1; # 设置保存点 UPDATE account SET balance = balance + 1 WHERE NAME = '张三'; ROLLBACK TO s1; # 回滚到保存点 结果：张三：800.00 ROLLBACK; 结果：张三：1000.00 三、事务并发存在的问题 对事务有个大概了解后，我们还得清楚事务并发执行会存在什么问题，即一个事务是怎么干扰到其它事务的，我们现在刚刚的表上添加几条数据： 3.1 脏读（dirty read） 假设现在有两个事务A、B： 小狗现在99元，事务A正在准备查询小狗的余额； 这时候，事务B扣减了小狗的余额，扣了29块钱 最后A读到的时候扣减后的余额，即70块钱 时间编号 事务A 事务B ① Begin; ② Begin; ③ update user_demo set money=money-29 where name = '小狗'; ④ select money from user_demo where name = '小狗'; ⑤ 读到小狗的 money 的值为 99 - 29 = 70 由上面数据可发现，事务A、B交替执行没有隔离，事务A被事务B干扰了，导致事务A读取到了事务B未提交的数据，这就是脏读。 3.2 不可重复读（unrepeatable read） 假设现在有两个事务C、D： 事务C先查询到了小红的余额84元； 事务B对小红的余额进行扣减，扣去4元； 事务C再去查询小红的余额，此时因为事务D为提交，所以读取出来的还是84元； 事务B提交事务； 事务C再去查询小红的余额，发现此时余额变成了80元。 时间编号 事务C 事务D ① Begin; ② select money from user_demo where name = '小红'; ③ 读到小红的余额为84元 ④ Begin; ⑤ update user_demo set money=money-4 where name = '小红'; ⑥ select money from user_demo where name = '小红'; ⑦ 读到小红的余额为84元 ⑧ commit; ⑥ select money from user_demo where name = '小红'; ⑦ 读到小红的余额为80元 从上面表格可以看到，虽然事务D提交之前的修改对事务C没有影响，但是事务D提交之后，事务C还是收到了干扰，在一个事务中两次相同的查询的结果却不一致，这就是不可重复读。 3.3 幻读（phantom problem） 假设现在有两个事务E、F： 事务E查询余额大于75的账户记录，得到小明78元，小红80元两条记录； 事务F开启，插入一条小花余额为96的数据，并且提交了； 事务E再去执行相同的查询，却得到了小明、小红和小花三条数据。 时间编号 事务E 事务F ① Begin; ② select * from user_demo where money &gt; 75; ③ 读到小明78元，小红80元两条记录 ④ Begin; ⑤ insert into user_demo (name, money) value ('小花', 96); ⑥ commit; ⑦ select * from user_demo where money &gt; 75; ⑧ 读到小明78元，小红80元，小花96元三条记录 事务E查询到一个范围的结果集，另外一个事务F往这个范围中插入/删除了数据，并进行了提交，当事务E在当前事务再次查询相同的范围时，两次读取的结果却不一样，这就是幻读。 不可重复读和幻读都是两次读到的数据不一致，那么它们有什么区别呢？ 不可重复读的重点是修改，同样的条件，第1次和第2次读取的值不一样。 幻读的重点在于新增或者删除，同样的条件， 第1次和第2次读出来的记录数不一样。 从控制角度来看，不可重复读只需要锁住满足条件的记录，幻读要锁住满足条件及其相近的记录。 四、事务的四大隔离级别 虽然并发事务存在脏读、不可重复读和幻读等问题，但是InnoDB实现了下面四种隔离级别来应对： 读未提交（Read Uncommitted） 读已提交（Read Committed） 可重复读（Repeatable Read） 串行化（Serializable） # 设置隔离级别(当前会话有效) SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE # 设置隔离级别(全局有效) SET GLOBAL TRANSACTION ISOLATION LEVEL READ UNCOMMITTED SET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED SET GLOBAL TRANSACTION ISOLATION LEVEL REPEATABLE READ SET GLOBAL TRANSACTION ISOLATION LEVEL SERIALIZABLE # 查询当前隔离级别 SHOW [GLOBAL|SESSION] VARIABLES LIKE 'transaction_isolation'; SELECT @@session.transaction_isolation; SELECT @@global.transaction_isolation; 4.1 读未提交（Read Uncommitted） 如果一个事务读到了另一个未提交事务修改过的数据，那么这种隔离级别就称之为未提交读（READ UNCOMMITTED），示意图如下： 编号 事务A 事务B ① Begin; ② Begin; ③ update user_demo set name='小猫' where id=3; ④ select name from user_demo where id=3; ⑤ 读到的名称为'小猫' 在未提交读的隔离级别下，事务A可以读到事务B未提交的结果。 除此之外，如果事务B回滚了，那么事务A读到的则是不存在的数据： 编号 事务A 事务B ① Begin; ② Begin; ③ update user_demo set name='小猫' where id=3; ④ select name from user_demo where id=3; ⑤ ROLLBACK; ⑥ 读到的名称为'小猫' 以上现象我们称之为”脏读“，是一种级别最低，最不安全的一种隔离级别，基本不会使用。 4.2 读已提交（Read Committed） 如果一个事务只能读到另一个已经提交的事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值，那么这种隔离级别就称之为已提交读（READ COMMITTED），如图所示： 编号 事务A 事务B ① Begin; ② Begin; ③ update user_demo set name='小猫' where id=3; ④ select name from user_demo where id=3; ④ 因为未提交，所以读到的名称为'小狗' ⑤ COMMIT; ⑥ select name from user_demo where id=3; ⑦ 读到的名称为'小猫' 虽然读已提交解决了脏读的问题，但是从上面的例子也可以看到，在事务A的执行过程中，如果有别的事务修改了业务数据，那么会导致事务A在修改前后读的数据会不一样，这种现象我们称之为不可重复读。 这种隔离级别生产情况下我们也很少用到，因为事务之间的隔离性还是有缺陷。 4.3 可重复读（Repeatable Read） 在一些业务场景中，一个事务只能读到另一个已经提交的事务修改过的数据，但是第一次读过某条记录后，即使其他事务修改了该记录的值并且提交，该事务之后再读该条记录时，读到的仍是第一次读到的值，而不是每次都读到不同的数据。那么这种隔离级别就称之为可重复读（REPEATABLE READ），如图所示： 情况一：其它事务更改数据并提交 编号 事务A 事务B ① Begin; ② Begin; ③ select name from user_demo where id=3; ④ 读到的名称为'小狗' update user_demo set name='小猫' where id=3; ④ COMMIT; ⑤ select name from user_demo where id=3; ⑥ 读到的名称为'小狗' 事务B修改并提交后，事务A重复读取该数据是相同的，不用因为事务B的修改而影响，因此可重复读这种隔离级别解决了不可重复读的问题。 情况二：其它事务插入或者删除数据 编号 事务A 事务B ① Begin; ② Begin; ③ select name from user_demo where id&gt;=3; ④ 读到的名称为'小狗' 的一条数据 insert into user_demo (name, money) values ('小黄', 18); ⑤ COMMIT; ⑥ select name from user_demo where id&gt;=3; select name from user_demo where id&gt;=3; ⑦ 读到的名称为'小狗'的一条数据 读到的名称为'小狗'和'小黄'的两条数据 事务B修改并提交后，实际已有2条符合要求的数据，但是事务A重复读取该数据时是相同的，不用因为事务B的插入而影响，因此可重复读这种隔离级别可部分解决了幻读的问题，但是也有例外。 情况三：其它事务插入数据，当前事务更新了该数据 编号 事务A 事务B ① Begin; ② Begin; ③ select name from user_demo where id&gt;=3; ④ 读到的名称为'小狗' 的一条数据 insert into user_demo (name, money) values ('小黄', 18); ④ COMMIT; ⑤ select name from user_demo where id&gt;=3; select name from user_demo where id&gt;=3; ⑦ 读到的名称为'小狗的一条数据 读到的名称为'小狗'和'小黄'的两条数据 ⑧ update user_demo set money = 88 where id = 4; ⑨ select name from user_demo where id&gt;=3; ⑩ 读到的名称为'小狗'和'小黄'的两条数据 ①~⑦的步骤和结果跟情况二都是一样的，没有出现幻读的情况，但是当事务A在第⑧步的时候对新增的数据进行修改后，再读取数据则会出现幻读的情况，因此可重复读这种隔离级别还是有可能会出现幻读的问题。 解决的方案后面会给出😘 4.4 串行化（Serializable） 以上3种隔离级别都允许对同一条记录进行读-读、读-写、写-读的并发操作，如果我们不允许读-写、写-读的并发操作，可以使用串行化（Serializable）隔离级别，如下： 编号 事务A 事务B ① Begin; ② Begin; ③ update user_demo set name = '小狗-1' where id = 3; ④ select name from user_demo where id =3; ④ 等待中... ⑤ COMMIT; ⑥ 读到的名称为'小狗-1'的数据 事务B对id为3的记录进行操作时，事务A访问或者操作这条记录时就会被卡住，需要等事务B提交之后，事务A才能操作，即串行化操作。 这个一般生产环境也不会用，并发能力太差了。 五、为什么大厂使用RC不使用RR？ 在RR隔离级别下，存在Gap Lock，导致出现死锁的几率比RC大的多，而RC 在加锁的过程中，是不需要添加Gap Lock和 Next-Key Lock 的，只对要修改的记录添加行级锁就行了； 在RR隔离级别下，条件列未命中索引会锁表！而在RC隔离级别下，只锁行； 在RC隔离级别下，半一致性读(semi-consistent)特性增加了update操作的并发性（当 Update 语句的 where 条件中匹配到的记录已经上锁，会再次去 InnoDB 引擎层读取对应的行记录，判断是否真的需要上锁（第一次需要由 InnoDB 先返回一个最新的已提交版本））。 此外，使用RC，服务器会自动使用基于row 格式的日志记录。 原因是因为RR级别下，因为有临键锁的关系，事务A对数据进行操作时会锁住相关数据，导致事务B需要等事务A提交后才能操作，此时statement记录是按照commit顺序记录的，所以不会有问题。 但是如果在RC级别下，会出现事务A操作了数据把age = 10的数据更新sort为100，事务B把数据 age = 20 的数据更新 age 为 10并提交，然后A再提交。 事务A 事务B ① begin; ② select * from student where age =10 for update; begin; ③ update student set sort=100 where age=10; ④ update student set age=10 where age=20; ⑤ commit; ⑥ commit; 如果使用statment记录的话，按照commit顺序记录，会导致事务B的数据的sort也会变成100，以下是伪日志： 事务B： begin； update student set age=10 where age=20; commit; 事务A： begin； update student set sort=100 where age=10; commit; 如果是row格式，那么会详细记录事务修改的是那条数据，从而避免上述的情况，那么伪日志记录如下： 事务B： begin； update t where id=x and age = 20 set age=10; commit; 事务A： begin； update t where id=y and age=10 set sort = 100; commit; 总的来说就是减少死锁、提高并发。 👉👉👉至此，事务及其四个隔离级别的基础知识就差不多了，至于这四个隔离级别的实现原理我们在下一章节再学习😚。 ","link":"http://mofish.pily.life/post/mysql_learning_08_01/"},{"title":"MySql学习之路(七)：读写分离、主从复制、主从延迟","content":"👉 你们有没有做 MySQL 读写分离？如何实现 MySQL 的读写分离？MySQL 主从复制原理的是啥？如何解决 MySQL 主从同步的延时问题？ 一、读写分离 为什么要做读写分离？因为在大流量网站，往往会出现读多写少的情况，如果把读写都放到一个库里面，可能会影响数据的写入和更新。 所以针对这一情况，就有了读写分离的概念。使用一个主库来处理写操作，多个从库来处理读操作，把读写操作分离开，即可不影响写操作，同时也能支持更多的读操作，提供读的并发量。 既然只有主库才会写数据，那么从库是怎么读到新写入的数据的呢？接下来我们就得提到“主从复制”。 二、主从复制 主从，即有主数据库(Master)，也有从数据库(Slave)，当主数据库变化时，把主数据库的数据复制给从数据库，使得主从数据变得完全一致。 2.1 主从复制的作用 读写分离，使数据库能支撑更大的并发 主从只负责各自的写和读，极大程度的缓解X锁和S锁争用。在报表中尤其重要。由于部分报表sql语句非常的慢，导致锁表，影响前台服务。如果前台使用master，报表使用slave，那么报表sql将不会造成前台锁，保证了前台速度。 实时灾备（数据备份） 可作为后备数据库，当主库发生故障时，可切换到从库继续工作，提供系统可用性 高可用性 数据备份实际上是一种冗余的机制，通过这种冗余的方式可以换取数据库的高可用性，也就是当服务器出现故障或宕机的情况下，可以切换到从服务器上，保证服务的正常运行。 架构扩展 当业务量越来越大的时候，一主一丛或者多从已经满足不了性能，此时需要多主多从，增加物理服务器，减轻数据库负担，那么这多主多从之间也需要主从复制。 2.2 主从复制的必要条件 主库开启binlog日志 主从server_id需不同 从服务器能连接主服务器 2.3 主从复制的原理 主库将数据变更的逻辑写入到binlog，然后从库连接到主库后，主库会有一个log dump线程把binlog发送给从库，从库有个I/O线程将主库传过来的binlog日志内容写入到relay log中，之后从库有一个SQL线程会从relay log读取内容，把内容写入到库中，使得主从数据一致。 relay log是什么？ relay log又叫中续日志，一般情况下它在MySql主从复制读写分离集群的从节点才开启，主节点一般不需要这个日志，可通过修改my.cnf中relay-log配置来开启。 它是一个中介临时的日志文件，用于存储从主库同步过来的binlog日志内容，所以它里面的内容和binlog日志内容是一致的，然后slave从节点从这个relaylog日志文件中读取数据应用到数据库中，来实现数据的主从复制。 2.4 主从复制的具体流程 主库db的更新事件(update、insert、delete)被写到binlog； 从库发起连接，连接到主库； 此时主库创建一个binlog dump thread线程，把binlog的内容发送到从库； 从库启动之后，创建一个I/O线程，读取主库传过来的binlog内容并写入到relay log； 还会创建一个SQL线程，从relay log里面读取内容，从Exec_Master_Log_Pos位置开始执行读取到的更新事件，将更新内容写入到slave的db。 2.5 如果主服务器挂了怎么办(手动切换)？ 假设发生了突发事件，master宕机，需要手动切换把从库提升为主库需要怎么做？ 停止从库 IO_THREAD 线程 在从库中执行stop slave io_thread;，使从库断开接收主库的信息，有利于从库完成剩余的数据同步。 确认slave完成所有同步 在从库中执行show processlist;，直到看到Has read all relay log,则表示从库更新都执行完毕了。 选择合适的从库作为主库 登陆所有从库，查看master.info文件，对比选择pos最大的作为新的主库 切换从库为主库 执行stop slave来完全停止从库的复制工作 执行reset salve all来完全清空slave信息 执行reset master来清空本机上的master信息 检查新主库配置 检查read-only是否已经关闭 检查是否开启binlog等等 修改后重启生效 调整其它从库，连接到新的主库中 测试master-slave是否能正常通信和正常同步 2.6 如何主库突然宕机，数据来不及同步到从库导致数据丢失怎么办？ 传统的异步同步，主库写事务事件到二进制日志，从库索取主库日志，这不能确保事务事件被传送到从库。而对于全同步（fully synchronous replication）复制，主库提交事务，必须等待从库也提交这个事务成功，才能完成这个事务，这样容易造成事务的延迟。所以，出现了半同步，。 因此MySql中有个介于异步和全同步之间的半同步复制机制，所谓的半同步复制就是master每commit一个事务(简单来说就是做一个改变数据的操作）,要确保slave接受完主服务器发送的binlog日志文件并写入到自己的中继日志relay log里，然后会给master信号，告诉对方已经接收完毕，这样master才能把事物成功commit。这样就保证了master-slave的数据绝对的一致（但是以牺牲master的性能为代价)。 主从库都需要安装半同步复制插件，并开启半同步复制才能生效。 --- 主库开启 install plugin rpl_semi_sync_master soname 'semisync_master.so'; show global variables like 'rpl_semi_sync%'; set global rpl_semi_sync_master_enabled=on; show global status like 'rpl_semi_sync%'; ## my.cnf参数配置 rpl_semi_sync_master_wait_for_slave_count=1; # 等待直到至少一个salve接收并记录了事件后才提交事务 --- 从库开启 install plugin rpl_semi_sync_slave soname 'semisync_slave.so'; show global variables like 'rpl_semi_sync%'; set global rpl_semi_sync_slave_enabled=on; show global variables like 'rpl_semi_sync%'; stop slave IO_THREAD; start slave IO_THREAD; 2.7 组复制技术 异步复制和半同步复制都无法最终保证数据的一致性问题，半同步复制是通过判断从库响应的个数来决定是否返回给客户端，虽然数据一致性相比于异步复制有提升，但仍然无法满足对数据一致性要求高的场景，比如金融领域。MGR 很好地弥补了这两种复制模式的不足。 组复制技术，简称 MGR（MySQL Group Replication）。是 MySQL 在 5.7.17 版本中推出的一种新的数据复制技术，这种复制技术是基于 Paxos 协议的状态机复制。 那么MGR 是如何工作的？ 首先我们将多个节点共同组成一个复制组，在 执行读写（RW）事务 的时候，需要通过一致性协议层 （Consensus 层）的同意，也就是读写事务想要进行提交，必须要经过组里“大多数人”（对应 Node 节 点）的同意，大多数指的是同意的节点数量需要大于 （N/2+1），这样才可以进行提交，而不是原发起方一个说了算。而针对 只读（RO）事务 则不需要经过组内同意，直接 COMMIT 即可。 在一个复制组内有多个节点组成，它们各自维护了自己的数据副本，并且在一致性协议层实现了原子消息和全局有序消息，从而保证组内数据的一致性。 三、主从延迟 竟然有主从复制，那么主库把数据复制到从库时，中间肯定有个时间差，从而导致了主从延迟。但是除了正常的延迟外，可能还会有其它异常导致主从延迟，下面我们继续学习一下。 3.1 如何查看主从是否延迟？ 在从库上执行show slave satus;可以看到很多主从同步的信息： Seconds_Behind_Master：不为0的时候，或者说数值很大的时候。 Relay_Log_File和Master_Log_File显示binlog的编号相差很大，说明binlog在从库上没有及时的同步，所以近期执行的binlog和当前IO线程读取的binlog编号会相差比较大。 从库的目录下存在着大量的relay log，一般来说同步完成后会自动把relay log删除掉，如果大量存在说明主从复制延迟很厉害，导致很多relay log没执行。 3.2 主从延迟的原因和解决方案还有哪些？ 主从之间有网络延迟，导致同步binlog异常； 从库较多，因为主服务器因为向多个从服务器发送更新的二进制日志过程是串行的，所以从库较多时，会出现部分从库同步较慢的情况 主库写并发较高，产生的DDL数量超过slave一个sql线程所能承受的范围 随机重放，SQL线程对数据重放是一个随机写盘的过程，因为相较于顺序写盘，效率会慢一点 锁等待，当从库的查询量大或者有锁等待时，导致执行relay log时需要待锁解除后此啊能继续执行 从库中因为sql_thread只有单线程，因此执行relay log时如果有个DDL卡住了，那么所有之后的DDL会等待这个DDL执行完才会继续执行，这就导致了延时，但是可以使用并行复制来解决。 “主库上那个相同的DDL也需要执行10分，为什么slave会延时？”，答案是master可以并发，Slave_SQL_Running线程却不可以，后面我们会再次介绍什么是并行复制。 3.3 解决主从延迟的方法有哪些？ 硬件方面 采用性能好一点的服务器，使MySql的性能更好，支撑更大的业务量 主从尽量放到同一局域网内，减少网络延迟，提供binlog传输速度 架构方面 业务采用分库架构，mysql服务可平行扩展，分散每个库的压力 当从库较多时，可使用级联结构，减轻主库同步binlog的压力 加入缓存层，减轻MySql的读压力 主从库设计 开启并行复制，既然 SQL 单线程进行重放时速度有限，MySQL 5.6 版本后，提供了一种并行复制的方式，通过将 SQL 线程转换为多个 work 线程来进行重放。 如果不是级联模式，从库可关闭生成binlog以及关闭logs-slave-updates设置 代码层面 当出现慢查询和锁等待时，需要考虑执行的业务sql是否有问题或者需要优化 写入主库后，马上读取主库数据，确保强一致，但是这样读写分离的意义就不大，甚至提高了主库的压力 四、额外问题 4.1 什么是并行复制 MySQL的从库是要通过IO_thread去拉取主库上的binlog的，然后存入本地，落盘成relay log，通过sql_thread来应用这些relay log。 在MySQL5.6之前的版本中，当主库上有多个线程并发执行SQL时，sql_thread只有一个，在某些TPS比较高的场景下，会出现主库严重延迟的问题。MySQL为了解决这个问题，将sql_thread演化了多个worker的形式，在slave端并行应用relay log中的事务，从而提高relay log的应用速度，减少复制延迟。这就是并行复制的由来。 -- 主库配置 -- 表示 binlog 提交后等待延迟多少时间再同步到磁盘，默认0 ，不延迟。当设置为 0 以上的时候，就允许多个事务的日志同时一起提交，也就是我们说的组提交。组提交是并行复制的基础，我们设置这个值的大于 0 就代表打开了组提交的功能。 binlog_group_commit_sync_delay=100 -- 表示等待延迟提交的最大事务数，如果上面参数的时间没到，但事务数到了，则直接同步到磁盘。若 binlog_group_commit_sync_delay 没有开启，则该参数也不会开启。 binlog_group_commit_sync_no_delay_count=10 -- 从库配置 -- slave_parallel_type：默认值DATABASE，基于库的并行复制方式；LOGICAL_CLOCK基于组提交的并行复制方式（与主库的组提交对应） slave_parallel_type=logical_clock -- 开启多个work线程并行复制，slave_parallel_workers不要设为1，否则要经过一层负载器，更慢，不如关闭 slave_parallel_workers=8 -- 参数在多线程复制环境下，能够保证从库回放relay log事务的顺序与这些事务在relay log中的顺序完全一致，也就是与主库提交的顺序完全一致。 slave_preserve_commit_order=1 -- 开启MTS功能后，务必将参数master_info_repostitory设置为TABLE，这样性能可以有50%~80%的提升。这是因为并行复制开启后对于元master.info这个文件的更新将会大幅提升，资源的竞争也会变大。在之前InnoSQL的版本中，添加了参数来控制刷新master.info这个文件的频率，甚至可以不刷新这个文件。因为刷新这个文件是没有必要的，即根据master-info.log这个文件恢复本身就是不可靠的。在MySQL 5.7中，Inside君推荐将master_info_repository设置为TABLE，来减小这部分的开销。 master_info_repository=TABLE relay_log_info_repository=TABLE relay-log-recovery = ON log_slave_updates=0 4.2 为什么主从的server_id需要不一样？ server-id用于标识数据库实例，防止在链式主从、多主多从拓扑中导致SQL语句的无限循环。 当主库和备库server-id重复时 由于默认情况replicate-same-server-id=0，因此备库会跳过所有主库同步的数据，导致主从数据的不一致。 当两个备库server-id重复时 会导致从库跟主库的连接时断时连，产生大量异常。根据MySQL的设计，主库和从库通过事件机制进行连接和同步，当新的连接到来时，如果发现server-id相同，主库会断开之前的连接并重新注册新连接。当A库连接上主库时，此时B库连接到来，会断开A库连接，A库再进行重连，周而复始导致大量异常信息。 4.3 主主、主从、主备有什么不同？ 主主是为了减轻单台服务器的压力，使用多主来分散压力。 主主复制是通过互为从节点来实现同步，通过增值的规避解决主键冲突，但是如果两个节点同时写入时，仍然会有两个节点同时修改数据的情况，所以此种主主模式一般仅用于快速切换主备，不用来直接同时提供双写入。 主从是为了读写分离，减轻主库压力，从库只能读不能写，同步数据的话只能从主库同步到从库。 主备即读写都在主库，单一主库能够应该工作时使用，从库只是用来备份数据 4.4 GTID主从 与 传统主从复制 MySQL 5.6 的新特性之一，全局事务标识符（GTID）是创建的唯一标识符，并与在源（主）服务器上提交的每个事务相关联。此标识符不但是唯一的，而且在给定复制设置中的所有服务器上都是唯一的。所有交易和所有GTID之间都有一对一的映射关系 。它由服务器ID以及事务ID组合而成。这个全局事务ID不仅仅在原始服务器上唯一，在所有存在主从关系 的mysql服务器上也是唯一的。正是因为这样一个特性使得mysql的主从复制变得更加简单，以及数据库一致性更可靠。一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱或者主从不一致。 https://www.cnblogs.com/Confession/p/7777577.html ","link":"http://mofish.pily.life/post/mysql_learning_07/"},{"title":"MySql学习之路(六)：explain执行计划详解","content":"使用explain关键字可以模拟优化器执行SQL查询语句，从而知道MySql是如何处理你的SQL语句的，分析你的查询语句或是表结构的性能瓶颈。 explain执行计划包含信息 其简单含义如下： 字段 含义 id 该语句的唯一标识 select_type 查询类型 table 表名 partitions 匹配的分区 type 联接类型 possible_keys 可能的索引选择 key 实际选择的索引，如果没有选择索引,键是NULL。 key_len 索引的长度（单位：字节），值越大越好，如果键是NULL，则长度为NULL。 ref 索引的哪一列被引用了，如果可能，是一个常量const rows 根据表统计信息以及索引选用情况，大致估算出找到所需的记录所需要读取的行数，值越小越好。 filtered 显示了通过条件过滤出的行数的百分比估计值。 Extra 附加信息 其中比较重要的字段会单独拧出来讲：id、select_type、type、key、rows、Extra 一、id select查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序。 1、id相同 执行顺序由上至下 2、id不同 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 有一点值得注意的是，查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询，使得id相同。 所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划即可。 小结： id如果相同，可以认为是一组，从上往下顺序执行 在所有组中，id值越大，优先级越高，越先执行 关注点；id号每个号码，标识一趟独立的查询，一个sql的查询躺数越少越好 二、select_type 查询类型，主要用于区别普通查询，联合查询，子查询等的复杂查询，常见部分类型如下： 1、SIMPLE 简单的select查询，查询中不包含子查询或者union 2、PRIMARY 查询中包含任何复杂的子部分，最外层查询被标记 3、SUBQUERY 在 select 或 where 列表中包含子查询 4、DERIVED 在 from 列表中包含的子查询被标记为 DERIVED(衍生)，Mysql会递归执行这些子查询，把结果放在临时表里 5、UNION 和 UNION RESULT UNION：当通过 union 来连接多个查询结果时，第二个之后的 select 其 select_type 为 UNION UNION RESULT：从union表获取结果的select 三、type 访问类型，sql查询优化中一个很重要的指标，结果值从好到坏依次是： system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL 注意：黑色标注的是常见的类型 3.1 system👇 表只有一行记录（等于系统表），这是const类型的特例，平时不会出现，可以忽略不计。 3.2 const👇 示通过索引一次就找到，const用于比较primary key或者unique索引。因为只匹配一行数据，所以如果将主键置于where列表中，mysql能将该查询转换为一个常量。 3.3 eq_ref👇 唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键 或 唯一索引扫描。 3.4 ref👇 非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问，它返回所有匹配某个单独值的行，可能会找多个符合条件的行，属于查找和扫描的混合体。 3.5 range👇 只检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般就是where语句中出现了between,in等范围的查询。这种范围扫描索引扫描比全表扫描要好，因为它开始于索引的某一个点，而结束另一个点，不用全表扫描。 3.6 index👇 index:index 与all区别为index类型只遍历索引树。通常比all快，因为索引文件比数据文件小很多，数据在索引就能拿到。 3.7 all👇 遍历全表以找到匹配的行 注意:一般保证查询至少达到range级别，最好能达到ref。 四、Extra 顾名思义，Extra列是用来说明一些额外信息的，包含不适合在其他列中显示但十分重要的额外信息。我们可以通过这些额外信息来更准确的理解MySQL到底将如何执行给定的查询语句。MySQL提供的额外信息有好几十个，我们就不一个一个介绍了，所以我们只挑选比较重要的额外信息介绍给大家。 4.1 Using filesort👇 mysql对数据使用一个外部的索引排序，而不是按照表内的索引进行排序读取。也就是说mysql无法利用索引完成的排序操作成为“文件排序”： 由于索引是先按email排序、再按address排序，所以查询时如果直接按address排序，索引就不能满足要求了，mysql内部必须再实现一次“文件排序”。 4.2 Using temporary👇 使用临时表保存中间结果，也就是说mysql在对查询结果排序时使用了临时表，常见于order by 和 group by： 4.3 Using index👇 表示相应的select操作中使用了覆盖索引（Covering Index），避免了访问表的数据行，效率高 如果同时出现Using where，表明索引被用来执行索引键值的查找（参考上图） 如果没用同时出现Using where，表明索引用来读取数据而非执行查找动作： 覆盖索引（Covering Index）：也叫索引覆盖。就是select列表中的字段，只用从索引中就能获取，不必根据索引再次读取数据文件，换句话说查询列要被所建的索引覆盖。 4.4 Using where👇 使用了where过滤 4.5 Using join buffer👇 使用了链接缓存 4.6 Impossible WHERE where子句的值总是false，不能用来获取任何元祖。 4.7 select tables optimized away 在没有group by子句的情况下，基于索引优化MIN/MAX操作或者对于MyISAM存储引擎优化COUNT（*）操作，不必等到执行阶段在进行计算，查询执行计划生成的阶段即可完成优化。 4.8 distinct 优化distinct操作，在找到第一个匹配的元祖后即停止找同样值得动作。 五、key_len 实际使用到的索引长度 (即：字节数) 帮你检查是否充分的利用了索引，值越大越好，主要针对于联合索引，有一定的参考意义。 -- key_len的长度计算公式： varchar(10)变长字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+1(NULL)+2(变长字段) varchar(10)变长字段且不允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+2(变长字段) char(10)固定字段且允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1)+1(NULL) char(10)固定字段且不允许NULL = 10 * ( character set：utf8=3,gbk=2,latin1=1) ","link":"http://mofish.pily.life/post/mysql_learning_06/"},{"title":"MySql学习之路(五)：redo log、undo log、binlog 的总结","content":"👏 本文主要总结下重做日志(redo log)、回滚日志(undo log)、二进制日志(binlog)的概念，了解一下它们的适用场景和不同之处。 前言 redo log 是物理日志，undo log 和 binlog 是逻辑日志，物理日志的恢复速度远快于逻辑日志🚀。 物理日志记录：在某个数据页上做了什么修改 逻辑日志记录：👉这个语句的原始逻辑，简单理解为记录的就是sql语句，比如“给 ID=2 这一行的 c 字段加 1 ” 一、InnoDB存储引擎 redo log 和 undo log是InnoDB存储引擎中特有的日志文件，redo log 是重做日志，提供前滚操作，undo log 是回滚日志，提供回滚操作，其中前者保证事务的持久性，后者保证事务的原子性，两者可以统称为事务日志。 前滚：事务提交过程中数据库实例奔溃了，导致该事务只有部分已提交，那么当数据库实例恢复正常时，就需要用前滚操作把剩下的事务完全提交。 回滚：事务未提交或提交失败了，当数据库实例恢复正常时，就需要用回滚操作把数据回滚到操作之前的样子。 所以在学习 redo log 和 undo log之前，我们再重新了解一下上一章提到得 buffer pool，下图是InnoDB存储引擎结构： buffer pool 是物理页的缓存，对 InnoDB 的任何修改操作都会首先在 buffer pool 的 page 上进行，然后这样的页面将被标记为 dirty 并被放到专门的flush list 上，后续将由专门的刷脏线程阶段性的将这些页面写入磁盘。这样的好处是避免每次写操作都操作磁盘导致大量的随机 IO，阶段性的刷脏可以将多次对页面的修改 merge 成一次IO操作，同时异步写入也降低了访问的时延。 然而，如果在 dirty page 还未刷入磁盘时，server非正常关闭，这些修改操作将会丢失，如果写入操作正在进行，甚至会由于损坏数据文件导致数据库不可用。为了避免上述问题的发生，Innodb 将所有对页面的修改操作写入一个专门的文件，并在数据库启动时从此文件进行恢复操作，这个文件就是 redo log。这样的技术推迟了 buffer pool 页面的刷新，从而提升了数据库的吞吐，有效的降低了访问时延。带来的问题是额外的写 redo log 操作的开销（顺序 IO，比随机 IO 快很多），以及数据库启动时恢复操作所需的时间。 二、undo log 2.1 简述 Innodb 存储引擎的最大特点就是支持事务，如果事务提交失败，那么该事务中所有的操作都必须回滚到执行前的样子，而这个回滚的操作，就是利用 undo log 文件完成的。 2.2 undo log是怎么保证事务的原子性的 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。 而 undo log 主要记录了数据的逻辑变化来实现事务的原子性，简单的说比如一条 INSERT 语句，对应一条DELETE 的 undo log ，对于每个 UPDATE 语句，对应一条相反的 UPDATE 的 undo log ，这样在发生错误时，就能回滚到事务之前的数据状态。 当然实际情况会更加复杂，undo log不单单存储时是一个逆向操作，而是一个数据副本组成的版本链记录，具体可看《MySQL · 引擎特性 · InnoDB undo log 漫游》 此外，undo log会产生redo log，也就是undo log的产生会伴随redo log的产生，这是因为undo log也需要持久化性的保护。 2.3 undo log的作用 作用1：回滚数据 作用2：MVCC undo的另一个作用是MVCC，即在InnoDB存储引擎中MVCC的实现是通过undo来完成。当用户读取一行记录时，若该记录以及被其他事务占用，当前事务可以通过undo读取之前的行版本信息，以此实现非锁定读取。 2.4 工作流程 在准备更新一条SQL语句的时候，该条语句对应的数据已经被加载到 buffer pool 中了，实际上这里还有这样的操作，就是在将该条语句加载到 buffer Pool 中的时候同时会往 undo log 日志文件中插入一条日志，也就是将 id=1 的这条记录的原来的值记录下来，便于事务失败后进行回滚： 到这一步，我们执行的 SQL 语句对应的数据已经被加载到 buffer Pool 中了，然后开始更新这条语句，更新的操作实际是在 buffer Pool中执行的。 那问题来了，更新完数据之后，buffer Pool缓冲池中的中的数据就会和数据库中的数据库不一致，那就是说Buffer Pool 中的数据成了脏数据？ 没错，目前这条数据就是脏数据，buffer Pool 中的记录是“小强”数据库中的记录是“旺财” ，这种情况 MySql 是怎么处理的呢？我们等等接着往下看 redo log！ 三、redo log 3.1 简述 因为数据库的许多操作都是在内存中完成的，但是内存中的数据有一个缺点就是断电丢失，那就会导致buffer pool 的数据也丢失，此时 redo log 就能大显神通了。 当发生宕机且数据未刷到磁盘的时候，可以通过redo log来恢复，保证ACID中的D，这就是redo log的作用。 redo 就是准备去做、将要去做的意思，redo log 记录的是将要做的一些操作。例如，此时将要做的是 update students set stuName='小强' where id=1; 那么这条操作就会被记录到 redo log buffer 中，redo log buffer 是 MySQL 为了提高效率，所以将这些操作都先放在内存中去完成。 3.2 存储形式 redo log 实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此 redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。如下图： 同时我们很容易得知， 在Innodb中，既有redo log 需要刷盘，还有 buffer pool 的数据页也需要刷盘， redo log存在的意义主要就是降低对 buffer pool 数据页刷盘的要求。 在上图中， write pos 表示 redo log 当前记录的 LSN (逻辑序列号)位置， check point 表示 数据页更改记录 刷盘后对应 redo log 所处的 LSN(逻辑序列号)位置。 write pos是当前记录的位置，一边写一边后移 checkpoint是当前要擦除的位置，也是往后推移 每次刷盘 redo log 记录到日志文件组中，write pos 位置就会后移更新。每次MySQL加载日志文件组恢复数据时，会清空加载过的 redo log 记录，并把check point后移更新。write pos 和 checkpoint 之间的还空着的部分可以用来写入新的 redo log 记录。 如果 write pos 追上 checkpoint ，表示日志文件组满了，这时候不能再写入新的 redo log记录，MySQL 得 停下来，清空一些记录，把 checkpoint 推进一下。 3.3 记录流程 准备更新一条 SQL 语句 MySQL（innodb）会先去缓冲池（Buffer Pool）中去查找这条数据，没找到就会去磁盘中查找，如果查找到就会将这条数据加载 到缓冲池（Buffer Pool）中 在加载到 buffer pool 的同时，会将这条数据的原始记录保存到 undo log 中 Innodb 会在 buffer pool 中执行更新操作 更新后的数据会记录在 redo log buffer 中，并根据innodb_flush_log_at_trx_commit参数决定 redo log buffer 刷入磁盘文件 redo log 进行持久化的时机 Mysql 重启的时候会将 redo log 日志恢复到缓冲池中 3.4 恢复流程 启动 innodb 的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 binlog )要快很多。 情况一：重启 Innodb 时，首先会检查磁盘中数据页的 LSN ，如果数据页的LSN 小于日志中的 LSN ，则会从 check point 开始恢复。 情况二：在宕机前正处于check point 的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 LSN 大于日志中的 LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。 3.5 那如果服务器宕机了，redo log buffer里面的数据不还是会丢失吗？ redo log 先放在内存中操作目的是为了提高速率，如果此时宕机了也没关系，因为Mysql会认为此次事务是失败的，会利用 undo log 进行回滚，并不会有任何影响。 注意：redo log 为了提高效率，会先将记录写入对应得 log buffer，后续某个时间点再一次性将多个操作记录写到对应的 log file。 这种先写日志再写磁盘的技术就是 MySql 里经常说到的 WAL(Write-Ahead Logging) 技术。 3.6 redo log是怎么保证事务的持久性的(刷盘时机)？ 具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态 。 因为成功提交的事务数据会写入 redo log buffer 中，然后落盘到 redo log 中，此时如果宕机导致 buffer pool 数据丢失，那么在重新启动后会在 redo log 中把已经罗盘的数据进行恢复，从而实现持久化存储。 此外，Mysql支持三种将 redo log buffer 写入 redo log file 的时机，可以通过innodb_flush_log_at_trx_commit 参数配置，各参数值含义如下： 参数 含义 0 每秒将 redo log buffer 中的数据将以写入到os cache中，同时flush到磁盘log文件中。在机器crash并重启后，会丢失一秒的事务日志数据 1 每次事务提交时，将 redo log buffer 中的数据写入到os cache中，并同时flush到磁盘。在机器crash并重启后，不会丢失事务日志，但是性能差。 2 每次事务提交时，将 redo log buffer 中的数据写入到os cache中，并每秒flush一次到磁盘。在机器crash并重启后，有可能丢失数据 一般建议选择取值2，因为 MySQL 挂了数据没有损失，整个服务器挂了才会损失1秒的事务提交数据。 3.7 buffer pool 的数据也要落盘，redo log buffer 也要落盘，为什么 redo log buffer 的落盘的性能影响会比较小？ 首先，我们看看如果每次事务提交，都对 buffer pool 的数据进行落盘会有哪些影响： 因为 Innodb 是以 页 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！ 而 redo log 对此做出了哪些优化： 只记录事务对数据页做了哪些修改，相对而已文件更小。 redo log 的内容是一个 append 操作，可以理解为顺序IO，因此落盘的速度要比数据页的随机IO快得快。 四、binlog 4.1 简述 binlog 用于记录数据库执行的写入行操作信息，以二进制的形式保存在磁盘中。 binglog 是Mysql的逻辑日志，并且由 Server 层进行记录，跟 redo log 和 undo log不用的是，binlog不是 Innodb 特有的日志，而是任何存储引擎都有记录的。 binlog 是通过追加的方式进行写入的，可以通过max_binlog_size 参数设置每个 binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。 4.2 使用场景 在实际应用中，binlog 的主要使用场景有两个，分别是主从复制和数据恢复。 主从复制：在 Master 端开启 binlog 发送到各个 Slave 端，Slave 端通过重放 binlog 从而达到主从数据一致。 数据恢复：通过 mysqlbinlog 工具来恢复数据。 4.3 刷盘时机 对于 InnoDB 存储引擎而言，只有在事务提交时才会记录biglog ，此时记录还在内存中，那么 biglog是什么时候刷到磁盘中的呢？ Mysql 通过 sync_binlog 参数控制刷盘时机，取值范围是 0 - N： 取值 作用 0 不去强制要求，由系统自行判断刷盘时机 1 每次 commit 都将 binlog 写进磁盘 N 每N个事务 commit，才会将 binlog 写入磁盘 从上面可以看出， sync_binlog 最安全的是设置是 1 ，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。 4.4 日志格式 binlog 日志有三种格式，分别为STATMENT、ROW 和MIXED。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT ， MySQL 5.7.7 之后，默认值是 ROW。日志格式通过 binlog-format 指定。 格式 作用 STATMENT 基于SQL 语句的复制( statement-based replication, SBR )，每一条会修改数据的sql语句会记录到binlog 中。 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO , 从而提高了性能； 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate() 、 slepp()、uuid() 等，当同步到从库的时候，再次执行可能会得到另外的结果。 ROW 基于行的复制(row-based replication, RBR )，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了 。 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题 ； 缺点：会产生大量的日志，尤其是alter table 的时候会让日志暴涨。 MIXED 基于STATMENT 和 ROW 两种模式的混合复制(mixed-based replication, MBR )，一般的复制使用STATEMENT 模式保存 binlog ，对于 STATEMENT 模式无法复制的操作使用 ROW 模式保存 binlog。 为什么默认用 ROW 呢？我们举个删除数据的例子说明一下。 STATEMENT 记录的删除语句是原始Sql语句，如下： DELETE FROM t WHERE age &gt; 10 and modified_time &lt;= '2022-01-02' limit 1; 此时如果主从不同步，会导致两边删除的数据记录不一致的情况。 而ROW格式记录的是实际受到影响的真实删除行的记录，如下： DELETE FROM t WHERE id = 3 and age = 12 and modified_time = '2022-01-01'; 这样 binlog传到备库去的时候，就肯定会删除id=3的行，不会存在主备删除不同行的问题。 4.5 工作流程 既然 bin log 也是日志文件，那它是在什么时候记录数据的呢？我们可以看看下图，就是包含 buffer pool、undo log、redo log 和 binlog的整个处理流程： 首先 Mysql 执行器根据执行计划调用存储引擎的 API 查询数据 存储引擎先从缓存时 buffer pool 中查询，如果没有就去磁盘查询，并放到 buffer pool中 在数据加载到 buffer pool 的同时，会将这条数据的原始记录保存到 undo log 日志文件中 Innodb 会在 buffer pool 中执行更新操作 更新后的数据会在记录在 redo log buffer 中，标记prepare状态 更新的同时把执行dml语句期间生成的event不断写入到binlog cache和binlog 临时文件 提交事务，同时会完成以下三件事： 7.1 将binlog cache和binlog 临时文件数据写入到 binlog 文件中同时释放binlog cache和binlog 临时文件； 7.2 将 binlog 文件名字和更新内容在 binlog 中的位置记录到 redo log 中，同时在 redo log 最后添加 commit 标记； 7.3 将 redo log buffer 中的数据刷入到 redo log 文件中； Mysql 有一个后台线程，它会在某个时机将我们 buffer pool 中的更新后的数据刷脏落盘，这就讲内存和数据库的数据保持统一了 4.6 buffer pool 刷脏落盘时机是什么时候？ 上面我们提到Mysql 有一个后台线程，在合适的时候会进行刷脏，把内存的数据刷到磁盘中，那么这个合适的时候究竟是什么的时候呢？ redo log 写满时，停止所有更新操作，将 checkpoint 向前推进，推进那部分日志的脏页更新到磁盘； 需要将一部分数据页淘汰，如果是干净页，直接淘汰就行了，脏页的话，需要全部同步到磁盘； mysql自认为空闲时 mysql正常关闭之前 4.7 redo log为什么有两阶段提交/为什么有prepare和commit两个状态？ 通过两个状态的提交的方式，保证提交事务之后，两个日志都已经写入了，同时如果采用两阶段的方式中间如果服务发生崩溃的话： redo 好没写入之前崩溃，这时binlog也还没写入，恢复数据不受影响 redo写好了，binlog还没写崩溃时，这是redo处于prepare状态，还没有提交，恢复时事务会回滚，binlog也还没有记录，所以不会影响 redo已经有了commit标识，则直接提交事务，同时因为binlog有记录，则恢复数据也不受影响 redo写好了，binlog写好了，但是还没有commit时崩溃了，这时会判断对应事务的binlog是否存在并完整： （1）如果存在并完整则提交事务，这时恢复到事务提交之后的状态，因为binlog中有记录，所以恢复成功 （2）如果binlog不存在或者不完整，这时会恢复到事务提交之前的状态，因为binlog中无记录或者不完整的记录不会生效，所以恢复也成功。 三、总结 总之，redo log 和 undo log 是保证 Innodb 事务执行，而 binlog 是用于主从复制和数据恢复的。 redo log是恢复在Buffer Pool更新后，还没来得及刷到磁盘的数据。 ","link":"http://mofish.pily.life/post/mysql_learning_05/"},{"title":"MySql学习之路(四)：一条Mysql的执行过程","content":"😤作为后端开发，每天都要与数据库打交道，那么每条Sql语句的执行过程究竟是什么样的呢？！这章节就来整理学习一下。 目录 一、连接器 二、查询缓存 三、分析器 四、优化器 五、执行器 六、buffer pool 在介绍MySQL数据库中SQL语句在Server的执行步骤前，我们先了解下MySQL的整体架构： 如果上图不清楚，可以再看看下面的图： 通过上面的架构图可以得知，Server层中主要由连接器、查询缓存、解析器/分析器、优化器、执行器几部分组成的，下面将主要描述下这几部分。 一、连接器🗾 客户端想要对数据库进行操作的前提就是要与数据库建立好连接，而连接器就是用来负责跟客户端建立连接、获取权限、维持和管理连接的。 而这个功能就是由MySQL驱动底层帮我们完成的，建立完连接之后，我们只需要发送 SQL 语句就可以执行 CRUD 了。 连接方式 MySql即支持短连接，也支持长连接。短连接就是操作完毕后，马上close关掉。长连接可以保持打开，减少服务端创建和释放连接的消耗，后续程序访问的时候还可以使用这个连接。 连接池 与客户端的连接池一样，为了减少频繁创建和销毁连接造成的不必要的性能损失，这里也采用了“池化”的思想，通过数据库连接池去管理连接。一般我们会在连接池中使用长连接，例如：druid、c3p0、dbcp等 二、查询缓存📈 缓存中数据存储格式：key（sql语句）- value（数据值），所以如果SQL语句（key）只要存在一点不同之处就会直接进行数据库查询了，否则无需查库直接从缓存中读取数据。 MySQL缓存是默认关闭的，也就是说不推荐使用缓存，并且在 MySQL 8.0 版本直接将查询缓存的整块功能删掉了。 这是因为表中的数据不是一成不变的，大多数是经常变化的，而当数据库中的数据变化了，那么相应的与此表相关的缓存数据就需要移除掉； 三、分析器🔍 分析器的工作主要是对要执行的Sql语句进行解析，最终得到抽象语法树，然后再使用预处理器判断抽象语法树中的表是否存在，如果存在的话，接着判断select投影列字段是否在表中存在等等。 词法分析 词法分析用于将SQL拆解为不可再分的原子符号，称为Token。并根据不同数据库方言所提供的字典，将其归类为关键字，表达式，字面量和操作符。 语法分析 语法分析就是根据词法分析拆解出来的Token（原子符号）将SQL语句转换为抽象语法树。 下面就直接举例说明，看一个SQL它的抽象语法书到底长什么样： SELECT id, name FROM t_user WHERE status = 'ACTIVE' AND age &gt; 18 然后上面的SQL语句经过词法分析、语法分析后得到的抽象语法书如下： 注意，为了便于理解，抽象语法树中的关键字的Token用绿色表示，变量的Token用红色表示，灰色表示需要进一步拆分。 预处理器 预处理是用来对生成的抽象语法树进行语义校验，语义校验就是对查询的表、select投影列字段进行校验，判断表、字段是否存在等。 四、优化器💡 优化器的作用主要是将SQL经过词法解析/语法解析后得到的语法树，通过MySQL的数据字典和统计信息的内容，经过一系列运算 ，最终得出一个执行计划，包括选择使用哪个索引。 在优化过程中，经过的一系列运算是什么呢？ 逻辑变换 例如SQL的where条件中存在 8&gt;9，那逻辑转换就是将语法树中存在的这种常量表达式直接进行化简，化简为 false；除了化简还有常量表达式计算等。 代价优化 就是通过付出一些数据统计分析的代价，来得到这个SQL执行是否可以走索引，以及走哪些索引；除此之外，在多表关联查询中，确定最终表join的顺序等。 在分析是否走索引查询时，是通过进行动态数据采样统计分析出来。只要是统计分析出来的，那就可能会存在分析错误的情况，所以在SQL执行不走索引时，也要考虑到这方面的因素。 五、执行器🏵 MySql通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。执行器最终就是根据一系列的执行计划去调用存储引擎提供的API接口去调用操作数据，完成sql的执行。 开始执行的时候，要先判断一下建立连接的对象对这个表有没有执行操作的权限，如果没有，就会返回没有权限的错误；如果有，就按照生成的执行计划进行执行。 六、buffer pool 在了解buffer pool之前，我们先了解一下存储引擎相关的知识。 存储引擎是对底层物理数据执行实际操作的组件，为Server服务器层提供各种操作数据的 API，数据是被存放在内存或者是磁盘中的。MySQL 支持插件式的存储引擎，包括 InnoDB 、MyISAM、Memory 等。一般情况下，MySQL默认使用的存储引擎是 InnoDB。如下图所示，InnoDB存储引擎整体分为内存架构（Memory Structures）和磁盘架构（Disk Structures） buffer pool(缓冲池)是InnoDB存储引擎中非常重要的内存结构，类似redis起到缓存数据的作用。Mysql的数据最终是存储在磁盘中的，如果没有buffer pool，那么每次的数据库请求都会磁盘中查找，这样必然会存在 IO 操作。 但是有了buffer pool，在第一次查询时就会将查询的结果保存到buffer pool中，这样后面再有请求时就会先从缓冲池中去查询，如果没有再去磁盘中查找，然后在放到buffer pool中，如下图： UPDATE students SET name = '小明' \bWHERE id = 1; 比如这条SQL，按照上面的那幅图，SQL 语句的执行步骤大致是这样子的： innodb 存储引擎先在缓冲池中查找 id=1 的这条数据是否存在 如果缓存不存在，那么就去磁盘中加载，并将其存放在缓冲池中 该条记录会被加上一个独占锁(也叫做写锁，防止修改) 那么buffer pool和查询缓存有什么区别呢？？ 查询缓存位于Server层，MySql Server首选会从查询缓存中查看是否曾经执行过这个SQL，如果曾经执行过的话，之前执行的查询结果会以Key-Value的形式保存在查询缓存中。key是SQL语句，value是查询结果。我们将这个过程称为查询缓存！ buffer pool位于存储引擎层。buffer pool就是MySql存储引擎为了加速数据的读取速度而设计的缓冲机制。 ","link":"http://mofish.pily.life/post/mysql_learning_04/"},{"title":"MySql学习之路(三)：MyISAM和InnoDB的区别","content":"😗 简单记录一下MyISAM和InnoDB的区别。 1. 构造上的区别 Innodb：frm是表定义文件，ibd是数据文件 8.0 只保留了ibd文件，表定义也放在ibd中了，可通过下面命令解析ibd文件，看到里面存有表结构数据 ibd2sdi --dump-file=student.txt student.ibd 可能还会存在opt是数据库的配置文件（字符集、比较规则等） Myisam：frm是表定义文件，myd是数据文件，myi是索引文件 8.0 中frm改成了sdi，也是表定义文件 2. 事务的处理方式 InnoDB支持事务，对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务 MyISAM不支持事务 3. 外键的支持 InnoDB支持外键 MyISAM不支持外键，对一个包含外键的InnoDB表转为MYISAM会失败 4. 表的具体行数 InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描 MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快（注意不能加有任何WHERE条件） 为什么InnoDB没有了这个变量呢？ 因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询。InnoDB会尝试遍历一个尽可能小的索引除非优化器提示使用别的索引。如果二级索引不存在，InnoDB还会尝试去遍历其他聚簇索引。 如果索引并没有完全处于InnoDB维护的缓冲区（Buffer Pool）中，count操作会比较费时。可以建立一个记录总行数的表并让你的程序在INSERT/DELETE时更新对应的数据。和上面提到的问题一样，如果此时存在多个事务的话这种方案也不太好用。 如果得到大致的行数值已经足够满足需求可以尝试SHOW TABLE STATUS 5. 全文索引的支持 InnoDB在5.7之后支持全文索引 MyISAN支持全文索引 6. 主键 InnoDB必须有主键，如果用户没有指定的话会自己生产一个隐藏列row_id来作为主键 MyISAN可以没有主键 7. MyISAM表格可以被压缩后进行查询操作 ... 8. 锁的区别 InnoDB支持表锁和行锁（innodb的行锁是实现在索引上的，而不是锁在物理记录上，意思是：访问没有命中索引，也无法使用行锁，转为使用表锁） MyISAN支持表级锁 9. 索引结构 InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。 MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 也就是说：InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针。 10.hash自适应 InnoDB本身不支持 Hash索引，但是提供自适应索引（Adaptive Hash Index）。如果当某个数据经常被访问，达到某个条件时，就会将这个数据页的地址存放到 Hash表 中，这样下次查询时就能直接找到这个页面所在的位置，加快检索速度。 &gt; show variables like '%adavtive_hash_index%' ","link":"http://mofish.pily.life/post/mysql_learning_03/"},{"title":"MySql学习之路(二)：组合索引的最左匹配原则","content":"😉 上一章节我们讲到了索引，其中一个索引类型是组合索引，本章节就整理一下该索引的最左匹配原则，以便更加了解如何正确的使用该索引。 一、定义 最左前缀匹配原则：在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。 下面我们建立一个数据表来演示： -- 建立数据表 CREATE TABLE staffs( id INT PRIMARY KEY AUTO_INCREMENT, `name` VARCHAR(24) NOT NULL DEFAULT'' COMMENT'姓名', `age` INT NOT NULL DEFAULT 0 COMMENT'年龄', `position` VARCHAR(20) NOT NULL DEFAULT'' COMMENT'职位', `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT'入职时间' )CHARSET utf8 COMMENT'员工记录表'; -- 插入数据 INSERT INTO staffs(`name`,`age`,`position`,`create_time`) VALUES('Amy',22,'manager',NOW()); INSERT INTO staffs(`name`,`age`,`position`,`create_time`) VALUES('July',23,'dev',NOW()); INSERT INTO staffs(`name`,`age`,`position`,`create_time`) VALUES('Sara',23,'dev',NOW()); -- 建立联合索引，顺序是name-&gt;age-&gt;position ALTER TABLE staffs ADD INDEX idx_list(name,age,position); -- 查看索引 show index from staffs; 二、全索引顺序 首先我们使用全部字段，通过不同顺序来看看索引的使用情况： explain select *from staffs where name='Amy'and age=22 and position='manager'; explain select *from staffs where age=22 and name='Amy' and position='manager'; explain select *from staffs where position='manager' and age=22 and name='Amy' ; 我们可以看到，无论怎么颠倒这三个字段的顺序都用到了联合索引，最重要的原因是因为Mysql中有查询优化器，它会判断纠正这条sql语句以什么样的方式执行效率最高，最后才生成真正的执行计划，所以sql语句中对的字段顺序不需要和联合索引定义的顺序相同。 总的来说，如果sql语句中的查询条件用来了联合索引的所有列，那么不论以何种顺序都可使用到联合索引。 三、部分索引顺序 只查第一个索引：explain select * from staffs where name = 'Amy'; 因为name 是联合索引的正序第一个，所以这里可以正常使用联合索引。 只查前两个索引：explain select * from staffs where name = 'Amy' and age =22; 因为name 和age是联合索引的正序前两个，所以这里可以正常使用联合索引。 跳过中间的索引：explain select * from staffs where name = 'Amy' and position = 'manager'; 因为name 是联合索引的正序第一个，所以这里可以正常使用联合索引，但是因为position是正序的第三个，跳过了第二个age，导致使用联合索引查询时，只有name这个索引有效，导致查询时把符合name条件的数据拿出来后，再进行二次匹配。 跳过第一个索引：explain select * from staffs where position = 'manager' and age =22; 因为联合索引是按照顺序来建立索引的，当跳过了正序索引第一个name，就导致了整个联合索引失效。 四、模糊搜索 模糊索引就会使用到like的语句，如果复合最左前缀的话，会使用到range或者是index的类型进行索引。 右模糊搜索，最左前缀索引，类型为index或者range： explain select *from staffs where name like 'Am%'; 左右模糊搜索，类型为all，全表查询： explain select *from staffs where name like '%Am%'; 左模糊搜索，类型为all，全表查询： explain select *from staffs where name like '%Am'; 因为右模糊搜索明确了是以**'Am'**开头，所以可以使用索引来检索'Am'开头的数据。类似于查字典，知道开头的拼音在目录很快就能定位到，但是只知道中间或者结尾的，就不好确定了。 五、范围搜索 mysql 会一直向右匹配直到遇到范围查询（&gt;、&lt;、between、like）就停止匹配。范围列可以用到索引，但是范围列后面的列无法用到索引。即，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。 联合索引支取第一个索引列，看看key_len长度：74 explain select * from staffs where name='Amy'; 根据联合索引，取前两个字段，其中age进行范围搜索，key_len长度为：78 explain select * from staffs where name='Amy' and age &gt; 22; 根据联合索引，取所有字段，其中age进行范围搜索，key_len长度也为：78 explain select * from staffs where name='Amy' and age &gt; 22 and position = 'dev'; 根据联合索引，取所有字段，key_len长度也为：140 explain select * from staffs where name='Amy' and age = 22 and position = 'dev'; 可以看到当聚合索引的中间索引使用了范围搜索时，后面的索引就会失效，因此可以调整索引的顺序，把age放到最后。 六、联合索引结构 很多博客中都是说：联合索引在B+树上的 非叶子节点 中只会存储 联合索引 中的第一个索引字段 的值，联合索引的其余索引字段的值只会出现在 B+树 的 叶子节点 中 。 如下图：就是错误的联合索引B+树存储结构图： 实际上，在联合索引的B+树中，非叶子节点是会存储联合索引的多个字段的值的，如在此数据表的前提下建立一个(b,c,d)的联合索引 则每个节点都会存在(b,c,d)的值，并依次排序，其存储结构图如下： 查找步骤如下： 从根节点查找第一个索引值，第一个索引的第一个索引列为1，12大于1，第二个索引的第一个索引列为56，12小于56，因此读取下个节点； 找到第一个索引为12的值，如果有多个12的值，再判断第二个索引，以此类推； 一直找到最后一层时，得到主键ID，回表再冲主键索引树找到行记录。 ","link":"http://mofish.pily.life/post/mysql_learning_02/"},{"title":"MySql学习之路(一)：索引及其实现原理","content":"📖 开发过程中都避免不了使用数据库索引，所以这章节详细整理一下什么是数据库索引、 聚簇索引与非聚簇索引的却别、MyISAM和InnoDB的索引实现方式的区别等等。 目录 一、什么是索引 1.1 索引的优缺点 1.2 索引的使用场景 二、常见索引类型 2.1 主键索引 2.2 普通索引 2.3 唯一索引 2.4 全文索引 2.5 组合索引 三、聚簇索引和非聚簇索引 3.1 聚簇索引 3.2 非聚簇索引 四、普通索引 vs 唯一索引 4.1 查询过程 4.2 更新/插入过程 4.3 什么是change buffer 五、为什么使用B+树作为索引 六、MySQL索引的实现 6.1 MyISAM索引的实现 6.2 InnoDB索引的实现 6.3 索引下推（ICP） 七、什么情况下索引会失效 八、COUNT(*)、COUNT(1)和COUNT(具体字段)效率 一、什么是索引❓❓ 索引其实是一种数据结构，通过缩小一张表中需要查询的数据来加快搜索的速度，类似于字典目录一样，可以目录快速定位到需要查找内容的位置，而不需要一页一页的去找（全表扫描）。 1.1 索引的优缺点✔️ 优点😄 减少查询需要检索的行数，加快查询速度，避免全表扫描（主要优点）； 如果索引的结构是B+树的话，使用分组和排序时，可以显著减少所需操作时间； 通过唯一性索引，可保证每行数据的唯一性； 联表查询时，加速表与表之间的连接，提高查询速度。 缺点😅 当对表中的数据进行增加、删除和修改时，索引也要进行更新，维护的耗时随着数据量的增加而增加（记录移位、页分裂、页回收）； 索引需要占用物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 1.2 索引的使用场景📑 需要建立索引的列⭕️ 在Where条件上经常出现的列加上索引，加快条件的判断速度； 按范围获取的列或者在group by、order by使用的列，因为索引已经排序，所以可以利用索引加快排序查找时间； 在经常用于连接的列上，这些列主要时一些外键，可以加快连接查询的速度； 作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 有时候我们需要对某个字段进行去重，使用 DISTINCT时也可对这个字段创建索引； 不建议建立索引的列⚠️ 频繁更新的列不适合创建过多的索引，因为每次更新不单单是更新记录，还需要重新维护索引，反而降低了系统的维护速度和增大了空间需求； 查询中很少用到的列不需要创建索引，减少空间占用和降低索引的维护资源； 只有很少数据值的列不应该增加索引。如性别，由于这些列的取值很少，在表中搜索的数据行的比例很大，所以增加索引，并不能明显加快检索速度； 不建议用无序的值作为索引，例如身份证、MD5、HASH、无序长字符串等； 删除不再使用或者很少使用的索引； 不要定义夯余或重复的索引。 如果数据量比较少的情况下，有时会出现即使建立了索引也会出现全表扫描的情况，那是因为数据量少，使用索引还得回表，效率还没全表扫描的高，所以mysql会经分析后，使用最优的查询方案。 此外，索引也不是越多越好，因为每个索引都需要占用磁盘空间，索引越多，占用越大，而且索引会影响增、删、改等语句的性能，因为在数据修改的同时，索引也需要进行调整和更新，会造成负担。 二、常见索引类型💭 按作用分类：普通索引、唯一索引、主键索引、全文索引、组合索引。 按结构分类：聚簇索引、非聚簇索引 说明：无论何种类型的索引，其实现的原理都是建立关键字与位置的对应关系来实现的。 接下来我们先来整理按作用分类的不同索引。 2.1 普通索引👈👈 最基本的索引，没有任何限制，在功能上能够提高索引字段的搜索效率。 --直接创建索引： CREATE INDEX index_name ON table_name(column_name); --修改表结构的方式添加索引： ALTER TABLE table_name ADD INDEX index_name (column_name); --创建表的时候同时创建索引： CREATE TABLE table_name( ID INT NOT NULL, username VARCHAR(16) NOT NULL DEFAULT '', INDEX index_name (username) ); 2.2 唯一索引👈👈 与普通索引类型，但是索引列的值必须是唯一的，允许有空值和NULL值，但是如果是组合唯一索引时，则多个列的值的组合必须是唯一的。 --直接创建索引： CREATE UNIQUE INDEX index_name ON table_name(column_name,...); --修改表结构的方式添加索引： ALTER TABLE table_name ADD UNIQUE INDEX index_name (column_name,...); --创建表的时候同时创建索引： CREATE TABLE table_name( ID INT NOT NULL, username VARCHAR(16) NOT NULL DEFAULT '', UNIQUE index_name (username) ); 2.3 主键索引👈👈 主键索引其实可以说是一种有约束的唯一索引，它与唯一索引的区别在于： 唯一索引允许NULL，而主键不允许 主键索引可以被其他表引用为外键，而唯一索引不能 一张表仅能有一个主键，但可有多个唯一索引 主键是为了提升查询速度，InnoDB中默认主键为聚簇索引，但聚簇索引不一定是主键，至于什么是聚簇索引，下面会讲到。 --创建表的时候创建，当把某个列设为主键的时候,数据库会自动的创建一个以主键作为名称的主键索引。 CREATE TABLE table( id INT PRIMARY KEY AUTO_INCREMENT, name VARCHAR(32) NOT NULL DEFAULT '' ) --修改表结构： ALTER TABLE `table_name` ADD PRIMARY KEY ( `col` ); 2.4 全文索引👈👈 全文索引仅可用于 MyISAM 表，并只支持CHAR、VARCHAR或TEXT类型，用于替代效率较低的like模糊匹配操作，而且可以通过多字段组合的全文索引一次性全模糊匹配多个字段。对于大容量的数据表，生成全文索引是一个非常消耗时间和空间的做法。 一般不建议使用，如果对全文搜索有需求的话，建议使用elasticsearch，其搜索效率和准确度会更高。 --直接创建索引： CREATE FULLTEXT INDEX index_name on table_name(column); --修改表结构： ALTER TABLE `table_name` add FULLTEXT INDEX index_name(column); --创建表的时候同时创建索引： CREATE TABLE `table_name` ( id INT PRIMARY KEY auto_increment, name VARCHAR(11), remarks text , FULLTEXT (remarks) ); 2.5 组合索引👈👈 为了更多的提高查询效率可建立组合索引，即把多个列组合起来归为一个索引。创建组合索引时应该将最常用（频率高）作限制条件的列放在最左边，依次递减，即我们常说的《最左匹配原则》(点击查看)。 --直接创建索引： CREATE INDEX index_name on table_name(column1, column2, column3); --修改表结构： ALTER TABLE `table_name` add INDEX index_name(column1, column2, column3); 当创建(a,b,c)联合索引时，相当于创建了(a)单列索引，(a,b)联合索引以及(a,b,c)联合索引。 想要索引生效的话,只能使用 a和a,b和a,b,c三种组合；当然，a,c组合也可以，但实际上只用到了a的索引，c并没有用到！ 因此创建和使用组合索引时，需要注意索引中列的顺序。 组合索引和多个单列索引的区别？ 多个单列索引在多条件查询时优化器会选择最优索引策略，可能只用一个索引，也可能将多个索引全用上！ 但多个单列索引底层会建立多个B+索引树，比较占用空间，也会浪费一定搜索效率，故如果有多条件联合查询时最好建联合索引！ 三、聚簇索引和非聚簇索引🕺 MySQL的InnoDB索引数据结构是B+树，主键索引叶子节点的值存储的就是MySQL的数据行，普通索引的叶子节点的值存储的是主键值，这是了解聚簇索引和非聚簇索引的前提。 那二者究竟有什么区别，简单的说： 聚簇索引：找到了索引就找到了需要的数据，那么这个索引就是聚簇索引，所以主键就是聚簇索引，修改聚簇索引其实就是修改主键。 非聚簇索引：索引的存储和数据的存储是分离的，也就是说找到了索引但没找到数据，需要根据索引上的值(主键)再次回表查询，非聚簇索引也叫做辅助索引。 3.1 聚簇索引👈👈 聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。具体细节依赖于其实现方式。 聚簇索引就是按照每张表的主键构造一颗B+树，同时叶子节点中存放的就是整张表的行记录数据，也将聚集索引的叶子节点称为数据页。这个特性决定了索引组织表中数据也是索引的一部分，每张表只能拥有一个聚簇索引。 Innodb通过主键聚集数据，如果没有定义主键，innodb会选择非空的唯一索引代替。如果没有这样的索引，innodb会隐式的定义一个主键来作为聚簇索引。 特点 页内的记录是按照主键大小顺序排成一个单向链表 各个存放用户记录的页也是根据页中记录的主键大小顺序排成一个双向链表 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表 B+树叶子结点存储的是完整的用户记录，就是所谓的索引即数据，数据即索引 优点 数据访问更快，因为聚簇索引将索引和数据保存在同一个B+树中，因此从聚簇索引中获取数据比非聚簇索引更快 聚簇索引对于主键的排序查找和范围查找速度非常快 缺点 插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键 更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新 非聚簇索引，访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据，即我们常说的回表查询 3.2 非聚簇索引👈👈 非聚簇索引也成为辅助索引，辅助索引访问数据总是需要二次查找。Innodb辅助索引的叶子节点并不包含行记录的全部数据，只存储了索引对应的值外，还包含了相应行数据的聚簇索引键。 辅助索引的存在不影响数据在聚簇索引中的组织，所以一张表可以有多个辅助索引。 四、普通索引 vs 唯一索引 从性能的角度考虑，你选择唯一索引还是普通索引呢？选择的依据是什么呢？ 4.1 查询过程 假设，执行查询的语句是 select id from test where k=5。 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一 个不满足k=5条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检 索。 4.2 更新/插入过程 update table where k=4 当要更新的记录在内存中的时候： 普通索引k会找到索引记录等于3和5的位置，然后在中间插入k=4的记录； 唯一索引k会找到索引记录等于3和5的位置，然后判断没有冲突，插入k=4的记录； 可以看出，当记录在内存中的时候，这俩差别不大，仅仅是唯一索引多了一步判断唯一性的操作。这个判断，仅仅会消耗很小一部分cpu的资源。 当要更新的记录不在内存中的时候： 唯一索引需要将数据页加载到内存中，判断这个值没有冲突，然后插入这个新值； 普通索引则是将更新记录在change buffer，语句执行就结束了。 可以看到，普通索引利用了change buffer，减少了磁盘上的随机访问，对性能的提升比较明显。 唯一索引的更新和查询就不能使用change buffer，实际上也只有普通索引可以使用。 4.3 什么是change buffer 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话， 在不影响数据一致性的前提下， InooDB会将这些更新操作缓存在change buffer中 ，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 将change buffer中的操作应用到原数据页，得到最新结果的过程称为 merge 。除了 访问这个数据页 会触 发merge外，系统有 后台线程会定期 merge。在 数据库正常关闭（shutdown） 的过程中，也会执行merge 操作。 如果能够将更新操作先记录在change buffer， 减少读磁盘 ，语句的执行速度会得到明显的提升。而且， 数据读入内存是需要占用 buffer pool 的，所以这种方式还能够 避免占用内存 ，提高内存利用率。 五、为什么使用B+树作为索引？🤔 👆👆👆点击查看B+树的由来，便可更加深刻的了解到为什么要用B+树作为索引了。 问：高度为3的B+树可以存储多少条数据？ 答： InnoDB引擎中数据页的大小是16K，假设key值使用bigint占用8个字节，指针也占用8个字节，那么一页可以存储 16 * 1024 / (8+8) = 1024个索引指针 再假设一行数据的大小是1K，那么一页可以存储16条数据 那么可以算出高度为3个一个B+树可以存储：1024 * 1024 * 16 = 16777216条数据。 问：为什么说一般最多只需要1-3次I/O即可？ 答： 从上面来看，层数为3的B+树就可以存储千万级别的数据量，实际情况上每个节点的数据可能不能填满，因此在数据库中， B+树的高度一般为1～4层。 但是！InnbDB存储引擎在设计时将根节点常驻内存的，因此第一层无需再参与磁盘I/O了，因此减去根结点的话，一般1-3次I/O即可。 六、MySQL索引的实现👏 在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本部分主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。 当查询非主键的字段时 , MyISAM查询性能更好，从索引文件数据文件的设计来看 , MyISAM每一个索引字段都有上面的索引树，MyISAM直接找到物理地址后就可以直接定位到数据记录。 但是InnoDB查询到叶子节点后，还需要再查询一次主键索引树，才可以定位到具体数据，等于MyISAM一步就查到了数据，但是InnoDB要两步，那当然MyISAM查询性能更高。 如果是查询的主键的字段时,Innodb更好一些 , 因为直接从主键的聚簇索引里就取到了数据 , 不需要再次用指针去拿。 6.1 MyISAM索引的实现 MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址，因此MyISAM都可以看作是非聚簇索引。 主键索引 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM主键索引的原理图： 辅助索引 在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示： 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 6.2 InnoDB索引的实现 InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 主键索引 在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 辅助索引 InnoDB的所有辅助索引都引用主键作为data域。例如，下图为定义在UserName上的一个辅助索引： 辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。不过由于辅助索引会包含主键列，所以，如果主键使用过长的字段，将会导致其他辅助索引变得更大。所以争取尽量把主键定义得小一些。 6.3 索引下推（ICP） 索引下推是从 MySql5.6 开始引入一个特性，英文是 index condition pushdown，一般简称为 ICP，索引下推通过减少回表的次数，来提高数据库的查询效率。 # 打开索引下推 SET optimizer_switch = 'index_condition_pushdown=on'; # 关闭索引下推 SET optimizer_switch = 'index_condition_pushdown=off'; // 创建联合索引 create index idx_user on users (username,age); // 查询数据 select * from users where username = '1' and age = 99; MySql5.6之前：存储层通过索引拿到 username = '1'的数据后通过回表找到完整数据并返回给serve层，server层拿到数据后再判断 age = 99，如果不满足则丢弃。 如果我们能够把 age 直接传入存储引擎，在存储引擎中直接去判断 age 是否满足条件，满足条件了，再去回表，不满足条件就到此结束，这样就可以减少回表的次数，进而提高查询效率。 MySql5.6及之后：由于是联合索引，所以B+数节点会包含username和age的值，在拿到username = '1' 的数据后，再判断 age = 99 是否满足，只有满足条件的情况下才返回给server层。 这就是索引下推（index condition pushdown，ICP），有效的减少了回表次数，提高了查询效率。 七、什么情况下索引会失效😤 在WHERE中使用OR时，有一个列没有索引，那么其它列的索引将不起作用； 不匹配数据类型，会造成索引失效。如字段类型是字符串，WHERE时一定用引号括起来，否则索引失效； 对列进行计算或者是使用函数，则该列的索引会失效； LIKE通配符可能导致索引失效，如果是覆盖索引或者右模糊查询可能会走索引，如果是左模糊或全模糊(非覆盖索引)的话则不会走索引； 如果是联合索引，但是不满足最左匹配原则的； is null可以使用索引，is not null无法使用索引； 索引字段上使用（！= 或者 &lt; &gt;，not in）时，可能会导致索引失效； 左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效； mysql估计使用全表扫描要比使用索引快,则不使用索引。 七、排序优化 问：在 WHERE 条件字段上加索引，但是为什么在 ORDER BY 字段上还要加索引呢？ 答：在MySQL中，支持两种排序方式，分别是 FileSort 和 Index 排序。 Index 排序中，索引可以保证数据的有序性，不需要再进行排序，效率更高。 FileSort 排序则一般在 内存中 进行排序，占用CPU较多。如果待排结果较大，会产生临时文件 I/O 到磁盘进行排序的情况，效率较低。 优化建议： SQL 中，可以在 WHERE 子句和 ORDER BY 子句中使用索引，目的是在 WHERE 子句中 避免全表扫描 ，在 ORDER BY 子句 避免使用 FileSort 排序 。当然，某些情况下全表扫描，或者 FileSort 排序不一定比索引慢。但总的来说，我们还是要避免，以提高查询效率。 尽量使用 Index 完成 ORDER BY 排序。如果 WHERE 和 ORDER BY 后面是相同的列就使用单索引列； 如果不同就使用联合索引。 无法使用 Index 时，需要对 FileSort 方式进行调优。 八、COUNT(*)、COUNT(1)和COUNT(具体字段)效率 InnoDB中，COUNT(*)和COUNT(1)都是对所有结果进行COUNT，二者本质上没什么区别，如果使用COUNT(具体字段)来统计，要尽量使用二级索引。因为主键是聚簇索引，包含的信息较多，明显会大于二级索引（非聚簇索引）。 但对于COUNT(*)和COUNT(1)来说，它们不需要查找具体的行，只是统计行数，系统会自动采用占用空间更小的二级索引来进行统计。 如果有多个二级索引，会使用key_len小的二级索引进行扫描，当没有二级索引时，才会使用主键索引来进行统计。 如果是MyISAM存储引擎，统计整个数据表的行数只需要O(1)复杂度，因为每张MyISAM的数据都有一个meta信息存储了row_count值，可直接获取。 而InnoDB在统计整表数据量时，由于其支持事务，采用行级锁和MVCC机制，因此需要扫描全表。 ","link":"http://mofish.pily.life/post/mysql_learning_01/"},{"title":"数据结构学习之路(八)：B和B+树","content":"该篇文章内容来源于B站的一个咖喱味视频《彻底理解B树和B+树！为何它们常用在数据库中？》，从磁盘结构、数据如何存储到磁盘到索引的由来，再由此引入了搜索树、B-树以及B+树，以便大家一步一步的由浅至深的理解为何要使用B-树和B+树。 目录： 一、磁盘 1.1 磁盘结构 1.2 磁盘读写 二、索引 2.1 对象存储 2.2 稠密索引 2.3 稀疏索引 三、搜索树 3.1 BST-二叉搜索树 3.2 多路搜索树 3.3 B树 3.4 B+树 一、磁盘 1.1 磁盘结构 逻辑圈：磁盘（图中蓝色圈）是同心圆，从外向内为第一、第二、第N分区，逻辑上是连续的但物理上不连续。 第一磁道：最外面的蓝色圈 扇区：磁盘和扇形区域相交的位置，如第0磁道和扇形0区相交的区域（绿色），扇区是磁盘的基本读写单位，通常是512字节，但是因厂商不同可能也会有差异，接下来的我们都以512字节为标准，因此如上图一共有24个扇区（4磁道*6扇形），共可存储12k的数据。 1.2 磁盘读写 区图中的扇区（绿色）为例： 0 ~ 511：假定每个扇区的大小是 512 byte offset：内存地址偏移量 内存地址：通过磁道号、扇区号和内存地址偏移量才能定位到对应的内存地址 数据读写：数据无法在磁盘上直接进行计算和处理，必须将其放入到主存中，当计算和处理完毕后再放回磁盘 二、索引 对磁盘的结构有个大概了解后，接下来我们看一下如何将数据存储在磁盘上。 2.1 对象存储 Employee对象：首先我们建立了一个Employee对象，里面有eid、name、dept等属性，假设每个Employee对象占用128byte，其中eid占10byte； Employee数据表：假设表中有100行数据，每行数据占128byte； 磁盘：每个扇区能够存储512byte数据 则100行数据需要 100 * 128 / 512 = 25个扇区来存储，所以当我们需要查询数据时，最多需要访问25个扇区才能找到对应的数据。 2.2 稠密索引 😣我们思考一下，如果不止100行，而是100万行数据呢，那是不是最坏的情况下需要访问10000个扇区才能找到对应的数据？ 那有没有办法可以减少扇区的访问数量呢？那就是增加索引🤓。 Index数据表：假如我们只用eid来作为索引，那么index每行都由eid（10byte）和pointer指针（假设6byte）组成，每行占16byte，共100行。 index占用扇区：index占用的扇区数量为 16 * 100 / 512 = 3.125，需要4个扇区存储（每块可以存储516/16=32条记录） 因此，100行数据需要25块扇区来存储，而100行数据使用稠密索引的话，稠密索引需要4块扇区来存储，此时先通过索引最坏情况下需要访问4个扇区找到对应的数据，再通过索引访问指定的那1快扇区找到所需的数据，一共需要4+1=5次IO就能拿到数据😌。 2.3 稀疏索引 但是😲，如果是100万行数据的时候呢？那么100万行索引就需要31250个扇区来存储，导致最坏的情况下需要31250+1=31251次IO才能拿到数据了😭导致数据量一大，IO的次数又增多了！ 因此在数据量增长的情况下，索引的IO次数也会增加，此时我们就需要增加稀疏索引，即多级索引来解决这一问题👏 假设此时有1000个employee对象存到数据表中： 数据占用扇区 : 每行数据占用 128 bytes，数据一共需要 1000 * 128 / 512 = 250 个扇区 稠密索引表 : 每个索引行占用 16 bytes， 稠密索引表一共需要 1000 * 16 / 512 = 31.25 ≈ 32 个扇区，此时搜索速度下降，需要更高级的索引 稀疏索引表 : 每个索引行占用 16 bytes， 其中每行的索引范围为 32，1000个稠密索引需要 1000 / 32 = 31.25 ≈ 32 行稀疏索引行，而32行 16 bytes的稀疏索引刚好为一个扇区 此时，我们查找ID为50的数据时，则从稀疏索引扇区33~64的索引行获取到稠密索引所在扇区的指针，从稠密索引处获取指向ID为50的具体数据记录内存地址的指针，再访问该数据扇区，共3次IO。 稀疏索引，即多级索引，简单的可以理解为索引的索引，但数据的量级比较大的时候，可通过多级索引来减少搜索IO次数，以空间换时间。 三、搜索树 当索引的层级越来越高时，一翻转90°，我们看到的是什么🙀？ 没错，就是树，就是一颗多路搜索树，这也是B树和B+树的由来，但是在讲它们时，我们先简单了解一下二叉树和多路搜索树。 3.1 BST-二叉搜索树 左子树上的所有节点的值均小于或等于它的根节点的值 右子树上的所有节点的值均大于或等于它的根节点的值 左右子树都是一个二叉搜索树 每个节点只有一个key，有两个子节点 这里我们可以看到虽然二叉搜索树的值是一个有序的状态，查找起来效率也不错，但是当数据量庞大的时候，树高度的增加导致了IO次数的增加，从而影响了查找效率，那么有木有方法可以减少树的高度或者说能够减少IO次数增加查找效率呢🤔🤔？ 3.2 多路搜索树 多路查找树(muitl-way search tree)也成m路搜索树，有m个children，m-1个key，其每一个节点的孩子数可以多于两个，且每一个节点处可以存储多个元素。主要有4中特殊形式：2-3树、2-3-4树、B树和B+树。 M路搜索树主要用于解决数据量大无法全部加载到内存的数据存储。通过增加每层节点的个数和在每个节点存放更多的数据来在一层中存放更多的数据，从而降低树的高度，在数据查找时减少磁盘访问次数。 2-3树 定义： 1、2-3树中每一个节点都具有两个孩子(我们称它为2节点)或三个孩子(我们称它为3节点)； 2、一个2节点包含一个元素和两个孩子，且与二叉排序树类似，左子树包含的元素小于该元素，右子树包含的元素大于该元素； 3、一个3节点包含一小一大两个元素和三个孩子。如果某个3节点有孩子，左子树包含小于较小元素的元素，右子树包含大于较大元素的元素，中间子树包含介于两元素之间的元素。 2-3-4树 定义： 1、2-3-4搜索树是2-3搜索树的概念扩展，包括了4个节点的使用； 2、一个4节点中包含小中大三个元素和四个孩子； 3、如果某个4节点有孩子的话，左子树包含小于最小元素的元素；第二子树包含大于最小元素，小于第二元素的元素；第三子树包含大于第二元素，小于最大元素的元素；右子树包含大于最大元素的元素。 此时，我们利用一下用多路搜索树的结构来构建一下索引： 4-way搜索树中有3个key，每个key字都有对应指向记录的指针rp 4-way搜索树中有4个children，有4个指向子节点的指针 此时，使用多路搜索树，变成在减少IO次数的情况下，快速拿到对应的索引，以及该索引指向的源数据。但是多路搜索树还有一个问题，就是缺少了某些限制条件，导致如果一直是有序添加key时，会退化成链表，导致搜索效率降低。 所以，为了防止这种情况出现，必须加上限制条件，而加上限制条件后，就变成我们熟知的B-Tree。 3.3 B树 B树是一种m路搜索树，B树主要用于解决m路搜索树的不平衡导致树的高度变高，跟二叉树退化为链表导致性能问题一样。B树通过对每层的节点进行控制、调整，如节点分离，节点合并，一层满时向上分裂父节点来增加新的层等操作来保证该m路搜索树的平衡。 具体规则如下： 1、如果根节点不是叶子节点，则至少有两棵子树 2、每一个非根的分支节点都有k-1个元素和k和孩子，其中[m/2]&lt;=k&lt;=m 3、每个叶子节点都有k-1个元素，其中[m/2]&lt;=k&lt;=m 4、所有叶子节点都位于同一层 5、创建过程是自下而上 创建过程：（m为4） ①插入10、20、30 ②插入50、60，因为节点元素个数&gt;=m，所以需要把中间节点20向上提，形成根节点 ③插入70、80，因为右子节点元素个数&gt;=m，而父节点的孩子个数还&lt;m，所以需要分裂节点，并把中间节点往上提 特点： 每个叶子节点都存储key和data 非叶子节点还存储了指向子节点的指针，但叶子节点的该指针指向null 此时，通过构建B树，我们发现在大量的索引中想要获取指定的数据，IO的次数已经大量减少，查找效率已经大大的提升。 但是现在还有2个问题： 1、如果每个节点都有指向数据记录的指针的话，那么每个节点占用的空间会比较大，那有没有办法减少每个节点的空间，让每个节点可以存放更多的key，减少获取节点时的IO次数？ 2、如果需要范围搜索时，B树似乎不太友好，需要中序遍历，较为复杂，那有没有办法可以让范围搜索更加简单呢？ 3.4 B+树 相较于B树，B+树增加了如下规则： 1、B+树中，非叶子节点中的key没有指向data的指针，仅叶子节点的key有指向data的指针 该规则解决了上述问题1，由于只有叶子节点有指向data的指针，因此减少了非叶子节点的占用空间大小，一次性可读取更多的节点数据，减少IO次数。 2、B+树中，每个key在叶子节点都有其副本 3、B+树中，叶子节点之间有指向下个节点的兄弟指针（链表-稠密索引） 规则2和3该规则解决了上述问题2，由于有指向下个节点的兄弟指针，并且叶子节点都有副本，因为在范围搜索是，只需要定位到边界值，就能通过链表在叶子节点直接获取范围内的数据。 ","link":"http://mofish.pily.life/post/data_structure_08/"},{"title":"RabbitMq学习之路(三)：4种交换机模式和5种工作模式","content":"🤗 这章节简单的记录一下RabbitMq中的5种工作模式和4种交换机模式，可更加直观的了解不同模式的使用场景。 一、4种工作模式 Direct 直连交换机 Fanout 扇形交换机 Topic 主题交换机 Headers 首部模式交换机 1.1 Direct 直连交换机 发送消息使用的routing-key，要与交换机使用的routing-key完全匹配。 比如exchange使用的routing-key是&quot;routing_direct_key&quot;，那发送消息使用的routing_key也要是&quot;routing_direct_key&quot;。 1.2 Fanout 扇形交换机 不处理路由键。你只需要简单的将队列绑定到交换机上。一个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。这种模式效率很高，因为不进行routing-key的匹配，大大减小了时间开销。 1.3 Topic 主题交换机 与Direct Exchange的完全匹配规则不同，Topic Exchange是一种模式匹配的模式。将路由键和某模式进行匹配。此时队列需要绑定要一个模式上。符号“#”匹配一个或多个词，符号“*”匹配不多不少一个词。因此“abc.#”能够匹配到“abc.def.ghi”，但是“abc.” 只会匹配到“abc.def”。 这种方式非常适合把一个消息投递到多个queue（应用）。 1.4 Headers 首部模式交换机 不处理路由键。而是根据发送的消息内容中的headers属性进行匹配。在绑定Queue与Exchange时指定一组键值对；当消息发送到RabbitMQ时会取到该消息的headers与Exchange绑定时指定的键值对进行匹配；如果完全匹配则消息会路由到该队列，否则不会路由到该队列。 匹配规则x-match有下列两种类型： x-match = all ：表示所有的键值对都匹配才能接受到消息 x-match = any ：表示只要有键值对匹配就能接受到消息 部分代码实例： &lt;?php /** 生产者 **/ ...... $channel = new \\AMQPChannel($connection); /** 创建一个header交换机 **/ $exchangeName = 'type_header_exchange'; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_HEADERS); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); // 设置header $msg1 = 'push type_header_exchange message'; $exchange-&gt;publish($msg1, null, AMQP_MANDATORY, ['delivery_mode' =&gt; 2, 'headers' =&gt; ['user'=&gt;'moyuxing', 'pwd'=&gt;'654321']]); ...... ?&gt; &lt;?php /** 消费者 **/ ...... $channel = new \\AMQPChannel($connection); /** 创建一个队列和exchange绑定起来 **/ $exchangeName = 'type_header_exchange'; $queueName = 'type_header_queue'; $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;declareQueue(); // 设置完全匹配才能接收信息 $queue-&gt;bind($exchangeName, null, ['x-match'=&gt;'all','user'=&gt;'moyuxing','pwd'=&gt;'111111']); ...... ?&gt; 二、5种工作模式 简单模式：一个生产者，一个消费者 work模式：一个生产者，多个消费者，每个消费者获取到的信息唯一（消费者彼此竞争成为接收者） 订阅模式：一个生产者发送的消息会被多个消费者获取 路由模式：发送消息到交换机并且要指定路由key，消费者将队列绑定到交换机时需要指定路由key 主题模式：与路由模式完全匹配不同，主题模式是通过通配符模式来匹配，通俗的说就是模式匹配一个或多个key。 2.1 简单模式 这种模式下不需要将Exchange进行任何绑定(binding)操作。 &lt;?php // Producer 生产者 $config = [ 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ]; $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_simple_exchange'; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_DIRECT); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); //消息内容 $msg = 'push message'; $exchange-&gt;publish($msg, null, AMQP_MANDATORY, array('delivery_mode' =&gt; 2)); &lt;?php // Consumer 消费者 $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_simple_exchange'; $queueName = 'type_simple_queue'; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_DIRECT); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;declareQueue(); $queue-&gt;bind($exchange-&gt;getName()); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo '队列：' . $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $queue-&gt;consume(&quot;receive&quot;); //$q-&gt;consume(&quot;receive&quot;, AMQP_AUTOACK);//隐式确认,不推荐 2.2 work模式 一个生产者，一个队列，多个消费者，每个消费者获取到的消息唯一，代码与”简单模式“是一样的，唯一的区分就是多开了消费者，增加了队列消息的消费能力。 2.3 订阅模式 这种模式需要提前将Exchange与Queue进行绑定，一个Exchange可以绑定多个Queue，一个Queue也可以同多个Exchange进行绑定。 这里用到Exchange的”fanout“模式（关于交换机Exchange模式后面会介绍） producer.php &lt;?php // Producer 生产者 $config = [ 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ]; $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_pubsub_exchange'; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_FANOUT); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); //消息内容 $msg = 'push message'; $exchange-&gt;publish($msg, null, AMQP_MANDATORY, array('delivery_mode' =&gt; 2)); ?&gt; consumer_1.php &lt;?php // 消费者1 $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_pubsub_exchange'; $queueName = 'type_pubsub_queue_1'; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_FANOUT); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;declareQueue(); $queue-&gt;bind($exchange-&gt;getName()); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo 'type_pubsub_queue_1：' . $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $queue-&gt;consume(&quot;receive&quot;); ?&gt; consumer_2.php &lt;?php // 消费者1 $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_pubsub_exchange'; $queueName = 'type_pubsub_queue_2'; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_FANOUT); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;declareQueue(); $queue-&gt;bind($exchange-&gt;getName()); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo 'type_pubsub_queue_2：' . $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $queue-&gt;consume(&quot;receive&quot;); ?&gt; 如上图，两个消费者获得了同一条消息。即就是，一个消息从交换机同时发送给了两个队列中，监听这两个队列的消费者消费了这个消息。 如果没有队列绑定交换机，则消息将丢失，因为交换机没有存储能力，消息只能存储在队列中。 2.4 路由模式 也是一种路由完全匹配模式。 发送消息到交换机并且要指定路由key 消费者将队列绑定到交换机时需要指定路由key 完全匹配，只有匹配到的消费者才能消费消息 一个队列可以绑定多个路由 &lt;?php // Producer 生产者 $config = [ 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ]; $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_route_exchange'; $keyNames = [ 'type_route_key_apple', 'type_route_key_pear', 'type_route_key_dog' ]; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_DIRECT); $exchange-&gt;setFlags(AMQP_DURABLE); // 绑定多个key $exchange-&gt;bind($exchangeName, $keyNames[0]); $exchange-&gt;bind($exchangeName, $keyNames[1]); $exchange-&gt;bind($exchangeName, $keyNames[2]); $exchange-&gt;declareExchange(); //消息内容，发送到不同的key $msg1 = 'push type_route_key_apple message'; $exchange-&gt;publish($msg1, 'type_route_key_apple', AMQP_MANDATORY, array('delivery_mode' =&gt; 2)); $msg2 = 'push type_route_key_pear message'; $exchange-&gt;publish($msg2, 'type_route_key_pear', AMQP_MANDATORY, array('delivery_mode' =&gt; 2)); $msg3 = 'push type_route_key_dog message'; $exchange-&gt;publish($msg3, 'type_route_key_dog', AMQP_MANDATORY, array('delivery_mode' =&gt; 2)); ?&gt; // 接收者1，绑定接收了`type_route_key_apple`和`type_route_key_pear`队列的信息 &lt;?php $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_route_exchange'; $queueName = 'type_route_queue_1'; $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;declareQueue(); $queue-&gt;bind($exchangeName, 'type_route_key_apple'); $queue-&gt;bind($exchangeName, 'type_route_key_pear'); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo 'type_route_queue_1：' . $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $queue-&gt;consume(&quot;receive&quot;); //$q-&gt;consume(&quot;receive&quot;, AMQP_AUTOACK);//隐式确认,不推荐 ?&gt; // 接收者2，绑定接收了`type_route_key_dog` &lt;?php $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_route_exchange'; $queueName = 'type_route_queue_2'; $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;declareQueue(); $queue-&gt;bind($exchangeName, 'type_route_key_dog'); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo 'type_route_queue_2：' . $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $queue-&gt;consume(&quot;receive&quot;); //$q-&gt;consume(&quot;receive&quot;, AMQP_AUTOACK);//隐式确认,不推荐 ?&gt; 如上图，生产者端把消息推送到type_route_exchange交换机，并绑定了apple、pear和dog三个路由。 消费者1设置了队列type_route_queue_1，并绑定了apple和pear两个路由，通过完全匹配路由接收生产者推送的消息。 2.5 主题模式 主题模式也可以说是一个通配符模式，通过通配符匹配多个路由来向对应的队列发送消息。 上面的路由模式是根据路由key进行完整的匹配（完全相等才发送消息），这里的通配符模式通俗的来讲就是模糊匹配。符号“#”表示匹配一个或多个词，符号“*”表示匹配一个词。 &lt;?php // Producer 生产者 $config = [ 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ]; $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_topic_exchange'; $keyNames = [ 'type_topic_key_fruit_apple', 'type_topic_key_fruit_pear', ]; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_TOPIC); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); // 绑定多个key $exchange-&gt;bind($exchangeName, $keyNames[0]); $exchange-&gt;bind($exchangeName, $keyNames[1]); $exchange-&gt;bind($exchangeName, $keyNames[2]); //消息内容，发送到不同的key $msg1 = 'push type_topic_key_fruit_apple message'; $exchange-&gt;publish($msg1, 'type_topic_key_fruit_apple', AMQP_MANDATORY, array('delivery_mode' =&gt; 2)); $msg2 = 'push type_topic_key_fruit_pear message'; $exchange-&gt;publish($msg2, 'type_topic_key_fruit_pear', AMQP_MANDATORY, array('delivery_mode' =&gt; 2)); ?&gt; &lt;?php // Consumer 消费者 $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } $channel = new \\AMQPChannel($connection); /** 创建一个普通的交换机和队列 **/ $exchangeName = 'type_topic_exchange'; $queueName = 'type_topic_queue'; $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;declareQueue(); // 模糊匹配，使用.*匹配一个字符，可同时通过apple和pear两个路由来匹配到该队列来获取信息 $queue-&gt;bind($exchangeName, 'type_topic_key_fruit_.*'); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo 'type_topic_queue：' . $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $queue-&gt;consume(&quot;receive&quot;); ?&gt; ","link":"http://mofish.pily.life/post/rabbitmq_learning_03/"},{"title":"RabbitMq学习之路(二)：如何保证消息消息不丢失，进行可靠性传输？","content":"该章节主要从生产者到RabbitMq、RabbitMq自身以及RabbitMq到消费者三个方面来整理使用RabbitMq时，如何保证消息的可靠性以及防止消息丢失的问题。 目录 一、生产者到RabbitMq 1.1 事务机制 1.2 确认机制 1.3 信息补偿机制 二、RabbitMq自身 三、RabbitMq到消费者 3.1 手动确认机制 3.2 死信队列DLX(dead-letter-exchange) 3.3 延迟队列(补充知识点) 一、生产者到RabbitMq 这个方面是为了确认生产者能把消息投送到broker代理服务器，只有到达了服务器，才能进行信息持久化存储，因为为了确保消息投递的可靠性，RabbitMq为我们提供了事务机制和Confirm确认机制（信息补偿机制是对机制上的一种完善）。 1.1 事务机制 简述： 类似Mysql的事务机制，在通开启事务之后，我们便可以发布消息给broker代理服务器了，如果提交成功了，则消息一定到达了broker了，如果在提交执行之前broker异常崩溃或者由于其他原因抛出异常，这个时候我们便可以捕获异常通过Rollback回滚事务了。 实现方式： &lt;?php // Producer 生产者 $config = [ 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ]; $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { return ['code' =&gt; 600, 'data' =&gt; '连接失败']; } $channel = new \\AMQPChannel($connection); $exchange = new \\AMQPExchange($channel); //消息的路由键，一定要和消费者端一致 $routingKey = 'key_1'; //交换机名称，一定要和消费者端一致， $exchangeName = 'exchange_1'; $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_DIRECT); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); //创建10个消息 $logs = []; try { $channel-&gt;startTransaction(); //消息内容 for ($i=1;$i&lt;=10;$i++){ $msg = array( 'data' =&gt; 'message_'.$i, 'hello' =&gt; 'world', ); if ($i == 5) { $msg['data'] = 'message_' . $i/0; } //发送消息到交换机，并返回发送结果 //delivery_mode:2声明消息持久，持久的队列+持久的消息在RabbitMQ重启后才不会丢失 $logs[] = &quot;Send Message:&quot;.$exchange-&gt;publish(json_encode($msg), $routingKey, AMQP_NOPARAM, array('delivery_mode' =&gt; 2)).&quot;\\n&quot;; } //代码执行完毕后进程会自动退出 $channel-&gt;commitTransaction(); echo &quot;全部发送成功&quot;; } catch (\\Exception $e) { $channel-&gt;rollbackTransaction(); echo &quot;全部失败&quot; . $e-&gt;getMessage(); } &lt;?php // Consumer 消费者 //声明连接参数 $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $cnn = new \\AMQPConnection($config); if (!$cnn-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } //在连接内创建一个通道 $ch = new \\AMQPChannel($cnn); //创建一个交换机 $ex = new \\AMQPExchange($ch); //声明路由键 $routingKey = 'key_1'; //声明交换机名称 $exchangeName = 'exchange_1'; //设置交换机名称 $ex-&gt;setName($exchangeName); //设置交换机类型 //AMQP_EX_TYPE_DIRECT:直连交换机 //AMQP_EX_TYPE_FANOUT:扇形交换机 //AMQP_EX_TYPE_HEADERS:头交换机 //AMQP_EX_TYPE_TOPIC:主题交换机 $ex-&gt;setType(AMQP_EX_TYPE_DIRECT); //设置交换机持久 $ex-&gt;setFlags(AMQP_DURABLE); //声明交换机 $ex-&gt;declareExchange(); //创建一个消息队列 $q = new \\AMQPQueue($ch); //设置队列名称 $q-&gt;setName('queue_1'); //设置队列持久 $q-&gt;setFlags(AMQP_DURABLE); //声明消息队列 $q-&gt;declareQueue(); //交换机和队列通过$routingKey进行绑定 $q-&gt;bind($ex-&gt;getName(), $routingKey); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $q-&gt;consume(&quot;receive&quot;); //$q-&gt;consume(&quot;receive&quot;, AMQP_AUTOACK);//隐式确认,不推荐 不足之处： 开启-提交-回滚，每次操作都相当于一次请求，降低了消息的吞吐量，另外因为走的通信太多，大量消息就会大量请求服务器，增加耗时。 总结： 事务确实能够解决producer与broker之间消息确认的问题，只有消息成功被broker接受，事务提交才能成功，否则我们便可以在捕获异常进行事务回滚操作同时进行消息重发，但是使用事务机制的话会降低RabbitMq的消息吞吐量。 1.2 确认机制 简述： Producer 将信道设置成 confirm 模式，一旦信道进入 confirm 模式，所有在该信道上面发布的消息都会被指派一个唯一的ID，一旦消息被投递到匹配的队列后，broker 就会发送一个 ack 给 Producer，这就使得Producer知道消息已经正确到达目的队列了。 注意：已经在transaction事务模式的channel是不能再设置成confirm模式的，即这两种模式是不能共存的。 实现方式 对于客户端来说，要实现 Producer 的 confirm 模式通过调用confirm_select()开启，一共有三种方式：普通模式、批量模式和异步模式。 普通confirm模式 原理：publish一条消息后，调用wait_for_pending_acks()等待服务器端confirm,如果服务端返回false或者超时时间内未返回，客户端进行消息重传。 缺点：如果需要批量推送消息时，效率低。 批量confirm模式 原理：客户端程序需要定期（每隔多少秒）或者定量（达到多少条）或者两则结合起来 publish 消息，然后调用wait_for_pending_acks()等待服务器端 confirm。 优点：相比普通 confirm 模式，批量极大提升 confirm 效率。 缺点：问题在于一旦出现 confirm 返回false或者超时的情况时，客户端需要将这一批次的消息全部重发，这会带来明显的重复消息数量，并且，当消息经常丢失时，批量 confirm 性能应该是不升反降的。 异步confirm模式 原理：提供一个回调方法，服务端 confirm 了一条或者多条消息后客户端会回调这个方法。 优点：异步、非阻塞的、性能好，所以比较常用。 缺点：复杂度增加。 优点 保证消息的准确到达broker服务器 相较于事务模式，confirm模式是异步的，性能更好、更高效 总结 confirm 模式最大的好处在于他是异步的，一旦发布一条消息，生产者应用程序就可以在等信道返回确认的同时继续发送下一条消息，当消息最终得到确认ack之后，生产者应用便可以通过回调方法来处理该确认消息，如果RabbitMq因为自身内部错误导致消息丢失，就会发送一条nack消息，生产者应用程序同样可以在回调方法中处理该nack消息。 1.3 信息补偿机制 关于 Producer 的可靠性投递，我们思考一下，一共有如下要求： 保证消息的成功发出 保证Mq节点的成功接收 发送端收到Mq节点(broker)确认应答 但是在实际生产中，很难保障这三点的完全可靠，比如在极端环境中，生产者发送消息失败了，发送端在接受确认应答时网络闪断等等情况，很难保证可靠性投递，因此就需要有完善的消息补偿机制。 简单的说，就是将消息持久化到DB并设置状态值，当收到 ack 应答时就改变当前记录的状态，除此之外再通过定时脚本轮询检查还木有收到 ack 应答的消息，在重试次数范围内进行重新推送。 二、RabbitMq自身 处理消息队列丢失数据的方式，一般都是开启持久化磁盘。 持久化配置可以和生产者的 confirm 机制配合使用，在消息持久化磁盘后，再给生产者发送一个Ack信号。这样的话，如果消息持久化磁盘之前，即使 RabbitMQ 挂掉了，生产者也会因为收不到Ack信号而再次重发消息。 持久化设置如下（必须同时设置以下 2 个配置）： 创建queue的时候，将queue的持久化标志durable在设置为true，代表是一个持久的队列，这样就可以保证 rabbitmq 持久化 queue 的元数据，但是不会持久化queue里的数据； 2.发送消息的时候将 deliveryMode 设置为 2，将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。 这样设置以后，即使rabbitMQ挂了，重启后也能恢复数据。 三、RabbitMq到消费者 消费者丢数据默认因为采用了自动确认消息模式。该模式下，虽然消息还在处理中，但是消费中者会自动发送一个确认，通知 RabbitMq 已经收到消息了，这时 RabbitMq 就会立即将消息删除。这种情况下，如果消费者出现异常而未能处理消息，那就会丢失该消息。 3.1 手动确认机制 解决方案就是采用手动确认消息，设置 autoAck = False，等到消息被真正消费之后，再手动发送一个确认信号，即使中途消息没处理完，但是服务器宕机了，那 RabbitMq 就收不到发的ack，然后 RabbitMq 就会将这条消息重新分配给其他的消费者去处理。 值得注意的是，只有消费者确认了消息，RabbitMq才能安全地把消息从队列中删除。这里并没有用到超时机制，RabbitMq仅通过Consumer的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMq给了Consumer足够长的时间来处理消息，但是下面有两种特殊情况： 如果消费者接收到消息，在确认之前断开了连接或取消订阅，RabbitMQ会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消息重复消费的隐患，需要根据业务Id去重）； 如果消费者接收到消息却没有确认消息，连接也未断开，则RabbitMQ认为该消费者繁忙，将不会给该消费者分发更多的消息。 3.2 死信队列DLX 简述 DLX也是一个正常的Exchange，和一般的Exchange没有任何区别。能在任何的队列上被指定，实际上就是设置某个队列的属性。当这个队列出现死信（dead message，就是没有任何消费者消费）的时候，RabbitMQ就会自动将这条消息重新发布到Exchange上去，进而被路由到另一个队列。可以监听这个队列中的消息作相应的处理。 消息队列中的消息会在一下几种情况下变成死信 消息被拒绝(basic.reject / basic.nack)，并且requeue = false； 消息TTL过期； 队列达到最大长度； 实现方式 &lt;?php // Producer 生产者 $config = [ 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ]; $connection = new \\AMQPConnection($config); if (!$connection-&gt;connect()) { return ['code' =&gt; 600, 'data' =&gt; '连接失败']; } $channel = new \\AMQPChannel($connection); /** 创建一个存放死信的交换机和队列 **/ $deadExchangeName = 'dead_exchange_1'; $deadQueueName = 'dead_queue_1'; $deadKeyName = 'dead_key_1'; $deadExchange = new AMQPExchange($channel); $deadExchange-&gt;setName($deadExchangeName); $deadExchange-&gt;setType(AMQP_EX_TYPE_DIRECT); $deadExchange-&gt;setFlags(AMQP_DURABLE); $deadExchange-&gt;declareExchange(); $deadQueue = new AMQPQueue($channel); $deadQueue-&gt;setName($deadQueueName); $deadQueue-&gt;setFlags(AMQP_DURABLE); $deadQueue-&gt;declareQueue(); $deadQueue-&gt;bind($deadExchange-&gt;getName(), $deadKeyName); /** 创建一个普通的交换机和队列 **/ $routingKey = 'key_1'; $exchangeName = 'exchange_1'; $queueName = 'queue_1'; $exchange = new \\AMQPExchange($channel); $exchange-&gt;setName($exchangeName); $exchange-&gt;setType(AMQP_EX_TYPE_DIRECT); $exchange-&gt;setFlags(AMQP_DURABLE); $exchange-&gt;declareExchange(); $queue = new AMQPQueue($channel); $queue-&gt;setName($queueName); $arguments = [ 'x-message-ttl' =&gt; 5000, 'x-dead-letter-exchange' =&gt; $deadExchangeName, //死信发送的交换机 'x-dead-letter-routing-key' =&gt; $deadKeyName, //死信routeKey ]; $queue-&gt;setFlags(AMQP_DURABLE); // 设置队列持久性 $queue-&gt;setArguments($arguments); $queue-&gt;declareQueue(); $queue-&gt;bind($exchange-&gt;getName(), $routingKey); //创建10个消息 $logs = []; try { //消息内容 for ($i=1;$i&lt;=10;$i++){ $msg = array( 'data' =&gt; 'message_'.$i, 'hello' =&gt; 'world', ); //发送消息到交换机，并返回发送结果 //delivery_mode:2声明消息持久，持久的队列+持久的消息在RabbitMQ重启后才不会丢失 $logs[] = &quot;Send Message:&quot;.$exchange-&gt;publish(json_encode($msg), $routingKey, AMQP_NOPARAM, array('delivery_mode' =&gt; 2)).&quot;\\n&quot;; } //代码执行完毕后进程会自动退出 echo &quot;全部发送成功&quot;; } catch (\\Exception $e) { echo &quot;全部失败&quot; . $e-&gt;getMessage(); } &lt;?php $config = array( 'host' =&gt; '127.0.0.1', 'vhost' =&gt; '/', 'port' =&gt; 5672, 'login' =&gt; 'guest', 'password' =&gt; 'guest' ); //连接broker $cnn = new \\AMQPConnection($config); if (!$cnn-&gt;connect()) { echo &quot;Cannot connect to the broker&quot;; exit(); } //在连接内创建一个通道 $ch = new \\AMQPChannel($cnn); //创建一个交换机 $ex = new \\AMQPExchange($ch); //声明路由键 $routingKey = 'dead_key_1'; //声明交换机名称 $exchangeName = 'dead_exchange_1'; //设置交换机名称 $ex-&gt;setName($exchangeName); //设置交换机类型 //AMQP_EX_TYPE_DIRECT:直连交换机 //AMQP_EX_TYPE_FANOUT:扇形交换机 //AMQP_EX_TYPE_HEADERS:头交换机 //AMQP_EX_TYPE_TOPIC:主题交换机 $ex-&gt;setType(AMQP_EX_TYPE_DIRECT); //设置交换机持久 $ex-&gt;setFlags(AMQP_DURABLE); //声明交换机 $ex-&gt;declareExchange(); //创建一个消息队列 $q = new \\AMQPQueue($ch); //设置队列名称 $q-&gt;setName('dead_queue_1'); //设置队列持久 $q-&gt;setFlags(AMQP_DURABLE); //声明消息队列 $q-&gt;declareQueue(); //交换机和队列通过$routingKey进行绑定 $q-&gt;bind($ex-&gt;getName(), $routingKey); //接收消息并进行处理的回调方法 function receive($envelope, $queue) { //休眠两秒， sleep(2); //echo消息内容 echo '死信队列：' . $envelope-&gt;getBody().&quot;\\n&quot;; //显式确认，队列收到消费者显式确认后，会删除该消息 $queue-&gt;ack($envelope-&gt;getDeliveryTag()); } //设置消息队列消费者回调方法，并进行阻塞 $q-&gt;consume(&quot;receive&quot;); //$q-&gt;consume(&quot;receive&quot;, AMQP_AUTOACK);//隐式确认,不推荐 3.3 延时队列(额外知识点) 存储对应的延迟消息，当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。在 RabbitMQ 中并不存在延迟队列，但我们可以通过设置消息的过期时间和死信队列来实现延迟队列，消费者监听死信交换器绑定的队列，而不要监听消息发送的队列。 ","link":"http://mofish.pily.life/post/rabbitmq_learning_02/"},{"title":"RabbitMq学习之路(一)：基础简介和零碎知识点","content":"该章节主要是整理一下RabbitMq的基础的和零碎的知识点。 1. RabbitMq是什么？ RabbitMQ是一款开源的，Erlang编写的，基于AMQP协议的消息中间件，是一个使用队列来通信的组件。 2. AMQP 2.1 AMQP是什么? AMQP（Advanced Message Queuing Protocol, 高级消息队列协议）是一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制。 2.2 AMQP的三大组件 交换器 (Exchange)：接收Publisher发送的消息，并根据一定的规则将这些消息路由到“消息队列。 队列 (Queue)：存储消息，直到这些消息被消费者安全处理完为止，位于硬盘或内存中。 绑定 (Binding)：定义了exchange和queue之间的关联，提供路由规则。 3. 为什么要用RabbitMq？ 优点：解耦、异步、削峰 缺点：复杂度提高、系统可用性降低、一致性问题、重复消费问题、可靠性传输问题 4. 专业名称 &gt; * Server(broker)：接受客户端连接，实现AMQP消息队列和路由功能的进程。 &gt; * Virtual Host：一个虚拟概念，类似于权限控制组，一个Virtual Host里面可以有若干个Exchange和Queue，但是权限控制的最小粒度是Virtual Host &gt; * Producer：消息生产者 &gt; * Consumer：消息消费者 &gt; * Queue：存储消息得队列容器 &gt; * Message：消费者真正需要的消息数据 &gt; * Connection：无论是生产者还是消费者，都需要和 RabbitMQ Broker 建立连接，这个连接就是一条 TCP 连接，也就是 Connection。 &gt; * Channel：一旦 TCP 连接建立起来，客户端紧接着可以创建一个 AMQP 信道（Channel），每个信道都会被指派一个唯一的 ID。信道是建立在 Connection 之上的虚拟连接，RabbitMQ 处理的每条 AMQP 指令都是通过信道完成的一个管道连接。便于TCP 连接复用，不仅可以减少性能开销，同时也便于管理。 &gt; * Exchange：消息路由,生产者发送消息并不是直接发送到队列中的而是先到指定方式路由中，然后由路由根据路由key绑定的队列发送到指定队列中(某种算法求出对应的queue,如对消息取模)。 &gt; * Binding：对路由与队列容器的绑定关系 &gt; * Routing Key：路由键，主要用来寻找队列Queue 5. 消息基于什么传输？ 由于TCP连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ使用信道的方式来传输数据。信道是建立在真实的TCP连接内的虚拟连接，且每条TCP连接上的信道数量没有限制。 6. 生产者消息运转 Producer先连接到Broker，建立连接Connection，开启一个信道Channel； Producer声明一个交换器Exchange并设置好相关属性 Producer声明一个队列Queue并设置号相关属性 Producer通过路由键（Routing Key）将交换器和队列绑定起来 Producer发送消息到Broker,其中包含路由键、交换器等信息 相应的交换器根据接收到的路由键查找匹配的队列 如果找到，将消息存入对应的队列，如果没有找到，会根据生产者的配置丢弃或者退回给生产者 关闭信道，关闭连接 7. 消费者接受信息过程 Consumer先连接到Broker，建立连接Connection，开启一个信道(Channel) 向Broker请求消费响应的队列中消息，可能会设置响应的回调函数 等待Broker回应并投递相应队列中的消息，接收消息 Consumer确认收到的消息，Ack RabbitMq从队列中删除已经确定的消息 关闭信道，关闭连接 8. 交换器无法根据自身类型和路由键找到符合条件队列时，有哪些处理？ mandatory：true 返回消息给生产者 mandatory：false 直接丢弃 immediate：true 找到对应的queue，但是找不到消费者时，返回消息给生产者 9. RabbitMQ中消息可能有的几种状态 alpha: 消息内容(包括消息体、属性和 headers) 和消息索引都存储在内存中 。 beta: 消息内容保存在磁盘中，消息索引保存在内存中。 gamma: 消息内容保存在磁盘中，消息索引在磁盘和内存中都有 。 delta: 消息内容和索引都在磁盘中 。 10. 什么是优先级队列？ 概念：优先级高的队列会先被消费 参数：生产者可通过传递x-max-priority参数告知消费者通过优先级来处理消息 注意：当消费速度大于生产速度且Broker没有堆积的情况下，优先级显得没有意义 实际应用：不同vip的视频转码优先级 11. 如何确保消息消费的顺序性？ 方案一：拆分Queue，使得一个Queue只对应一个消费者（利用先进先出的特性）； 方案二：每个消息生成一个有序的id，可以在消费端实现前一条消息未消费，不处理下一条消息；也可以在生产端实现前一条消息未处理完毕，不发布下一条消息。 12. 如何保证消息不被重复消费(幂等性)？ 正常情况下，消费者在消费消息的时候，消费完毕后，会发送一个确认消息给消息队列，消息队列就知道该消息被消费了，就会将该消息从消息队列中删除。 但是因为网络传输等等故障，确认信息没有传送到消息队列，导致消息队列不知道自己已经消费过该消息了，再次将消息分发给其他的消费者。 针对以上问题，一个解决思路是：保证消息的唯一性，就算是多次传输，不要让消息的多次消费带来影响，保证消息等幂性。 比如：在写入消息队列的数据做唯一标示，消费消息时，根据唯一标识判断是否消费过。 ","link":"http://mofish.pily.life/post/rabbitmq_learning_01/"},{"title":"Linux防火墙及开放端口管理（已迁移）","content":"## 查看防火墙是否开启 $ systemctl status firewalld ## 若没有开启则是开启状态，关闭则start改为stop $ systemctl start firewalld ## 查看所有开启的端口 $ firewall-cmd --list-ports ## 查看防火墙所有开放的端口 $ firewall-cmd --zone=public --list-ports ## 防火墙开启端口访问。 $ firewall-cmd --zone=public --add-port=80/tcp --permanent ## 命令含义： --zone #作用域 --add-port=80/tcp #添加端口，格式为：端口/通讯协议 --permanent #永久生效，没有此参数重启后失效 ## 防火墙关闭端口访问 $ firewall-cmd --zone=public --remove-port=5672/tcp --permanent ## 重启命令，重新载入配置，比如添加规则之后，需要执行此命令 $ firewall-cmd --reload ## 查看防火墙状态，是否是running $ firewall-cmd --state ## 列出支持的zone $ firewall-cmd --get-zones ## 列出支持的服务，在列表中的服务是放行的 $ firewall-cmd --get-services ## 查看ftp服务是否支持，返回yes或者no $ firewall-cmd --query-service ftp ## 永久开放ftp服务 $ firewall-cmd --add-service=ftp --permanent ## 永久移除ftp服务 $ firewall-cmd --remove-service=ftp --permanent ## 关闭防火墙，如果要开放的端口太多，嫌麻烦，可以关闭防火墙，安全性自行评估 $ systemctl stop firewalld.service ","link":"http://mofish.pily.life/post/linux-fang-huo-qiang-ji-kai-fang-duan-kou-guan-li/"},{"title":"Linux进程端口查看命令（已迁移）","content":"主要简单记录一下ps、lsof、netstat这三个命令。 1. ps Linux ps （英文全拼：process status）命令用于显示当前进程的状态，类似于 windows 的任务管理器。 1.1 使用格式 $ ps [options] [--help] 常用参数： -aux：显示所有包含其他使用者的行程、 输出格式：USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND USER: 行程拥有者 PID: pid %CPU: 占用的 CPU 使用率 %MEM: 占用的记忆体使用率 VSZ: 占用的虚拟记忆体大小 RSS: 占用的记忆体大小 TTY: 终端的次要装置号码 (minor device number of tty) STAT: 该行程的状态： D: 无法中断的休眠状态 (通常 IO 的进程) R: 正在执行中 S: 静止状态 T: 暂停执行 Z: 不存在但暂时无法消除 W: 没有足够的记忆体分页可分配 &lt;: 高优先序的行程 N: 低优先序的行程 L: 有记忆体分页分配并锁在记忆体内 (实时系统或捱A I/O) START: 行程开始时间 TIME: 执行的时间 COMMAND:所执行的指令 -ef：与aux类型，只不过是以System V方式显示 输出格式：UID PID PPID C STIME TTY TIME CMD UID：启动这些进程的用户 PID：进程的进程ID PPID：父进程的进程号（如果该进程是由另一个进程启动的） C：进程生命周期中的CPU利用率 STIME：进程启动时的系统时间 TTY：进程启动时的终端设备 TIME：运行进程需要的累计CPU时间 CMD：启动的程序名称 1.2 常用实例 ## 用户查找指定进程，查看该进程的相关状态 $ ps -ef | grep mysql $ ps -aux | grep mysql 2. lsof lsof(list open files)是一个列出当前系统打开文件的工具。 2.1 使用格式 $ lsof ［options］ filename 输出格式：COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME COMMAND：进程名称 PID：进程ID USER：进程所有者 FD：文件描述符，应用程序通过文件描述符识别该文件。如cwd、txt等 TYPE：文件类型，如DIR、REG等 DEVICE：指定磁盘的名称 SIZE：文件的大小 NODE：索引节点（文件再磁盘上的标识） NAME：打开文件的确切名称 2.2 常用实例 ## 查看8080端口的占用情况 $ lsof -i:8080 ## 显示开启文件abc.txt的进程 $ lsof abc.txt ## 显示nginx进程现在打开的文件 $ lsof -c nginx ## 显示目录下被进程开启的文件 $ lsof +d /usr/local/ 3. netstat netstat命令的功能是显示网络连接、路由表和网络接口信息，可以让用户得知目前都有哪些网络连接正在运作 3.1 使用格式 netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][--ip] 常用参数： -a或--all 显示所有连线中的Socket。 -e或--extend 显示网络其他相关信息。 -h或--help 在线帮助。 -i或--interfaces 显示网络界面信息表单。 -l或--listening 显示监控中的服务器的Socket。 -n或--numeric 以数字形式显示地址和端口号。 -o或--timers 显示计时器。 -p或--programs 显示正在使用Socket的程序识别码和程序名称。 -t或--tcp 显示TCP传输协议的连线状况。 -u或--udp 显示UDP传输协议的连线状况。 -v或--verbose 显示指令执行过程。 -V或--version 显示版本信息。 3.2 常用实例 ## 查看端口占用 $ netstat -tunlp | grep 端口号 ","link":"http://mofish.pily.life/post/linux-jin-cheng-duan-kou-cha-kan-ming-ling/"},{"title":"Linux日志关键字查找（已迁移）","content":"grep、tail、cat、more、less命令 1. grep Linux grep 命令用于查找文件里符合条件的字符串。 grep 指令用于查找内容包含指定的范本样式的文件，如果发现某文件的内容符合所指定的范本样式，预设 grep 指令会把含有范本样式的那一列显示出来。 若不指定任何文件名称，或是所给予的文件名为-，则 grep 指令会从标准输入设备读取数据。 1.1 使用格式 $ grep &lt;参数&gt; &lt;查询条件&gt; &lt;文件名或文件夹&gt; 常用参数： -A&lt;显示行数&gt; 或 --after-context=&lt;显示行数&gt; : 除了显示符合范本样式的那一列之外，并显示该行之后的内容。 -b 或 --byte-offset : 在显示符合样式的那一行之前，标示出该行第一个字符的编号。 -B&lt;显示行数&gt; 或 --before-context=&lt;显示行数&gt; : 除了显示符合样式的那一行之外，并显示该行之前的内容。 -c 或 --count : 计算符合样式的列数。 -C&lt;显示行数&gt; 或 --context=&lt;显示行数&gt;或-&lt;显示行数&gt; : 除了显示符合样式的那一行之外，并显示该行之前后的内容。 -e&lt;范本样式&gt; 或 --regexp=&lt;范本样式&gt; : 指定字符串做为查找文件内容的样式。 -i 或 --ignore-case : 忽略字符大小写的差别。 -n 或 --line-number : 在显示符合样式的那一行之前，标示出该行的列数编号。 -r 或 --recursive : 此参数的效果和指定&quot;-d recurse&quot;参数相同。 -v 或 --invert-match : 显示不包含匹配文本的所有行。 -w 或 --word-regexp : 只显示全字符合的列。 1.2 常用实例 ## 在logs目录中所有.log文件中查找包含“babytest”关键字的数据，并显示行数、忽略大小写 $ grep -in babytest /logs/*.log ## 查找完全匹配“babytest”的数据 $ grep -inw babytest /logs/*.log ## 在logs目录中所有.log文件中查找包含“babytest”关键字的数据，并显示该行之前后的3行内容 $ grep -in -C3 babytest /logs/*.log ## 以递归的方式查找符合条件的文件。例如，查找指定目录/logs及其子目录（如果存在子目录的话）下所有文件中包含字符串&quot;babytest&quot;的文件，并打印出该字符串所在行的内容 $ grep -r babytest /logs ## 从文件内容查找与正则表达式匹配的行： $ grep –e &quot;正则表达式&quot; 文件名 ## 从根目录开始查找所有扩展名为 .log 的文本文件，并找出包含 &quot;ERROR&quot; 的行 $ find / -type f -name &quot;*.log&quot; | xargs grep &quot;ERROR&quot; 2. tail tail 命令可用于查看文件的内容，有一个常用的参数 -f 常用于查阅正在改变的日志文件。 tail 命令可用于查看文件的内容，有一个常用的参数 -f 常用于查阅正在改变的日志文件。 2.1 使用格式 $ tail [参数] [文件] 常用参数： -f 循环读取 -n&lt;行数&gt; 显示文件的尾部 n 行内容 --pid=PID 与-f合用,表示在进程ID,PID死掉之后结束 -s, --sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 2.2 常用实例 ## 默认显示 notes.log 文件的最后 10 行 $ tail notes.log ## 要跟踪名为 notes.log 的文件的增长情况，间隔时间为5秒 $ tail -f -s 5 notes.log ## 显示文件 notes.log 的内容，从第 20 行至文件末尾 $ tail -n +20 notes.log 3. cat cat命令用于连接文件并打印到标准输出设备上。 3.1 使用格式 $ cat [参数] [文件] 常用参数： -n 或 --number：由 1 开始对所有输出的行数编号。 -b 或 --number-nonblank：和 -n 相似，只不过对于空白行不编号。 -s 或 --squeeze-blank：当遇到有连续两行以上的空白行，就代换为一行的空白行。 3.2 常用实例 ## 把notes.log的内容输出到设备上 $ cat notes.log ## 把 textfile1 的文档内容加上行号后输入 textfile2 这个文档里 $ cat -n textfile1 &gt; textfile2 ## 把 textfile1 和 textfile2 的文档内容加上行号（空白行不加）之后将内容附加到 textfile3 文档里 $ cat -b textfile1 textfile2 &gt;&gt; textfile3 4. more Linux more 命令类似 cat ，不过会以一页一页的形式显示，更方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示，而且还有搜寻字串的功能（与 vi 相似），使用中的说明文件，请按 h 。 4.1 使用格式 $ more [参数] [fileNames..] 常用参数： -num 一次显示的行数 -s 当遇到有连续两行以上的空白行，就代换为一行的空白行 +/pattern 在每个文档显示前搜寻该字串（pattern），然后从该字串之后开始显示 +num 从第 num 行开始显示 fileNames 欲显示内容的文档，可为复数个数 4.2 常用实例 ## 逐页显示error.log文档的内容，如果有两行以上的空行则以一行空行代替 $ more -s error.log ## 一次显示20行，并且从第40行开始显示 $ more -num 20 +40 error.log 常用操作命令： Enter 向下n行，需要定义。默认为1行 Ctrl+F 向下滚动一屏 空格键 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 ：f 输出文件名和当前行的行号 V 调用vi编辑器 !命令 调用Shell，并执行命令 q 退出more 5. less less 与 more 类似，less 可以随意浏览文件，支持翻页和搜索，支持向上翻页和向下翻页，使用中的说明文件，请按 h 。 5.1 使用格式 $ less [参数] [fileNames..] 常用参数： -e 当文件显示结束后，自动离开 -g 只标志最后搜索的关键词 -i 忽略搜索时的大小写 -m 显示类似more命令的百分比 -N 显示每行的行号 -o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来 -s 显示连续空行为一行 5.2 常用实例 ## 浏览多个文件 $ less -sim error2021.log error2022.log ## ps查看进程信息并通过less分页显示 $ ps -ef | less 常用操作命令： /字符串：向下搜索&quot;字符串&quot;的功能 ?字符串：向上搜索&quot;字符串&quot;的功能 n：重复前一个搜索（与 / 或 ? 有关） N：反向重复前一个搜索（与 / 或 ? 有关） b 向上翻一页 d 向后翻半页 h 显示帮助界面 Q 退出less 命令 u 向前滚动半页 y 向前滚动一行 空格键 滚动一页 回车键 滚动一行 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 ma - 使用 a 标记文本的当前位置 'a - 导航到标记 a 处 ","link":"http://mofish.pily.life/post/linux_learning_02/"},{"title":"Linux的五个查找命令及其区别（已迁移）","content":"find、locate、which、whereis、type命令。 1. find find是最常见和最强大的查找命令，你可以用它找到任何你想找的文件。 1.1 使用格式 $ find &lt;指定目录&gt; &lt;指定条件&gt; &lt;指定动作&gt; &lt;指定目录&gt;： 所要搜索的目录及其所有子目录。默认为当前目录。 &lt;指定条件&gt;： 所要搜索的文件的特征。 &lt;指定动作&gt;： 对搜索结果进行特定的处理。 1.2 常用实例 ## 搜索当前目录（包含子目录）中，所有文件名以my开头的文件 $ find . -name &quot;my*&quot; ## 搜索当前目录及子目录中，所有文件名以my开头的文件（-iname忽略大小写），并显示它们的详细信息。 $ find . -iname &quot;My*&quot; -ls ## 搜索当前目录中，名为test的目录。 $ find . -type d -name test ## 搜索当前目录及子目录中，所有以.go为结尾的文件 $ find . -type f -name &quot;*.go&quot; ## 查找所有空文件-type f，如果是查找目录则是-type d $ find . -type f -empty ## 查找最近1小时内修改过的文件，查找查看过的用-amin $ find . -cmin -60 ## 查找所有被修改超过50天以及少于100天的文件，查找查看过的用-atime $ find . -ctime +50 -ctime -100 ## 搜索当前目录及其子目录中，查出指定文件大小或范围大小的文件（默认单位是块，1块=512字节） $ find . -size -10000 -size +10 -ls $ find . -size 1M $ find . -size -1G 注意：find是在硬盘上遍历查找，因此非常消耗硬盘的资源，而且效率也非常低，因此建议大家优先使用whereis和locate。 2. locate locate命令其实是“find -name”的另一种写法，但是要比后者快得多，原因在于它不搜索具体目录，而是搜索一个数据库（/var/lib/locatedb），这个数据库中含有本地所有文件信息。 2.1 使用格式 ## 一般情况我们只需要输入 locate your_file_name 即可查找指定文件 $ locate &lt;参数&gt; &lt;文件名&gt; 2.2 常用实例 ## 查找php.ini配置文件的路径 $ locate php.ini /www/server/php/74/etc/php.ini Linux系统自动创建这个数据库，并且每天自动更新一次，所以使用locate命令查不到最新变动过的文件。为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库。 3. which which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 $ which netstat /usr/bin/netstat which 查找的可执行文件，必须是要在 PATH 下的可执行文件，而不能是没有加入 PATH 的可执行文件，即使他就是可执行文件，但是没有加入到系统搜索路径，它仍然无法被 which 发现。 4. whereis whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 4.1 使用格式 $ whereis &lt;参数&gt; &lt;程序名&gt; 4.2 常用实例 ## 显示netstat命令的二进制程序 $ whereis -b netstat netstat: /usr/bin/netstat # netstat命令的二进制程序的地址 ## 显示netstat命令的帮助文件 $ whereis -m netstat netstat: /usr/share/man/man8/netstat.8.gz #netstat命令的帮助文件地址 这个命令可以用来查找二进制（命令）、源文件、man文件。与which不同的是这条命令可以是通过文件索引数据库而非PATH来查找的，所以查找的面比which要广。 5. type type命令其实不能算查找命令，它是用来区分某个命令到底是由shell自带的，还是由shell外部的独立二进制文件提供的。 5.1 使用格式 $ type &lt;参数&gt; &lt;程序名&gt; 5.2 常用实例 ## 如果是系统命令，系统会提示，cd是shell的自带命令（build-in） $ type cd cd is a shell builtin ## 如果是外部命令，系统会提示，并显示该命令的路径 $ type netstat netstat is /usr/bin/netstat # 外部命令使用-p参数，会显示该命令的路径，相当于which命令 $ type -p netstat /usr/bin/netstat ","link":"http://mofish.pily.life/post/linux_learning_01/"},{"title":"数据结构学习之路(七)：SkipList跳跃表","content":"zset是Redis提供的一个非常特别的数据结构，常用作排行榜等功能，以用户id为value，关注时间或者分数作为score进行排序。与其他数据结构相似，zset也有两种不同的实现，分别是ZipList和HashMap+SkipList。 ZipList和HashMap前面我们已经介绍过了，这章节就着重介绍跳跃表。 1. 简述 SkipList实质是一个可进行二分查找的有序链表，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。在功能上主要用来快速查询一个或者一个范围内的数据。 那为什么要用跳表呢？ 我们先来看一个有序链表，如下图： 如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。 假如我们在原链表之上新建一个链表，新链表是原链表每隔一个节点取一个。假设原链表为L0，新链表为L1，L1中的元素是L0中的第1、3、5、7、9……个节点，然后再建立L1和L0中各个节点的指针。这样L1就可以将L0中的范围缩小一半，同理对L1再建立新链表L2……，更高level的链表划分更大的区间，确定值域的大区间后，逐级向下缩小范围，如下图： 假设我们想找13，我们可以在L3中确定2-14的范围，在L2中确定8-14的范围，在L1中确定10-14的范围，在L0中找到13，整体寻找路径如下图红色路径，是不是比直接在L0中找13的绿色路径所经过的节点数少一些。 我们可以看到这种实现很像二分查找，只不过是实现将二分查找的中间点存储下来，用额外的空间换取了时间，因为其时间复杂度也和二分查找一致，都是O(logn)。 注意，只要高Level中有的节点，低Level中一定有，但高Level链表中出现的概率会随着level指数递减，具体可看下面的随即层数。 2. 基本结构 /** * 有序集合结构体 */ typedef struct zset { // Redis 会将跳跃表中所有的元素和分值组成key-value 的形式保存在字典中 dict *dict; // 底层指向的跳跃表的指针 zskiplist *zsl; } zset; /** * 跳跃表结构体 */ typedef struct zskiplist { // 头指针header和尾指针tail struct zskiplistNode *header, *tail; // 链表长度length，即链表包含的节点总数。注意，新创建的skiplist包含一个空的头指针，这个头指针不包含在length计数中。 unsigned long length; // 目前表内节点的最大层数 int level; } zskiplist; /** * 跳跃表中的数据节点 */ typedef struct zskiplistNode { // sds对象。用于保存字符串 sds ele; // 分数 double score; // 后退指针，节点只有1个后退指针，所以只有第1层链表是一个双向链表。 struct zskiplistNode *backward; // 存放指向各层链表后一个节点的指针（前进指针） struct zskiplistLevel { // 前进指针， struct zskiplistNode *forward; /** * 跨度实际上是用来计算元素排名(rank)的， * 在查找某个节点的过程中，将沿途访过的所有层的跨度累积起来， * 得到的结果就是目标节点在跳跃表中的排位 */ unsigned long span; } level[]; } zskiplistNode; 3. 常用操作 下面简单记录一下skiplist的插入、删除和更新操作。 3.1 插入 从顶层逐步降级寻找目标节点，得到“搜索路径” 获取一个随机值作为新节点的层数 如果新的层数 &gt; 当前最高层数，则初始化表头节点 创建新节点 重排前向指针 重排后退指针 zskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele) { // 存储搜索路径 zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; // 存储经过的节点跨度 unsigned int rank[ZSKIPLIST_MAXLEVEL]; int i, level; serverAssert(!isnan(score)); x = zsl-&gt;header; // 在各个层查找节点插入的位置，从头节点开始搜索，一层层向下搜索， 直到直到最后一层, update数组中保存着每层应该插入的位置，即“搜索路径” for (i = zsl-&gt;level-1; i &gt;= 0; i--) { // 如果i不是zsl-&gt;level-1层，即最高层，那么 i 层的起始 rank 值为 i+1 层的 rank 值 // 各个层的 rank 值一层层累积，最终 rank[0] 的值加一就是新节点的前置节点的排位 rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1]; // 沿着前进指针遍历，如果score相等，还需要比较value while (x-&gt;level[i].forword &amp;&amp; (x-&gt;level[i].forward-&gt;score &lt; score || (x-&gt;level[i].forward-&gt;score == score &amp;&amp; sdscmp(x-&gt;level[i].forward-&gt;ele, ele) &lt; 0) ) ) { // 记录沿途跨越了多少个节点，即记录每层距离头部位置的距离 rank[i] += x-&gt;level[i].span; // 移至下一个指针 x = x-&gt;level[i].forward; } // 记录将要和新节点相连接的节点 update[i] = x; } // 正式进入插入过程：zslInsert() 的调用者会确保同分值且同成员的元素不会出现，所以这里不需要进一步检查，可以直接创建新元素。 // 获取一个随机值作为新节点的层数 level = zslRandomLevel(); // 如果新节点的层数比当前最大层数要大，那么初始化新的层数头节点，并记录到update数组中，将来也指向新节点。 if (level &gt; zsl-&gt;level) { for (i = zsl-&gt;level-1; i &lt; level; i++) { rank[i] = 0; update[i] = zsl-&gt;header; update[i]-&gt;level[i].span = zsl-&gt;length; // 这里的span应该是插入的节点的rank值？ } // 更新表中节点的最大层数 zsl-&gt;level = level; } // 创建新的节点 x = zslCreateNode(level, score, ele); // 重排前向指针，将前面记录的指针指向新节点，并做相应的设置 for (i == 0; i &lt; level; i++) { // 设置新节点得 forward 指针 x-&gt;level[i].forward = update[i]-&gt;level[i].forward; // 将沿途记录得各个节点得 forward 指针指向新节点 update[i]-&gt;level[i].forward = x; // 计算新节点跨越的节点数量，即更新update数组中span值和新插入元素span值, rank[0]存储的是x元素距离头部的距离, rank[i]存储的是update[i]距离头部的距离 x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]); // 更新新节点插入之后，沿途节点的 span 值，其中的 +1 计算的是新节点 update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1; } // level可能小zsl-&gt;level, 无变动的元素span依次增加1 for (i = level; i &lt; zsl-&gt;level; i++) { update[i]-&gt;level[i].span++; } // 设置新节点后退指针 x-&gt;backward = update[0] == zsl-&gt;header ? NULL : update[0]; if (x-&gt;level[0].forward) x-&gt;level[0].backward = x; else z-&gt;tail = x; //下一个元素为空,则表示x为尾部元素 // 跳跃表的节点计数增一 zsl-&gt;length++; return x; } 3.2 删除 从顶层逐步降级寻找目标节点，得到“搜索路径” 更新元素的span值 对于每个层的节点重排一下前进指针和后退指针 判断是否需要更新层高 释放删除的节点 int zslDelete(zskiplist *zsl, double score, sds ele) { zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; int i; x = zsl-&gt;header; for (i = zsl-&gt;level; i &gt;=0; i++) { while (x-&gt;level[i].forward-&gt;score &lt; score || (x-&gt;level[i].forward-&gt;score == score &amp;&amp; compareStringObjects(x-&gt;level[i].forward-&gt;ele,ele) &lt; 0) ) \bx = x-&gt;level[i].forward; update[i] = x; } // 由于score值可能相等，因此需要精确匹配score和ele值 x = x-&gt;level[0].forward if (x &amp;&amp; score == x-&gt;score &amp;&amp; equalStringObjects(x-&gt;ele, ele)) { zslDeleteNode(zsl, x, update); // 正式删除节点，修改前后指针 zslFreeNode(x); // 释放节点 return 1; } // not found return 0; } void zslDeleteNode(zskiplist *zsl, zskiplistNode *x, zskiplistNode **update) { int i; // 删除元素需要更新update元素的span值以及重置前指针 for (i = 0; i &lt; zsl-&gt;level; i++) { if (update[i]-&gt;level[i].forward == x) { update[i]-&gt;level[i].span += x-&gt;level[i].span - 1; update[i]-&gt;level[i].forward = x-&gt;level[i].forward } else { update[i]-&gt;level[i].span -= 1; } } // 处理元素的后退指针 if (x-&gt;level[0].forwad) { // 非尾部元素则需要重置backforward指针 x-&gt;level[0].forward-&gt;backward = x-&gt;backward; } else { // 删除x可能是最后一个元素, 需要重置尾部指针 zsl-&gt;tail = x-&gt;backward; } // 删除元素位于最上层，并且仅有此一个元素，删除之后，需要降低跳跃表层数 while (zsl-&gt;level &gt; 1 &amp;&amp; zsl-&gt;header-&gt;level[zsl-&gt;level-1].forward == NULL) { zsl-&gt;level--; } zsl-&gt;length--; } 3.3 更新节点 当我们调用 zadd 方法时，如果对应的 value 不存在，那就是插入过程。 如果 value 已经存在了，只是调整了一下 score 的值，那就走更新流程。假设这个新的 score 值不会带来排序上的改变，那么就不需要调整位置，直接修改元素的 score 值即可。 但是如果排序位置改变了，那就要调整位置。 /* Remove and re-insert when score changes. */ if (score != curscore) { zskiplistNode *node; // 重新插入新节点 serverAssert(zslDelete(zs-&gt;zsl, curscore, ele, &amp;node)); znode = zslInsert(zs-&gt;zsl, score, node-&gt;ele); // 删除释放旧节点 node-&gt;ele = NULL; zslFreeNode(node); // hash-table对应的节点无需删除，只需要更新其对应的score值即可 dictGetVal(de) = &amp;znode-&gt;score; /* Update socre ptr */ *flags |= ZADD_UPDATED; } return 1; 因为我们可以看到 Redis 使用的策略就是先删除元素，再插入元素，需要经过两次路径搜索。 4. 注意点 除了以上知识点，还有几个细节需要了解的。 4.1 随机层数 对于每一个新插入的节点，都需要调用一个随机算法给它分配一个合理的层数，源码在t_zset.c/zslRandomLevel(void)中被定义： int zslRandomLevel(void){ int level = 1; while((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF)) level += 1; retrun (level &lt; ZSKIPLST_MAXLAVEL) ? level : ZSKIPLST_MAXLAVEL; } // 0xFFFF 65535 // ZSKIPLIST_P 默认为 0.25，即1/4 // define ZSKIPLST_MAXLAVEL 32 常量值为32 任何数和0xFFFF与运算其实就是暗含高位清零，低位与结果就是一个0和65535之间的数 (random()&amp;0xFFFF 得到 &lt;= 0xFFFF的随机数，这个随机数比ZSKIPLIST_P * 0xFFFF小的概率为ZSKIPLIST_P。 每一次while为true的概率都为ZSKIPLIST_P，换个角度想就是level n的概率为 ZSKIPLIST_P ^ (n-1)。 4.2 时间复杂度分析 首先来分析对于有** n个节点的链表，需要建立多少级索引。根据上面的例子可以得到，如果我们每两个节点会提取一个节点作为一个索引节点，那么第一级索引节点的个数为 n/2**，第二级索引节点的个数为** n/4**，依此类推，则第K级的索引节点的个数为** n/(2^k)**。 假设索引有** h级，且第 h级的索引节点个数为2，如下图所示。则我们可以得出 n/(2^h)=2**，这样可以得到** h=logn-1**（这里的 log是指以2为底）,加上链表本身的一层，则整个跳表的高度为** logn**。我们在跳表中查询某个数据时，如果每一层都需要遍历** m个节点，那么在跳表中查询某个数的时间复杂度为 O(m*logn)**。 4.3 空间复杂度分析 跳表查找效率的提高是通过建立多级索引实现的，而建立索引肯定需要消耗内存空间。对于跳表的内存空间分析并不难，对于有** n个节点的链表，第一级索引节点的个数为 n/2**，第二级索引节点的个数为 n/4，最后一级的索引节点的个数为2，我们可以得出一个等比数列：** n/2+n/4+n/8+...+4+2=n-2**,因此跳表的空间复杂度为** O(n)**。 顺着公式依次带入：a1=n/2，an= 2，q=1/2，求得Sn= n-2，所以空间复杂度为O(n)，与此同时，我们顺便考虑一下每三个节点抽取一个索引的情况，还是依据刚刚的思路，发现Sn= n-1/2，空间复杂度将近缩减了一半。 总之，跳表就是空间换时间的那个思路，但如果链表中存储的对象很大时，其实索引占用的这些空间对整个来说是可以忽略不计的。 4.4 score值一样怎么办 score值一样的情况下，还需要比较value值（字符串比较）。 4.5 rank如何计算 Redis在SkipList的forward指针上进行了优化，给每一个 forward指针都增加span属性，span 是“跨度”的意思，表示从前一个节点沿着当前层的 forward针跳到当前这个节点中间会跳过多少个节点。 Redis 在插入、删除操作时会小心翼翼地更新 span 值的大小。 struct zslforward { zslnode* item; long span; // 跨度 } struct zsl { string value; double score; zslforward*[] forwards; // 多层连接指针 zslnode* backward; // 回溯指针 } 当我们要计算一个元素的排名时，只需要讲“搜索路径”经过的所有节点的跨度span进行叠加就可以算出元素最终的rank值。 5. 为什么使用跳跃表，而不是平衡树等用来做有序元素的查找 红黑树在插入删除的时候可能需要做rebalance的操作，这样的操作可能会涉及到整个树的其他部分；而链表的操作就会相对局部，只需要关注插入删除的位置即可 有序集合经常会进行 zrange 或 zrevrange 这样的范围查找，跳表里的双向链表可以十分方便的进行这操作 实现简单，zrank 还能达到O(logn)的时间复杂度 ","link":"http://mofish.pily.life/post/data_structure_07/"},{"title":"数据结构学习之路(六)：QuickList快速列表","content":"在 Redis 3.2 版本之后，为了进一步提升 Redis 的性能，统一采用了quicklist来存储列表对象。 1. 简述 quicklist存储了一个双向列表，每个列表的节点是一个ziplist，所以实际上quicklist并不是一个新的数据结构，它就是linkedlist和ziplist的结合。 为什么这么设计呢？ 双向链表在插入节点上复杂度很低，但它的内存开销很大，每个节点的地址不连续，会加剧内存的碎片化，影响内存管理效率。 ziplist是存储在一段连续的内存上，存储效率高，但是它不利于修改操作，插入和删除的复杂度相对较高，而且需要频繁的申请释放内存，特别是ziplist数据较多的情况下，搬移内存数据太费时。 2. 基本结构 quicklist 是ziplist和linkedlist的混合体，它将linkedlist按段切分，每一段使用 ziplist来紧凑存储，多个ziplist之间使用双向指针串接起来。 2.1 quicklist结构定义 strcut quicklist { quicklistNode* head; // 指向头节点（左侧第一个节点）的指针 quicklistNode* tail; // 指向尾节点（右侧第一个节点）的指针 unsigned long count; // 所有ziplist中entry数据项的总和 unsigned int len; // quicklist中quicklistNode节点的数量 int fill; // 填充因子，由list-max-ziplist-size配置项控制，表示ziplist中entry能保存的数量，具体看参数要点 unsigned int compress; // 压缩深度，由list-compress-depth配置项控制，具体看参数要点 ...... } quicklist; 2.2 quicklistNode结构定义 strcut quicklistNode { struct quicklistNode* prev; // 前节点指针 struct quicklistNode* next; // 后节点指针 unsigned char *zl; // 数据指针。当前节点的数据没有压缩，那么它指向一个ziplist结构；否则，它指向一个quicklistLZF结构。 unsigned int sz; // ziplist实际占用内存大小。需要注意的是：如果ziplist被压缩了，那么这个sz的值仍然是压缩前的ziplist大小 unsigned int count; // ziplist里面包含的数据项个数 unsigned int encoding; // ziplist是否压缩。1--ziplist，2--quicklistLZF unsigned int container; // 预留字段，暂时没啥用，目前使用固定值2，表示使用ziplist作为数据存储容器 unsigned int recompress; // 当我们使用类似lindex这样的命令查看了某一项本来压缩的数据时，需要把数据暂时解压，这时就设置recompress=1做一个标记，等有机会再把数据重新压缩 ...... } quicklistNode; 2.3 ziplist结构定义 详情可看《数据结构学习之路(四)：ZipList压缩链表》 2.4 quicklistLZF结构定义 struct quicklistLZF { unsigned int sz; // LZF大小，压缩后ziplist大小 char compressed[]; // 柔性数组，存放压缩后的ziplist字节数组 } quicklistLZF; 2.5 详细结构图 3. 常用操作 3.1 插入 quicklist的插入方式大体可分为两种，一种是在头尾插入（quicklistPushHead和quicklistPushTail），一种是在中间任意位置插入（quicklistInsertAfter和quicklistInsertBefore）。 3.1.1 头尾节点插入 不管是在头部还是尾部插入数据，都包含两种情况： 如果头节点（尾节点）上ziplist的大小没有超过限制（即**_quicklistNodeAllowInsert返回1），那么新数据被直接插入到ziplist中（调用ziplistPush**）； 如果头节点（尾节点）上ziplist的的大小超过限制，那么会新创建一个quicklistNode节点（对应的也会创建一个ziplist），然后把这个新创建的节点插入到quicklist双向链表中。 3.1.2 中间节点插入 quicklistInsertAfter和quicklistInsertBefore就是分别在指定位置后面和前面插入数据项。这种在任意指定位置插入数据的操作，要比在头部和尾部的进行插入要复杂一些，包含下面四种情况： 当插入位置所在的ziplist大小没有超过限制时，直接插入到ziplist中就好了； 当插入位置所在的ziplist大小超过了限制，但插入的位置位于ziplist两端，并且相邻的quicklist链表节点的ziplist大小没有超过限制，那么就转而插入到相邻的那个quicklist链表节点的ziplist中； 当插入位置所在的ziplist大小超过了限制，但插入的位置位于ziplist两端，并且相邻的quicklist链表节点的ziplist大小也超过限制，这时需要新创建一个quicklist链表节点插入； 对于插入位置所在的ziplist大小超过了限制的其它情况（主要对应于在ziplist中间插入数据的情况），则需要把当前ziplist分裂为两个节点，然后再其中一个节点上插入数据。 3.2 查找 quicklist查找元素主要是针对index，即通过元素在链表中的下标查找对应元素。 基本思路是，首先找到index对应的数据所在的quicklistNode节点，之后调用ziplist的接口函数ziplistGet得到index对应的数据，源码中的处理函数为quicklistIndex。 3.3 删除 删除相对于插入而言简单多了，因为删除节点无需判断节点大小（quicklist有节点最大容量，但没有最小容量限制），只需要通过index找到对应的数据直接删除即可。 除此之外，用的比较多的区间元素删除的函数是quicklistDelRange quicklist 在区间删除时，会先找到start所在的 quicklistNode，计算删除的元素是否小于要删除的 count，如果不满足删除的个数，则会移动至下一个quicklistNode继续删除，依次循环直到删除完成为止。 quicklistDelRange 函数的返回值为 int 类型，当返回 1 时表示成功的删除了指定区间的元素，返回0时表示没有删除任何元素。 3.4 其它 除了以上之外，还有很多别的常用Api接口，如下： 操作 解析 quicklistCreate 创建 quicklist quicklistInsertAfter 在某个元素的后面添加数据 quicklistInsertBefore 在某个元素的前面添加数据 quicklistReplaceAtIndex 替换某个元素 quicklistDelEntry 删除单个元素 quicklistDelRange 删除区间元素 quicklistPushHead 头部插入元素 quicklistPushTail 尾部插入元素 4. 参数要点 4.1 list-max_ziplist-size（ziplist中的元素个数设定） list-max-ziplist-size -2 // 后面的参数可为正，可为负 正数 表示按照数据项个数来限定每个节点中元素的个数，比如3代表每个节点存放的元素个人不能超过3个。 负数 表示按照字节个数来限定每个节点的元素个数，它只能取**-1**~-5这五个数，其含义如下： -1：每个节点的ziplist字节大小不能超过4kb -2：每个节点的ziplist字节大小不能超过8kb -3：每个节点的ziplist字节大小不能超过16kb -4：每个节点的ziplist字节大小不能超过32kb -5：每个节点的ziplist字节大小不能超过64kb 4.2 list-compress-depth（压缩深度） 在quicklist的源码中提到了一个LZF的压缩算法，该算法用于对quicklist的节点进行压缩操作。list的设计目的是能够存放很长的数据列表，当列表很长时，必然会占用很高的内存空间，且list中最容易访问的是两端的数据，中间的数据访问率较低，于是就可以从这个出发点来进一步节省内存用于其他操作、 Redis提供了一下的配置参数，用于表示中间节点是否压缩： list-compress-depth 0 0 特殊值，表示不压缩 1 表示quicklist两端各有一个节点不压缩，中间的节点压缩 2 表示quicklist两端各有两个节点不压缩，中间的节点压缩 3 表示quicklist两端各有三个节点不压缩，中间的节点压缩 以此类推 5. 总结 quicklist同样采用了linkedlist的双端列表特性，然后quicklist中的每个节点又是一个ziplist，所以quicklist就是综合考虑了linkedlist容易产生碎片空间的问题和ziplist的读写性能两个维度而设计出来的一个数据结构。 ","link":"http://mofish.pily.life/post/data_structure_06/"},{"title":"数据结构学习之路(五)：Intset整数集合","content":"Intset是Redis特有的一种数据结构，当Set的元素较小且为数字类型时，就是使用该数据结构来实现的。 特点 元素类型只能是数字 元素只有三种类型：int16_6，int32_t，int64_t 元素有序且不可重复 intset和sds一样，是紧凑的数据结构，内存连续，就像数组一样 结构 struct intset&lt;T&gt; { int32 encoding; // 编码类型 int16_t、int32_t、int64_t int32 length; // 长度 最大长度:2^32 int&lt;T&gt; contents; // 柔性数组，可是是16位、32位、64位 } 升级 Intset中最值得一提的就是升级。 当Intset中添加的整数超过当前编码类型的范围时，Intset会自动升级到能容纳改证书类型的编码模式，然后再讲新元素添加到整数集合内。 其中，升级整数集合并添加新元素的步骤如下： 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性不变 将新元素添加到底层数组 总结 整数集合intset的底层实现为数组，该数组中的元素有序、无重复的存放，为了更好的节省内存，intset提供了升级操作，但是不支持降级操作。 为什么Set要使用Intset，不使用Ziplist？ 可能和运用场景有关，Set一般用于去重校验，并且是无序输出，例如可以用来做抽奖功能，那么每个参与者都可以设置一个整数 id 进行存储。 那么如果Set 类型在实际生产环境中，更多的使用在存储整数方面的话，就可以解释说是为了提高效率且节约存储空间。 ","link":"http://mofish.pily.life/post/data_structure_05/"},{"title":"计算机网络知识点(七)：计算机网络体系结构（已迁移）","content":"🤠 本章主要讲讲TCP/IP四层、五层协议、OSI七层协议体系结构 一、OSI七层模型 OSI七层协议模型主要是： 应用层 表示层 会话层 运输层 网络层 数据链路层 物理层 二、TCP/IP四层模型 TCP/IP是一个四层的体系结构，主要包括： 应用层 运输层 网际层 网络接口层 三、五层体系结构 五层体系结构包括： 应用层 运输层 网络层 数据链路层 物理层 各层的作用 ","link":"http://mofish.pily.life/post/network_learning_07/"},{"title":"计算机网络知识点(六)：Cookie、Session、Token和Jwt（已迁移）","content":"🤠 继续整理一些比较零散的知识点。 目录 1. Cookie 2. Session 3. Token 4. Jwt Cookie和Session 会话跟踪是Web程序中常用的技术，用来跟踪用户的整个会话。常用的会话跟踪技术是Cookie与Session。Cookie通过在客户端记录信息确定用户身份，Session通过在服务器端记录信息确定用户身份。 1. Cookie 1.1 工作原理 由于Http是一种无状态的协议，服务器单从网络连接上无从知道客户身份，因此需要给客户端们颁发一个通行证，每人一个，无论谁访问都必须携带自己的通行证。 这样，服务器就能从通行证上确认客户身份了，这就是Cookie的工作原理。 1.2 使用方式 Cookie实际上是一小段文本信息。客户端请求服务器时，如果服务器需要记录该用户状态，就会使用response向客户端浏览器颁发一个Cookie。 客户端浏览器会把Cookie保存起来，当浏览器再次请求该网站时，就会把Cookie连同请求的内容一并提交给服务器。 服务器接收到Cookie后，以此来辨认用户状态，而且服务器还可以根据实际情况修改Cookie内容。 1.3 特点 部分浏览器的Cookie的数量有限制 Cookie的长度不能超过4k，否则会截断，而且每次请求都带Cookie的时候，无形的浪费的带宽 Cookie存储在浏览器立案 不可跨域 实现简单 存储时间灵活 安全性低，可能会被截取篡改（后面会讲到XSS和CSRF） 可被用户设置禁用 1.4 有效期 Cookie的maxAge决定着Cookie的有效期，单位为秒（Second）。Cookie中通过getMaxAge()方法与setMaxAge(int maxAge)方法来读写maxAge属性。 maxAge &gt; 0，则表示该Cookie会在maxAge秒之后自动失效，浏览器会将maxAge为正数的Cookie持久化，即写到对应的Cookie文件中。无论客户关闭了浏览器还是电脑，只要还在maxAge秒之前，登录网站时该Cookie仍然有效。 maxAge = 0，则表示删除该Cookie。Cookie机制没有提供删除Cookie的方法，因此通过设置该Cookie即时失效实现删除Cookie的效果。失效的Cookie会被浏览器从Cookie文件或者内存中删除。 maxAge &lt; 0，Cookie默认的maxAge值为–1，为临时性Cookie，不会被持久化，不会被写到Cookie文件中。因此关闭浏览器后，该Cookie就会失效。 1.5 Cookie的Secure属性和HttpOnly属性 基于安全的考虑，需要给Cookie加上Secure和HttpOnly属性，HttpOnly比较好理解，设置HttpOnly=true的Cookie不能被js获取到，无法用document.Cookie打出Cookie的内容。 Secure属性是说如果一个Cookie被设置了Secure=true，那么这个Cookie只能用Https协议发送给服务器，用Http协议是不发送的。 2. Session 除了使用Cookie外，还可以使用Session来记录客户端状态。Session是服务器端使用的一种记录客户端状态的机制，使用上比Cookie简单一点，但是相应的也恩加了服务器的存储压力。 2.1 工作原理 Session是另外一种记录客户状态的机制，客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。客户端浏览器再次访问时只需要根据客户端传过来的SessionId从该Session中查找该客户的状态就可以了。 如果说Cookie机制是通过检查客户身上的“通行证”来确定客户身份的话，那么Session机制就是通过检查服务器上的“客户明细表”来确认客户身份。Session相当于程序在服务器上建立的一份客户档案，客户来访的时候只需要查询客户档案表就可以了。 2.2 使用方式 Session对象是在客户端第一次请求服务器的时候使用session_start()创建，接下来就可以根据业务逻辑把需要的数据存在Session中，那么客户端在当前会话期间就可以很方便的获取到这些数据了。 2.3 特点 可存储任意类型、任意大小的数据 用于存储一次会话的多次请求的数据，存在服务端 需要借助Cookie实现，如果浏览器禁用了Cookie可使用URL重写方案解决 2.4 有效期 可设置，默认超时时间是1440秒，超过默认时间就自动删除 如果关闭浏览器，或退出登录时，会重新生成会话，即新的sessionID，因为即使旧的为过期也会获取不到 Cookie与Session的对比 作用范围不同：Cookie 保存在客户端（浏览器），Session 保存在服务器端。 存储方式不用：Cookie只能存储简单的字段串，Session可以存储任意数据类型。 有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。 安全性不同：Cookie是以明文的形式存放在客户端，容易遭到不法获取，可加密后存放或使用Https；Session的话，虽然也依赖Cookie，但是因为数据放在服务器端，所以相对安全一点点。 存储大小不同： 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。 3. Token 3.1 什么是Token？ Token的意思是“令牌”，是服务端生成的一串字符串，作为客户端后续请求的一个标识。 当用户第一次登陆后，服务器生成一个Token并将此Token颁发给客户端，客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里或者 Local Storage 里，之后客户端请求时带上这个颁发Token， 服务器端解析验证这个Token，验证成功后才能有权限获取和处理相关数据。 3.2 Token的生成方式 简单Token的组成： uid(用户唯一的身份标识)、time(当前时间的时间戳)、sign（服务器端签名）token的前几位以哈希算法压缩成的一定长度的十六进制字符串。为防止token泄露）； Jwt：后面介绍 3.3 为什么要用Token（对比Cookie和Session） a. 减轻服务器端存储负担 对于Session来说，服务端需要保存每个用户的Session，增加服务器端的开销。 而Token则无需担心，像Jwt，可通过解析Token来获取基本信息。 b. 开发简单 对于Session，在分布式场景下要考虑Session共享的问题，在浏览器Cookie禁用的情况要要考虑 SessionId的传输问题，另一方面还需要考虑解决跨域资源共享的问题。 而Token的话，只需要给客户端颁发令牌，客户端把令牌存起来，每次访问向服务器端访问资源的时候带上这个令牌即可。 c. 安全性较好，无需考虑CSRF（跨站请求伪造） 因为不再依赖于Cookie，所以就不需要考虑对CSRF（跨站请求伪造）的防范。 CSRF攻击者之所以屡屡得手，是因为用户请求的验证信息都存在Cookie中。攻击者甚至不需要知道验证信息具体是什么直接就通过Cookie绕过了验证。 针对这点，我们只要不将验证信息存放在Cookie中即可。通常的做法是Http 请求中利用参数加入一个随机产生的 Token，并在服务器端建立一个拦截器来验证这个 Token，如果请求中没有Token或者Token内容不正确，则认为可能是CSRF攻击而拒绝该请求。 d. 无需考虑CORS（跨域资源共享） Cookie是不允许垮域访问的，或者说需要比较繁琐的方案去实现，但是这一点对Token机制是不存在的，前提是传输的用户认证信息通过Http头传输，而不是通过Cookie传输。 e. 无状态 Token机制在服务端不需要存储，因为Token 自身包含了所有登录用户的信息，只需要在客户端的Cookie或本地介质存储Token即可。 f. 适用接口跨平台 当你的客户端是一个原生平台（iOS, Android，Windows 8等）时，Cookie是不被支持的（你需要通过Cookie容器进行处理），这时采用Token认证机制就会简单得多。 总的来说：Token的验证机制比Session的验证机制更加灵活方便。 3.4 如何保证token安全性 Token授权机制 可把Token缓存起来，收到请求后进行Token校验，如果Token不存在则说明请求无效。 时间戳超时机制 Token的过期时间避免过长，检验到Token过期后说明请求无效。时间戳超时机制是防御DOS攻击的有效手段。 签名机制 将 Token 和 时间戳 加上其他请求参数再用MD5或SHA-1算法（可根据情况加点盐）加密，加密后的数据就是本次请求的签名sign，服务端接收到请求后以同样的算法得到签名，并跟当前的签名进行比对，如果不一样，说明参数被更改过，直接返回错误标识。签名机制保证了数据不会被篡改。 拒绝重复调用机制（非必须） 其实就是记录Token的使用情况，如果再次使用就说明Token无效，请求失败。拒绝重复调用机制确保Token被别人截获了也无法使用（如抓取数据）。 安全保障总结： 如果有人劫持了请求，并对请求中的参数进行了修改，签名就无法通过； 如果有人使用已经劫持的URL进行DOS攻击和爬取数据，那么他也只能最多使用30s； 如果签名算法都泄露了怎么办？可能性很小，因为这里的“盐”值只有我们自己知道。 3.5 如果解决Token失效的问题 方案一 服务端保存Token状态，用户每次访问时自动推迟Token的过期时间--与Session的过期策略类似，但是如果频发发起请求时，就需要频繁去刷新过期时间，这个代价也是挺大的。 方案二 使用Refresh Token，它可以避免频繁的读写操作。这种方案服务端不用每次都刷新Token的过期时间，只需要每次判断一下，如果过期了就告诉客户端，让客户端使用Refresh Token去重新申请Access Token即可。但是Refresh Token也过期了，就只能重新登录了。 4. Jwt Jwt即JSON Web Token，也是Token的一种，只不过对比于自己实现的Token，Jwt提供了一个很具体的标准，把Token用”.“点号分割为三段，分别为：头部header、信息载体playload、签名signature。 header 头部承担两部分信息： 声明类型 jwt 加密算法 HMAC SHA256等 { 'typ': 'JWT', 'alg': 'HS256' } 将头部进行base64加密，构成了第一部分 playload 信息载体就是存放有效信息的地方，包含三部分 标准中注册的声明 (建议但不强制使用) iss: jwt签发者 sub: jwt所面向的用户 aud: 接收jwt的一方 exp: jwt的过期时间，这个过期时间必须要大于签发时间 nbf: 定义在什么时间之前，该jwt都是不可用的. iat: jwt的签发时间 jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 公共的声明 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密。 私有的声明 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。 signature 签名部分是对前两部分(头部playload，信息载体playload)的签名，防止数据篡改。 按下列步骤生成： 1、先指定密钥(secret) 2、把头部(header)和载荷(payload)信息分别base64转换 3、使用头部(header)指定的算法加密 最终，signature = HS256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload),secret) ","link":"http://mofish.pily.life/post/network_learning_06/"},{"title":"计算机网络知识点(五)：URI、URL、URN区别（已迁移）","content":"🤠 这章节整理一些比较零散的知识点。 URI：统一资源标识符 URL：统一资源定位符 URN：统一资源名称 其实，URL和URN都是URI的子集。换而言之，URL和URN都是URI，但是URI不一定是URL或者URN。下面这张图概括了他们之间的关系： 下面通过一个简单的示例解释下URI、URL和URN的区别： 下面构造一个URI： https://mofish.pily.life/article/details.html#intro // 构造一个URI 开始解析 http:// // 是定义如何访问资源的方式 mofish.pily.life/article/details.html // 资源存放的位置 #intro // 资源 URL是URI的一个子集，告诉我们访问资源位置的方式。在例子中，URL应该如下所示： https://mofish.pily.life/article/details.html URN是URI的子集，包括名字（给定的命名空间内），但是不包括访问方式。在例子中，URN如下所示： mofish.pily.life/article/details.html#intro 遇事不决，URI就完事了！ ","link":"http://mofish.pily.life/post/network_learning_05/"},{"title":"计算机网络知识点(四)：Http和Https（已迁移）","content":"😵 同样是面试重灾区呀！！ 👻 Http的报文格式，各个版本的特性，Https的工作原理等等 目录 1. Http 1.1 Http的请求和响应报文格式 1.2 Http协议的特征 1.3 Http协议与TCP/IP协议的关系 1.4 Http短连接和长连接 1.5 Http1.0、1.1和2.0协议版本的区别和特性 1.6 Http2.0的服务端推送和WebSocket的区别（为什么Http2.0的服务端推送不能代替websocket？） 2. Https 2.1 Https工作原理 2.2 Https与三次握手 2.3 对称加密和非对称加密 2.4 三个随机数的作用 2.5 Http和Https的区别 1. Http Http，简称为“超文本传输协议”，是客户端和服务器端之间数据传输的格式规范。 1.1 Http的报文格式 Http的报文有请求报文和响应报文两种，下面分开来学习一下。 请求报文 Http的请求报文由请求行、请求头、空行和请求内容4个部分组成： 请求行： 请求方法：GET、POST、PUT、DELETE、PATCH等； URL：统一资源定位符URI、URL、URN的区别 协议版本：1.0、1.1、2.0等 请求头： 请求头部为请求报文添加一些附件信息，由”键值对“组成，每行一对，键值之间用冒号分隔，常见的请求头有： 请求头 说明 Host 接收请求的服务器地址，可以是IP:端口号，也可以是域名 User-Agent 发送请求的应用程序名称 Accpet-Language 通知服务端可以发送的语言 Accept-Encoding 通知服务端可以发送的数据压缩格式 Accept-Charset 通知服务端可以发送的编码格式 Cookie 发送Cookie信息 空行： 请求头的最后会有一个空行，表示请求头部结束，接下来为请求正文，这一行非常重要，必不可少！ 请求正文： GET是没有请求正文的，请求数据以地址的形式表现在请求行，但是对请求长度有限制； POST、PUT等部分请求方法才会有请求正文，请求参数封装在Http请求的body中，以名称/值的形式出现，对传送数据的大小没有限制。 响应报文 Http的响应报文由状态行、响应头、空行和响应内容4个部分组成： 状态行： 协议版本：http/1.1、http/1.0、h2 状态码：1xx、2xx、3xx、4xx、5xx 状态码描述：状态码对应的描述，如OK、Forbidden、Internal Server Error等 响应头： 与请求头部类似，为响应报文添加了一些附加信息 响应头 说明 Server 服务器应用程序软件的名称和版本，如nginx/1.19.0 Content-Type 响应正文的类型（是图片还是二进制字符串），如application/json Content-Length 响应正文长度 Content-Charset 响应正文使用的编码 Content-Encoding 响应正文使用的数据压缩格式 空行： 响应头的最后会有一个空行，表示响应头部结束，接下来为响应正文，这一行非常重要，必不可少！ 响应内容： 服务端返回的数据。 1.2 Http协议的特征 支持客户/服务器模式 简单快速，由于Http协议简单，使得Http服务器的程序规模小，因而通信速度很快 灵活，Http允许传输任意类型的数据对象。 无连接，每次请求一次，释放一次连接。 这里的无连接指的是短链接，表示每次连接只能处理一个请求，优点就是存在连接都是有用的连接，不需要额外的控制，实现简单，但是如果会在建立和关闭连接上浪费时间。 对应的也会有长链接，当Http协议头部中字段Connection：keep-alive表示支持长链接，为了解决频繁建立和关闭链接而出现，长链接可以省去较多的tcp建立/关闭操作，减少资源浪费、节省请求时间，对于频繁请求资源的客户，较适用于长链接 无状态，Http协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。 Http 是一个无状态协议，这意味着每个请求都是独立的，Keep-Alive 没能改变这个结果。 缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 于是，两种用于保持Http连接状态的技术就应运而生了，一个是Cookie，而另一个则是Session，后面再详讲。 1.3 Http协议与TCP/IP协议的关系 Http协议是应用层协议，在TCP之上，可以说是基于TCP连接的，主要解决如何包装数据。 而TCP协议是传输层协议，IP协议是网络层协议。 IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠地传递数据包，使得网络上接收端收到发送端所发出的所有包，并且顺序与发送顺序一致。TCP协议是可靠的、面向连接的。 1.4 Http短连接和长连接 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次Http操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个Http会话。 而从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 Ps：HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 1.5 Http1.0、1.1和2.0协议版本的区别和特性 Http1.0 简述：Http1.0是一种无连接、无状态的应用层协议，每个请求都会新创建一个tcp连接，完成后关闭，不跟踪不记录过去的请求。 (无状态的意思是其数据包的发送、传输和接收都是相互独立的。无连接的意思是指通信双方都不长久的维持对方的任何信息。) 缺点： 资源浪费，正因为频繁的创建/关闭连接，无法复用连接，造成了资源上很大的浪费； 队头阻塞，在一个请求接收到响应之后才会接着发送下一个,这也造成了head of line blocking(队头阻塞)。 解决方案：现在的浏览器为了解决这个问题,采用了一个页面可以建立多个tcp连接的方式来进行。 Http1.1 简述：继承了http1.0的特点,同时改善了http的一些问题 改进点： 持久连接 Http1.1增加了一个Connection字段，通过设置Keep-Alive可以保持Http连接不断开，避免了每次客户端与服务器请求都要重复建立释放建立TCP连接，提高了网络的利用率。如果客户端想关闭HTTP连接，可以在请求头中携带Connection: false来告知服务器关闭请求。 管道机制 即在同一个Tcp连接里面，客户端可以同时发送多个请求，但是服务端必须按照客户端请求的先后顺序依次回送响应的结果，以保证客户端能够区分出每次请求的响应内容。 增加Host字段 在Http1.1中增加Host请求头字段后，Web浏览器可以使用主机头名来明确表示要访问服务器上的哪个Web站点，这才实现了在一台Web服务器上可以在同一个IP地址和端口号上使用不同的主机名来创建多个虚拟Web站点。 断点续传 &lt;code&gt;RANGE:bytes&lt;/code&gt;是Http1.1新增内容，表示要求服务器从文件XXXX字节处开始传送，这就是我们平时所说的断点续传。 其它 Http1.1还提供了与身份认证、状态管理和Cache缓存等机制相关的请求头和响应头。 缺点： 虽然允许复用Tcp连接，但是同一个Tcp连接中，所有的数据通信都是按顺序来的，如果有一个堵住的话，后面就会有许多请求排队等着。这将导致“队头堵塞” 避免方式：减少请求数、同时多开持久连接 Http2.0 简述：相较于之前的版本，Http2.0在性能上带来了很大的提升。 改进点： 二进制分帧（多路复用的实现基础） Http2.0在应用层和传输层之间增加了一个二进制分帧层。 Http1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。Http2.0 则是一个彻底的二进制协议，头信息header和数据体body都是二进制，并且统称为”帧”：头信息帧（header frame）和数据帧（data frame）。 多路复用 所有的HTTP2.0通信都在一个TCP连接上完成，这个连接可以承载任意数量的双向数据流,每个数据流都以消息的方式进行发送,这个发送可以使乱序的,然后在通过每个帧头部的流标识符进行组装,同时每个数据流都可以设置优先级,可见http2.0真正实现了并行发送数据。 在过去， HTTP 性能优化的关键并不在于高带宽，而是低延迟。TCP 连接会随着时间进行自我调谐，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度。这种调谐则被称为 TCP 慢启动。由于这种原因，让原本就具有突发性和短时性的 HTTP 连接变的十分低效。HTTP/2 通过让所有数据流共用同一个连接，可以更有效地使用 TCP 连接，让高带宽也能真正的服务于 HTTP 的性能提升。 优点：这种单连接多资源的方式，减少服务端的链接压力,内存占用更少,连接吞吐量更大；而且由于 TCP 连接的减少而使网络拥塞状况得以改善，同时慢启动时间的减少,使拥塞和丢包恢复速度更快。 首部压缩 一方面，客户端与服务端约定头部数据的编码来讲头部进行压缩后发送，减少请求头容量； 另一方面，对于相同头部，为了避免重复发送，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，产生一个索引号，之后就不发送同样字段了，只需发送索引号即可。 服务端推送 Http2.0允许服务器未经请求，主动向客户端发送资源 1.6 Http2.0的服务端推送和WebSocket的区别（为什么Http2.0的服务端推送不能代替websocket？） Http协议和WebSocket协议都是应用层的协议，两者应用场景不一样。 Http2.0主要用来一问一答的方式交付信息，是对Html、Css等JS资源的传输方式进行了优化，并没有提供新的JS API，也不能用于实时传输消息，比如我在Http2.0中请求 a.html 服务端会自动把css和一些其他资源一并返回。 而WebSocket是基于Http1.1的协议，可以创建一条Tcp连接，具有双向传输等特性，而且具有JS API，客户端可使用如下来连接： new WebSocket(&quot;ws://hostname/chattingrom/&quot;) 因此，如果需要实时传输消息，目前还是需要使用WebSocket。 2. Https Https是基于安全套接字的Http协议,也可以理解为是Http+SSL/TLS(数字证书)的组合，比 HTTP 协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性，虽然不是绝对的安全，但是它大幅增加了中间人攻击的成本。 Https协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 2.1 Https工作原理 第一步：证书验证 客户端向服务器端发送Https请求，等待服务器确认； 服务器服务器向客户端传送 SSL 协议的版本号，加密算法的种类，随机数以及其他相关信息，同时服务器还将crt公钥以证书的形式发送给客户端（服务器端存放crt私钥和crt公钥）； 客户端检验证书是否合法，如果合法性验证没有通过，通讯将断开；如果合法性验证通过，将继续进行下一步。 合法性包括：证书是否过期，发行服务器证书的 CA 是否可靠，发行者证书的公钥能否正确解开服务器证书的 &quot;发行者的数字签名&quot;，服务器证书上的域名是否和服务器的实际域名相匹配。 第二步：获取对称密钥 客户端生成一个随机值(对称密钥 Pre-master secret)，然后使用服务端发送过来的公钥对其进行加密，并发送给服务端； 服务端使用crt私钥解密，取出对称密钥，即得到了客户端的私钥，之后客户端和服务端就可以用过这个私钥进行通信了。 第三步：传输加密数据 之后服务器就用解密出来的对称密钥对返回数据进行加密后返回给客户端，客户端也使用该对称密钥对内容进行解密； 发送请求时也是，客户端对请求内容进行加密，服务端同样使用密钥对该请求进行解密后再进行业务处理。 2.2 Https与三次握手 三次握手是属性传输层的概念，Https通常是SSL+Http的简称，目前使用的Https/Http协议都是基于Tcp协议之上的，因此也需要三次握手。 但是Tcp的三次握手建立连接之后，才会进行SSL握手的过程（即身份认证和密钥协商的过程）。 三次握手是确保建立连接，双方准备好通信 SSL的目的是加密通信的上次数据 2.3 对称加密和非对称加密 对称加密：加密和解密时使用的密钥是一样的，我们将其称为堆成加密。例如上面提到的对称密钥（客户钥） 非对称加密：加密的密钥 e 和解密的密钥 d 是不同的（即 e != d），并且加密的密钥 e 是公开的，叫做公钥，而解密的密钥 d 是保密的，叫私钥。例如上面提到的的 crt 公钥和 crt 私钥。 Https在内容传输的加密上使用的是对称加密，非对称加密只作用于证书验证阶段， 因为非对称加密的加解密效率非常低，而Http的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的。 2.4 三个随机数的作用 客户端发送Https请求的时候，会随便生成一个随机数A发送给服务端 服务端返回crt公钥的时候，也会生成一个随机数B，一并返回给客户端 客户端收到crt公钥后，会生成一个随机密钥C(pre-master secret)，然后发送给服务端 对于客户端： 当其生成了Pre-master secret之后，会结合原来的A、B随机数，用DH算法计算出一个master secret，紧接着根据这个master secret推导出hash secret和session secret。 对于服务端： 当其解密获得了Pre-master secret之后，会结合原来的A、B随机数，用DH算法计算出一个master secret，紧接着根据这个master secret推导出hash secret和session secret。 在客户端和服务端的master secret是依据三个随机数推导出来的，它是不会在网络上传输的，只有双方知道，不会有第三者知道。同时，客户端推导出来的session secret和hash secret与服务端也是完全一样的。 由于SSL协议中证书是静态的，因此十分有必要引入一种随机因素来保证协商出来的密钥的随机性。 对于RSA密钥交换算法来说，pre-master secret本身就是一个随机数，再加上hello消息中的随机，三个随机数通过一个密钥导出器最终导出一个对称密钥。 客户端和服务器加上pre-master secret三个随机数一同生成的密钥就不容易被猜出了，一个伪随机可能完全不随机，可是是三个伪随机就十分接近随机了，每增加一个自由度，随机性增加的可不是1。 2.5 Http和Https的区别 Http 以http:// 开头，而 Https 以 https:// 开头 Https 协议需要到 CA （Certificate Authority，证书颁发机构）申请证书 Http 标准端口是 80 ，而 Https 的标准端口是 443 Http 是超文本传输协议，信息是明文传输，Https 则是具有安全性的 SSL 加密传输协议。 在 OSI 网络模型中，Https 的加密是在传输层完成的,因为 SSL 是位于传输层的 知识点参考 https://blog.csdn.net/Regino/article/details/105509254 https://www.cnblogs.com/aidixie/p/11764181.html https://blog.csdn.net/qq_38289815/article/details/80969419 https://blog.csdn.net/qq_31442743/article/details/116199453 https://www.zhihu.com/question/32039008 ","link":"http://mofish.pily.life/post/network_learning_04/"},{"title":"计算机网络知识点(三)：TCP和UDP（已迁移）","content":"😵 面试重灾区呀！！ 👻 TCP三次握手和四次挥手、TCP、UDP协议的区别、TCP 协议如何保证可靠传输等等 目录 1. TCP 1.1 TCP的报文格式 1.2 TCP的三次握手流程 1.3 TCP的四次挥手流程 1.4 相关问题 问题1：为什么TCP链接需要三次握手，两次不可以么，为什么？ 问题2：三次握手过程中可以携带数据吗？ 问题3：什么是半连接队列？ 问题4：为什么断开连接是是四次挥手呢？ 问题5：四次挥手释放连接时，等待2MSL的意义? 问题6：TCP 协议如何保证可靠传输？ 问题7：TCP 的短连接和长链接是啥？ 2. UDP 2.1 UDP的报文格式 3. TCP和UDP对比 3.1 主要区别 3.2 为什么 TCP 叫数据流模式？ UDP 叫数据报模式？ 3.3 TCP和UDP分别对应的常见应用层协议 1. TCP（传输控制协议） TCP是一种面向连接的、可靠的、基于字节流的传输层通信协议，是专门为了在不可靠的网络中提供一个可靠的端对端字节流而设计的，面向字节流。 1.1 TCP的报文格式 源端口： 数据发送方的端口号 目标端口：数据接受方的端口号 序号（是TCP可靠传输的关键部分）：序号是本报文段发送的数据组的第一个字节的序号。在TCP传送的流中，每一个字节一个序号。一个报文段的序号为300，此报文段数据部分共有100字节，则下一个报文段的序号为400。所以序号确保了TCP传输的有序性。 确认号（是TCP可靠传输的关键部分）：即ACK，指明下一个期待收到的字节序号，表明该序号之前的所有数据已经正确无误的收到。确认号只有当ACK标志为1时才有效。比如建立连接时，SYN报文的ACK标志位为0。 数据偏移：表示本报文数据段距离报文段有多远 保留：保留以后用的 紧急比特URG：当值为1时表示次报文段中有需要紧急处理。 确认比特ACK：仅当ACK=1时确认号字段才有效。当ACK=0时，确认号无效。TCP规定，在连接建立后所有的传送的报文段都必须把ACK置1。 推送PSH：为1表示是带有push标志的数据，指示接收方在接收到该报文段以后，应尽快将这个报文段交给应用程序，而不是在缓冲区排队。 复位比特RST：值为1时表示TCP连接存在严重的错误，需要重新进行连接。 同步比特SYN：值为1表示这是一个连接请求或连接接受报文。 终止比特FIN：用于释放连接，为1时表示发送方已经没有数据发送了，即关闭本方数据流。 窗口字段：滑动窗口大小，用来告知发送端接收端的缓存大小，以此来控制发送端发送数据的速率，从而达到流量控制。 校验和：用来检验首部和数据两部分的正确性 紧急指针字段：只有当 URG 标志置 1 时紧急指针才有效，表示紧急数据最后一个字节的序号 1.2 TCP的三次握手流程 第一次握手：Client讲数据包的标志位SYN设为1，随机产生一个值seq=j，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。 第三次握手：Client收到确认后，检查ack是否为j+1，ACK是否为1，如果正确，则将标志位ACK设为1，ack为k+1，并将该数据包发送给Server。Server检查ack是否为k+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态。 完成三次握手后，Client和Server之间可以开始传输数据了。 1.3 TCP的四次挥手流程 第一次挥手：Client向Server端发送断开 TCP 连接请求的 [FIN，ACK] 报文，在报文中随机生成一个序列号 SEQ=x，表示要断开 TCP 连接。 第二次挥手：当Server收到Client发来的断开请求后，回复发送ACK报文，表示已经收到断开请求。回复时，由于回复的是客户端发来的请求，所以在客户端请求序列号seq=m的基础上加 1，得到ack=m+1。 第三次挥手：Server在回复完后，不会马上断开连接，而是在断开前确认传输到Client的数据是否已经全部传输完毕。确认全部数据传输完毕后，向客户端发送[FIN, ACK]报文，随机生成一个序列号seq=n。由于还是对客户端发来的 TCP 断开请求序列号seq=m进行回复，因此ack依然为m+1。 第四次挥手：Client收到Server发来的FIN后，就知道可以关闭连接了，因此再次发送ACK报文，告诉Server端可以关闭了。 完成四次挥手后，Client在等待2MSL后，依然没有收到回复，则证明Server端已经关闭了，那么Client端也可以关闭连接了。 1.4 相关问题 问题1：为什么TCP链接需要三次握手，两次不可以么，为什么？ 原因一：因为只有三次握手，才能确认双方的接收和发送功能都是正常的 第一次握手：Server端(Client发送正常，自己接收正常) , Client(我不知道啥情况) 第二次握手：Server端(Client发送正常，自己接收正常) , Client(自己发送和接收正常，Server发送和接收正常) 第三次握手：Server端(Client发送和接收正常，自己发送和接收正常) , Client(自己发送和接收正常，Server发送和接收正常) 原因二：防止已失效的链接请求报文突然又传送到了服务端 如果此时网络拥塞Client发出的连接请求迟迟到达不了Server端，于是便发起超时重发请求，其实上一个请求并未丢失，而上一个请求因为网络问题延误连接释放后的某个时间点才到达Server 这是Server误以为是Client发出的新的请求，于是发送确认包，同意连接， 如果此时采用两次握手的话，那么Server发送确认包后就建立连接，进入ESTABLISHED状态了，但是其实Client此时并未发出新的建立连接请求，并且已经是CLOSED状态了，所以对于Server的确认包不予理睬，而Server却一直在等待Client的请求，这样Server就白白浪费资源了。 问题2： 三次握手过程中可以携带数据吗？ 其实第三次握手的时候是可以携带数据的，但是其它的时候不行. 因为第一次握手的时候，根本不知道Server端的接收和发送能力，如果可以发送数据的话，很容易造成攻击者每次都在第一次握手时发送大量数据，不理睬Server端的接收和发送能力，导致Server端需要消耗很大的资源来处理这些报文 但是第三次握手的时候，由于Client端已经处于ESTABLISHED状态，并且确认了Server端有数据收发的能力了，所以携带数据也是可以的。 问题3：什么是半连接队列？ Server在第一次收到Client的SYN之后，就会处理SYN_RCVD状态，此时双方还没完全建立连接，服务器会把此种状态下的连接请求放在一个队列里，这个队列我们称之为半连接队列。 除此之外，还有一个全连接队列，用于存放已经完成三次握手的连接，但是如果队列满了的话，有可能出现丢包现象。 问题4：为什么断开连接是是四次挥手呢？ 简单的来说，四次挥手是为了断开连接时，确保数据能够完整传输。 第一次挥手时，Server收到FIN报文通知时，仅仅是明确了Client端没有数据再发送过来了； 第二次挥手时，如果Server只挥手一次的，不一定能够一次性携带完整的数据给到Client，所以此时只能回复一个ACK给对方，表示已经收到，并进入CLOSE_WAIT等待关闭状态，但是此时还不能关闭，因为后续可能还有数据传输给Client； 第三次挥手时，数据传输完毕了，发送FIN报文进入LAST_ACK状态，告诉Client可以关闭了，如果此时只有三次挥手的话就结束，Client不返回一个ACK给Server端的话，那么Server端就不能马上关闭，而是一直在等待Client的应答，浪费资源 第四次挥手时，Client发送ACK应答，告诉Server可以关闭了，然后自己进入TIME_WAIT状态，接下来就是等2MSL后，就进入CLOSE状态了。 问题5：四次挥手释放连接时，等待2MSL的意义（为什么需要TIME_WAIT）? 理论上，四个报文都发送完毕，就可以直接进入CLOSE状态了，但是可能网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 MSL是TCP报文的最大生命周期。 为了确保客户端发送的最后一个ACK报文可以到达服务器。就算ACK丢失了，服务器收不到，会以为发送的FIN客户端没有收到，会再发一次。而客户端可以再2MSL这个时间内收到这个重传的报文，再次给出回应报文； 为了防止之前已经失效的连接请求报文出现在新连接中。在客户端发送完最后一个ACK后，在2MSL的时间内，就可以保证在两个传输方向上的尚未接收到或者迟到的报文段从网络中消失，这样新的连接就不会出现旧连接的请求报文。 问题6：TCP 协议如何保证可靠传输？ 1.校验和： 发送的数据包的二进制相加，进位补1，然后取反，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段。 2.序列号+确认应答（累计确认+seq） 序列号：TCP传输时将每个字节的数据都进行了编号 确认应答：TCP传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送ACK报文。这个ACK报文当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。 序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。这也是TCP传输可靠性的保证之一。 3.超时重传 当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 4.连接管理 就是三次握手和四次挥手的过程。 5.流量控制 接收端在接收到数据后，对其进行处理。如果发送端的发送速度太快，导致接收端的接收缓冲区被填充满了。此时如果发送端仍旧发送数据，那么接下来发送的数据可能都会丢包，继而发生一系列丢包的连锁反应，例如超时重传什么的。 而TCP根据接收端对数据的处理能力，决定发送端的发送速度，这个机制就是流量控制。 下面只考虑A向B发送数据，假设在连接建立时，B告诉A它的接收窗口rwnd=400（receiver window） 如果接收到窗口大小的值为0，那么发送方停止发送数据，并定期向接收端发送窗口探测数据的，让接收端把窗口大小告诉发送端。 6.拥塞控制 在TCP传输过程中，如果网络出现堵塞或者一开始就发送大量的数据导致堵塞，那么就会产生大量的丢包，导致大量的超时重传，严重影响传输，因此当出现堵塞时，应当控制发送方的速率。 这一点和流量控制很像，但是出发点不同。流量控制时为了接收端能够来得及接收，而拥塞控制时为了降低整个网络的拥塞程度。 TCP 主要通过四个算法来进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 慢开始算法原理： 发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口。 为了了便于讨论，做如下假设： 接收方有足够大的接收缓存，因此不会发生流量控制； 虽然 TCP 的窗口基于字节，但是这里设窗口的大小单位为报文段。 当TCP连接进行初始化是，将拥塞窗口置为1（图中的窗口单位不再使用字节而使用报文段，并且慢开始门限的初始值设置为16个报文段，即ssthresh=16） 慢开始和拥塞避免 1.开始慢开始算法（指数增长），当cwnd=16时（图中点1）开始执行拥塞避免算法，呈线性增长； 2.当拥塞窗口cwnd=24时（图中点2）出现超时，发送方判定为网络拥塞，于是调整门限值ssthresh=cwnd/2=12，同时设置拥塞窗口为1，再次进入慢开始阶段； 3.重复步骤1，开始慢开始算法，发送方每收到一个ACK拥塞窗口值就增加，当cwnd=12时（图中点3），再次执行拥塞避免算法。 快重传和快恢复 4.当cwnd=16时（图中点4）出现了一个新的情况，就是发送方连续收到3个对统一报文段的重复确认（3-ACK）。发送方执行快重传和快恢复算法； 5.发送方知道只是丢失了个别的报文段，于是不启动慢开始，而是先进行快重传然后执行快恢复算法。发送方设置调整门限值ssthresh=cwnd/2=8, 同时拥塞窗口cwnd=ssthresh=8（点5），然后进行拥塞避免算法。 注意： 快重传：收到3个同样的确认就立刻重传，不等到超时； 快恢复：cwnd不是从1重新开始。 问题7：TCP 的短连接和长链接是啥？ TCP短连接 过程：建立连接——数据传输——关闭连接...建立连接——数据传输——关闭连接 简述：短连接一般只会在 client/server间传递一次请求操作，操作完成后就关闭连接 优点：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段 TCP长连接 建立连接——数据传输...（保持连接）...数据传输——关闭连接 简述：client与server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。目前主流的都是使用Http1.1协议，默认使用Tcp长连接。 优点：可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。 2. UDP（用户数据报协议） UDP（用户数据报协议）是iso参考模型中一种无连接的传输层协议，提供简单不可靠的非连接传输层服务，面向报文。 2.1 UDP的报文格式 源端口号和目的端口号如上和TCP的相同 UDP长度：UDP报文的字节长度（包括首部和数据） UDP校验和: 检验UDP首部和数据部分的正确性 3. TCP和UDP对比 协议 优点 缺点 应用场景 TCP（传输控制协议） 可靠，稳定TCP的可靠提现在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源。 慢，效率低，占用系统资源高。TCP在传递数据之前，要先建连接，会消耗时间，在数据传递时，确认机制、重传机制、拥塞控制机制都会消耗大量的时间，而且要在每台设备上维护所有的传输连接，事实上，每个连接都会占用系统的CPU、内存等硬件资源。 需要传输大量数据且对可靠性要求高的情况下使用TCP。 UDP（用户数据报协议） 快，UDP是一个无状态的传输协议，所以它在传递数据时非常快。 不可靠，不稳定 因为UDP没有TCP那些可靠的机制，如果网络质量不好，就和容易丢包 对实时性要求高和高速传输的场合下使用UDP;在可靠性要求低，追求效率的情况下使用UDP。 3.1 主要区别 TCP是面向连接的，UDP是无连接的； TCP提供可靠的服务，UDP尽最大努力交付。因此TCP传送的数据，无差错，不丢失，不重复，且按序到达；而UDP传送的数据是有小小偏差的； TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多的通信模式； TCP是面向字节流的，UDP是面向报文的； TCP的开销大于UDP，除了TCP(20字节)首部开销比UDP(8字节)大之外，TCP还有建立连接、握手挥手、流量控制、拥塞控制等； TCP数据传输慢，UDP数据传输快 3.2 为什么 TCP 叫数据流模式？ UDP 叫数据报模式？ 所谓**“流模式”**，是指TCP发送端发送几次数据和接收端接收几次数据是没有必然联系的，比如通过TCP连接发送数据，发送方只调用一次 write，发送了100个字节，但是对方可以分10次接收完，每次10个字节，反之亦然。 原因：这是因为TCP是面向连接的，一个socket中收到的数据都是由同一台主机发出的，且有序地到达，所以每次读取多少数据都可以。 所谓的**“数据宝模式”**，是指UDP发送端调用了几次write，接收端就必选用相同的次数read读完。UDP是基于报文的，在接收的时候，每次最多只能读取一个报文，报文和报文是不会合并的，如果缓冲区小于报文长度，则多出的部分会被丢弃。 原因：这个是因为UDP是无连接的，只要知道接收端的IP和端口，任何主机都可以向接收端发送数据。这时候如果一次能读取超过一个报文的数据，则会乱套。比如，主机A向发送了报文P1，主机B发送了报文P2，如果能够读取超过一个报文的数据，那么就会将P1和P2的数据合并在了一起，这样的数据是没有意义的。 3.3 TCP和UDP分别对应的常见应用层协议 运输层协议 应用层协议 应用 TCP SMTP（简单邮件传输协议） 电子邮件 HTTP（超文本传输协议） 万维网 TELENT（远程终端协议） 远程终端登录 FTP（文件传输协议） 文件传送 UDP DNS（域名系统） 名称转换 TFTP（简单文件传输协议） 文件传送 SNMP（简单网络管理协议） 网络管理 ","link":"http://mofish.pily.life/post/network_learning_03/"},{"title":"Redis学习之路(十八)：集群之cluster","content":"😎 在前面的文章中介绍过了redis的主从和哨兵两种集群方案，redis从3.0版本开始引入了redis-cluster(集群)。 👍 从主从-哨兵-集群可以看到redis的不断完善，主从复制解决了数据同步问题、哨兵可以同时管理多个主从同步方案同时也可以处理主从自动故障转移，但是单个节点的性能压力问题无法解决。而集群解决了前面两个方案的所有问题。 1. 为什么需要Redis Cluster？ 主从复制不能实现高可用 业务量增大，超过服务器所能承受范围，需要使用分流 内存容量和QPS无法满足业务需求 2. Redis Cluster是什么？ 什么是Redis Cluster呢？很简单，就是n个主从架构组合在一起对外服务，而且Redis Cluster至少需要3主3从才能组成一个集群。 除此之外，集群的主从跟单机的主从有一个重要的区别，就是虽然Redis Cluster中的每个master都挂载了一个slave节点，但是slave节点只从当一个数据备份的角色，读写请求其实都是在master上完成的。 当master宕机时，就会讲对应的slave节点提拨为master，来重新对外服务。 3. 节点负载均衡 这么多个master节点，实际存储的时候到底选择哪个节点呢？ 3.1 普通哈希算法 一般的负载均衡算法，会选择哈希算法。 首先对key使用哈希算法计算出一个哈希值，然后用哈希值对master数量进行取模。由此可以将key负载均衡到每一个redis节点上，这就是简单的哈希算法实现。 但是Redis Cluster并没有采取这种哈希算法 因为如果此时某一台master发生了宕机的话，就可能会导致Redis中所有的缓存都失效😱！！ 为什么是所有呢？因为之前有n个master，那么算法应该是hash % 3，但是当一台master宕机后，就变成hash % 2了，会影响到之前所有的key😵，这样的问题显然是致命的！ 3.2 一致性哈希 那么Redis Cluster是采取什么样的算法来实现节点的选择呢？答案是使用一种类似于一致性哈希的算法来实现的。 在了解类一致性哈希算法前，我们先看一下什么是一致性哈希。 上面提到哈希算法是对master实例数量取模的，而一致性哈希则是对2^32^取模，也就是值的范围是[0, 2^32-1^]。一致性哈希将其范围抽象成了一个圆环，使用CRC16算法计算出来的哈希值会落到圆环的某个地方。 然后Redis实例也分布在圆环上，最后在圆环上顺时针找到第一个Redis实例，这样就完成了对key节点的分配 假设我们有A、B、C三个Redis实例按照如图所示的位置分布在圆环上，此时计算出来的hash值，取模之后位置落在了位置D，那么我们按照顺时针的顺序，就能够找到我们这个key应该分配的Redis实例B。同理如果我们计算出来位置在E，那么对应选择的Redis的实例就是A。 即使这个时候Redis实例B挂了，也不会影响到实例A和C的缓存。 例如此时节点B挂了，那之前计算出来在位置D的key，此时会按照顺时针的顺序，找到节点C。相当于自动的把原来节点B的流量给转移到了节点C上去。而其他原本就在节点A和节点C的数据则完全不受影响。 这就是一致性哈希，能够在我们后续需要新增节点或者删除节点的时候，不影响其他节点的正常运行。 优点： 采用客户端分片方式：哈希 + 顺时针(优化取余) 节点伸缩时，只影响邻近节点，较好的容错性和可扩展性x 缺点： 节点分布不均衡造成数据倾斜问题，导致部分节点需要处理更多的key 3.3 虚拟槽分区（类一致性哈希） 但是上面的一致性哈希也是有小问题的，例如当我们的节点分布不均匀时，就容易导致数据倾斜了： 此时数据落在节点A上的概率明显是大于其他两个节点的，其次落在节点C上的概率最小。这样一来会导致整个集群的数据存储不平衡，AB节点压力较大，而C节点资源利用不充分。为了解决这个问题，一致性哈希算法引入了虚拟节点机制。 在圆环中，增加了对应节点的虚拟节点，然后完成了虚拟节点到真实节点的映射。假设现在计算得出了位置D，那么按照顺时针的顺序，我们找到的第一个节点就是C #1，最终数据实际还是会落在节点C上。 通过增加虚拟节点的方式，使ABC三个节点在圆环上的位置更加均匀，平均了落在每一个节点上的概率。这样一来就解决了上文提到的数据存储存在不均匀的问题了，这就是一致性哈希的虚拟节点机制。 除此之外，Redis Cluster使用的类一致性哈希还有一点不同的是，一致性哈希是对2^32^取模，而Redis Cluster则是对2^14^（也就是16384）取模。 Redis Cluster将自己分成了16384个Slot（槽位）。通过CRC16算法计算出来的哈希值会跟16384取模，取模之后得到的值就是对应的槽位，然后每个Redis节点都会负责处理一部分的槽位，就像下表这样。 节点 处理槽位 A 0 - 5000 B 5001 - 10000 C 10001 - 16383 每个Redis实例会自己维护一份slot - Redis节点的映射关系，假设你在节点A上设置了某个key，但是这个key通过CRC16计算出来的槽位是由节点B维护的，那么就会提示你需要去节点B上进行操作。 总结一下： 把16384槽按照节点数量进行平均分配，由节点进行管理 对每个key按照CRC16规则进行hash运算，把hash结果对16383进行取余 节点接收到数据，验证是否在自己管理的槽编号的范围 如果在自己管理的槽编号范围内，则把数据保存到数据槽中，然后返回执行结果 如果在自己管理的槽编号范围外，则会把数据发送给正确的节点，由正确的节点来把数据保存在对应的槽中 Redis Cluster的节点之间会共享消息，每个节点都会知道是哪个节点负责哪个范围内的数据槽 4. 怎么做到高可用 关于Redis Cluster是如何做到高可用的，主要有两个方面： master节点故障时如何处理？ 集群需要扩容节点时怎么办？ 4.1 扩容 我们之前说过，Redis Cluster是可以支持横向扩容的，当有新的节点加进来时，可通过reshard（重新分片）来给新的节点分配槽位。 reshard可以将已经分配给某个节点的任意数量的slot迁移给另一个节点，在Redis内部是由redis-trib负责执行的。你可以理解为Redis其实已经封装好了所有的命令，而redis-trib则负责向获取slot的节点和被转移slot的节点发送命令来最终实现reshard。 注意：这里迁移的slot一定是空闲的、与键值没有映射关系的slot，而且该分配也是均匀分配的，即每个节点的slot基本都是均匀的。 reshard完成之后，redis-trib会向集群中所有主节点发送槽位的变更信息，以便明确自己可接收的槽位数据。 4.2 高可用及故障转移 Redis Cluster中保证集群高可用的思路和实现和Redis Sentinel其实差不多，无论是主观下线、客观下线、投票选择、节点切换都是差不多的。 4.2.1 故障发现 主观下线：针对A节点，某一个节点认为A宕机了，那么此时是主观下线 客观下线：如果集群内超过半数的节点认为A挂了， 那么此时A就会被标记为客观下线。 4.2.2 故障转移 资格检查：一旦节点A被标记为了客观下线，集群就会开始执行故障转移。其余正常运行的master节点会进行投票选举，从A节点的slave节点中选举出一个，将其切换成新的master对外提供服务。当某个slave获得了超过半数的master节点投票，就成功当选。 替换主节点：当选成功之后，新的master会执行slaveof no one来让自己停止复制A节点，使自己成为master。然后将A节点所负责处理的slot，全部转移给自己，然后就会向集群发PONG消息来广播自己的最新状态。 5. gossip协议消息 这是Redis Cluster各个节点之间交换数据、通信所采用的一种协议，叫做gossip。 很简单，就像图里这样。每个Redis节点每秒钟都会向其他的节点发送PING，然后被PING的节点会回一个PONG。 gossip协议消息类型 Redis Cluster中，节点之间的消息类型有5种，分别是MEET、PING、PONG、FAIL和PUBLISH。 消息类型 消息内容 MEET 给某个节点发送MEET消息，请求接收消息的节点加入到集群中。 PING 每隔一秒钟，选择5个最久没有通信的节点，发送PING消息，检测对应的节点是否在线；同时还有一种策略是，如果某个节点的通信延迟大于cluster-node-time的值的一半，就会立即给该节点发送PING消息，避免数据交换过久。 PONG 当节点接收到MEET、PING消息之后，会回一个PONG消息给发送方，代表自己收到了该消息。同时节点也可以主动的通过PONG消息向集群中广播自己的信息，让其他节点获取到自己的最新属性，就像上面完成故障转移后，新的master向集群发送PONG消息一样。 FAIL 用于广播自己对某个节点的宕机判断，假设当前节点对A节点判断未宕机，就会立即向集群广播自己对A节点的判断，所有收到消息的节点就会对A节点做标记。 PUBLISH 用于向指定的Channel发送消息，某个节点收到PUBLISH消息之后会直接在集群内广播，这样一来，客户端无论连接到任何节点都能够订阅这个Channel。 使用gossip的优劣 优点： 扩展性：网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致。 容错性：由于每个节点都持有一份完整元数据，所以任何节点宕机都不会影响gossip的运行 健壮性：与容错性类似，由于所有节点都持有数据，地位平台，是一个去中心化的设计，任何节点都不会影响到服务的运行 最终一致性：当有新的信息需要传递时，消息可以快速的发送到所有的节点，让所有的节点都拥有最新的数据 缺点： 消息可能最终会经过很多轮才能到达目标节点，而这可能会带来较大的延迟 由于节点会随机选出5个最久没有通信的节点，这可能会造成某一个节点同时收到n个重复的消息 6.常见问题 6.1 为啥RedisCluster设计成16384个槽？ 如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大 redis的集群主节点数量基本不可能超过1000个 槽位越小，节点少的情况下，压缩率高 简而言之，它是消息大小和主机持有的平均slot数之间权衡的结果。 6.2 集群支持读写分离吗？ 集群模式下，从节点不接受任何读写请求 6.3 数据倾斜原因？ 节点和槽分配不均（如果使用redis-trib.rb工具构建集群，则出现这种情况的机会不多） 不同槽对应键值数量差异比较大（可能存在hash_tag或者包含bigkey） 内存相关配置不一致 请求倾斜：某个节点访问量大（某个节点有一个非常重要的key，就会存在热点问题） 集群倾斜：某个节点使用的内存量大（可能存在hash_tag或者包含bigkey） 7.总结 Redis Cluster数据分区规则采用虚拟槽方式(16384个槽)，每个节点负责一部分槽和相关数据，实现数据和请求的负载均衡。 搭建Redis Cluster划分四个步骤：准备节点，meet操作，分配槽，主从复制数据。 集群伸缩通过在节点之间移动槽和相关数据实现 3.1 扩容时根据槽迁移计划把槽从源节点迁移到新节点 3.2 收缩时如果下线的节点有负责的槽需要迁移到其他节点，再通过cluster forget命令让集群内所有节点忘记被下线节点 集群自动故障转移过程分为故障发现和节点恢复。节点下线分为主观下线和客观下线，当超过半数节点认为故障节点为主观下线时，标记这个节点为客观下线状态。从节点负责对客观下线的主节点触发故障恢复流程，保证集群的可用性。 ","link":"http://mofish.pily.life/post/redis_learning_18/"},{"title":"Redis学习之路(十七)：集群之哨兵模式","content":"👮 哨兵（sentinel）是一个分布式系统，用于对主从结构中的每台服务器进行监控，当出现故障时，通过投票机制选择新的master，并讲slave连接到新的master。 目录 1. 哨兵的作用 2. 启用哨兵模式 3. 哨兵工作原理 4. 实现哨兵模式 1. 哨兵的作用 监控 不断的检查master和slave是否正常运行 检测master存活、检测master与slave运行情况 通知（提醒） 当被监控的服务器出现问题时，向其它（哨兵、客户端）发送通知 自动故障转移 断开master与slave连接，选取一个slave作为master，将其它slave连接到新的master，并告知客户端新的服务器地址 注意 哨兵也是一台redis服务器，只是不提供数据服务 通常哨兵配置数量为单数，防止投票新的master时出现平票的情况 2. 启用哨兵模式 配置一拖二的主从结构 配置三个配置相同的哨兵，可参考sentinel.conf |配置项| 范例| 说明| |---|---|---|---| |sentinel auth-pass &lt;服务器名称&gt; | sentinel auth-pass mymaster itcast|连接服务器口令| |sentinel down-after-milliseconds &lt;自定义服务名称&gt;&lt;主机地址&gt;&lt;端口&gt;&lt;主从服务器总量&gt;| sentinel monitor mymaster 192.168.194.131 6381 1| 设置哨兵监听的主服务器信息，最后的参数决定了最终参与选举的服务器数量（-1）| |sentinel down-after-milliseconds &lt;服务名称&gt;&lt;毫秒数（整数）&gt;| sentinel down-after milliseconds mymaster 3000|指定哨兵在监控Redis服务时，判定服务器挂掉的时间周期，默认30秒 （30000），也是主从切换的启动条件之一| |sentinel parallel-syncs &lt;服务名称&gt;&lt;服务器数（整数）&gt;|sentinel parallel-syncs mymaster 1 |指定同时进行主从的slave数量，数值越大，要求网络资源越高，要求越小，同步时间约长| |sentinel failover-timeout &lt;服务名称&gt;&lt;毫秒数（整数）&gt; |sentinel failover-timeout mymaster 9000 |指定出现故障后，故障切换的最大超时时间，超过该值，认定切换失败，默认3分钟| |sentinel notification-script &lt;服务名称&gt;&lt;脚本路径&gt;| - |服务器无法正常联通时，设定的执行脚本，通常调试使用。| 启动哨兵：redis-sentinel sentinel.conf 服务类型 是否是主服务器 IP地址 端口 Redis 是 192.168.1.110 6379 Redis 否 192.168.1.111 6379 Redis 否 192.168.1.112 6379 Sentinel - 192.168.1.110 26379 Sentinel - 192.168.1.111 26379 Sentinel - 192.168.1.112 26379 3. 哨兵工作原理 哨兵在进行主从切换过程中，经历了三个阶段 监控 通知 故障转移 3.1 阶段一：监控阶段 监控阶段，哨兵有三个定时监控任务完成对各节点的发现和监控 任务1：每个哨兵节点每10秒会向主节点和从节点发送info命令，当有新的从节点加入时可以马上感知到。 哨兵配置时，只要配置主节点的监控即可，通过向主节点发送info命令，便可获取从节点信息。 任务2：每个哨兵节点每隔2秒向redis数据节点的指定频道上发送该哨兵节点对于主节点的判断以及当前哨兵节点的信息，同时每个哨兵节点也会订阅该频道 主要用来了解其它哨兵节点的信息以及对主节点的判断，其实就是通过消息publish和subscribe来完成的。 任务3：每隔1秒每个哨兵都会向主节点、从节点以及其余哨兵节点发送一次ping命令。 这就是心跳机制，哨兵用来判断节点是否正常的重要依据。 3.2 阶段二：通知 哨兵要做的就是定时监控这些数据和节点运行情况，每隔一定时间向这些节点发送PING命令来监控。间隔时间和down-after-milliseconds选项有关，down-after-milliseconds的值小于1秒时，哨兵会每隔down-after-milliseconds指定的时间发送一次PING命令，当down-after-milliseconds的值大于1秒时，哨兵会每隔1秒发送一次PING命令。 主观下线 当超过down-after-milliseconds指定时间后，如果被PING的节点仍然未恢复，则哨兵认为其主观下线，主观下线表示从当前的哨兵进程来看，该节点已经下线了。 客观下线 在主观下线后，如果该节点是主节点，则哨兵会进一步判断是否需要对其进行故障转移，哨兵发送sentinel is-master-down-by-addr命令询问其它哨兵节点，以便了解其它哨兵节点对该主节点的主观下线状态。 如果达到指定数量时，哨兵会认为其客观下线，并开始进行故障转移。 sentinel monitor mymaster 127.0.0.1 6379 2 // 该配置表示只有当至少有两个Sentinel节点(包括当前节点)认为该主数据库主观下线时，当前哨兵节点才会认为该主数据库客观下线。 3.3 阶段三：故障转移 选举领头哨兵 当前哨兵虽然发现了主节点客观下线了，需要故障恢复，但是故障恢复需要由领头哨兵来完成，这样才能保证同一时间是由一个哨兵来执行故障恢复，选举领头哨兵的过程使用了Raft算法，过程如下： 步骤 描述 1 发现主节点客观下线的哨兵节点(A)向每个哨兵节点发送命令，要求对象选择自己成为领头哨兵。 2 如果目标哨兵节点没有选过其它节点，则会将A设置为领头哨兵。 3 如果A发现有超过半数且超过quorum参数值的哨兵节点同样选择自己成为领头哨兵，则A成功成为领头哨兵。 4 当有多个哨兵节点同时参选领头哨兵，则会出现没有任何节点当选的可能，此时每个参选节点将等待一个随机事件重新发起参选请求进行下一轮选举，直到选举成功。 故障恢复 选出领头哨兵后，领头哨兵将会开始对主节点进行故障恢复，步骤如下： 步骤 描述 1 领头哨兵将从停止服务的主数据库的从数据库中挑选一个来充当新的主数据库。 2 选出一个从数据库后，领头哨兵将向从数据库发送SLAVEOF NO ONE命令使其升格为主数据库，而后领头哨兵向其他从数据库发送 SLAVEOF命令来使其成为新主数据库的从数据库，最后一步则是更新内部的记录，将已经停止服务的旧的主数据库更新为新的主数据库的从数据库，使得当其恢复服务时自动以从数据库的身份继续服务。 挑选新主节点的依据： 所有在线的从数据库中，选择优先级最高的从数据库。优先级通过replica-priority参数设置 优先级相同，则复制的命令偏移量越大(复制越完整)越优先，数据越完整 如果以上都一样，则选择运行ID较小的从数据库，数据越完整 ","link":"http://mofish.pily.life/post/redis_leatning_15/"},{"title":"Redis学习之路(十六)：集群之主从复制","content":"😡 为了避免单点Redis服务器故障，准备多台服务器，互相连通。将数据复制多个副本保存在不同的服 务器上，连接在一起，并保证数据是同步的。 😲 即使有其中一台服务器宕机，其它服务器依然可以继续提供服务，实现Redis的高可用，同时实现数据冗余备份。 目录 1. CAP原理 2. 主从复制简介 2.1 单机的风险 2.2 多台服务器连接方案 2.3 主从复制 3. 主从复制流程 3.1 阶段一：建立连接阶段 3.1.1 流程 3.1.2 连接方式 3.1.3 断开连接 3.1.4 配置参数 3.2 阶段二：数据同步阶段工作流程 3.2.1 工作流程 3.2.2 数据同步阶段master说明 3.2.3 数据同步阶段slave说明 3.3 阶段三：命令传播阶段 3.4 主从复制完成流程 4. 主从复制常见的问题 4.1 心跳机制 4.2 频繁的全量复制 4.3 频繁的网络中断 4.4 数据不一致 1. CAP原理 在了解Redis的主从复制之前，我们先来理解一下现代分布式系统的理论基石——CAP原理。 C：Consistent，一致性 A：Availability，可用性 P：Partition Tolerance，分区容忍性 分布式系统的节点往往都是分布在不通的机器进行网络隔离开的，这意味着必然有网络断开的风险，这个网络断开的场景的专业词汇叫做网络分区。 当发生网络分区时，两个分布式节点之间无法进行通信了，导致数据无法同步，从而一致性无法得到满足。除非我们牺牲可用性，也就是暂停分布式节点服务，在网络分区分生时，不再提交修改数据的功能，知道网络状况完全恢复正常再继续对外服务。 简单的用一句话概括CAP原理就是：当网络分区发生时，一致性和可用性两难全。 2. 主从复制简介 2.1 单机的风险 问题1：可靠性低，遇到宕机的话，会导致数据丢失、数据库压力骤增，对业务造成灾难性打击 问题2：内存容量瓶颈，没得横向扩展容量 2.2 多台服务器连接方案 提供数据方：master 主服务器、主节点、主库、主客户端 接收数据放：slave 从服务器、从节点、从库、从客户端 需要解决的问题：数据同步 核心工作：master数据复制到slave中 2.3 主从复制的作用 读写分离：提高服务器读写负载能力 负载均衡：基于主从结构，配合读写分离，由slave分担master的负载，并根据需求的变化，改变slave的数量，通过多个节点分担数据读取负载，大大提供redis服务器的并发量和数据吞吐量 故障恢复：当master出问题时，有slave提供服务，实现快速的故障恢复 数据冗余：实现数据热备份，时持久化之外的一种数据冗余方式 高可用基石：基于主从复制，构建哨兵模式或集群，实现Redis的高可用方案 3. 主从复制流程 主从复制过程大体可以分为3个阶段 建立连接阶段（即准备阶段） 数据同步阶段 命令传播阶段 3.1 阶段一：建立连接阶段 建立slave到master的连接，使得master能够识别slave，并保存slave端口号。 3.1.1 流程 步骤1： 设置master的地址和端口，保存master信息 步骤2：建立socket连接 步骤3：发送ping命令（定时器任务） 步骤4：身份验证 步骤5：发送slave端口信息 至此，主从连接成功！ 状态： salve：保存了master的地址和端口 master：保存slave端口 总体：master和slave之间创建了连接的socket 3.1.2 连接方式 方式一：客户端发送命令 slaveof 方式二：启动服务器参数 redis-server -slaveof 方式三：服务器配置 slaveof slave系统信息 master_link_down_since_seconds（记录与主节点连接失败的系统时间） master host masterport master系统信息 slave_listening_port（多个） 3.1.3 断开连接 客户端发送命令：slaveof no one slave断开连接后，不会删除已有数据，只是不再接受master发送的数据。 配置参数 master 配置 daemonize no ----&gt; daemonize yes(不影响当前会话,启动过程隐藏,守护进程) protected-mode yes ---&gt; protected-mode no(关闭保护模式,其他服务器可访问) slave 配置 daemonize no ----&gt; daemonize yes(不影响当前会话,启动过程隐藏,守护进程) protected-mode yes ---&gt; protected-mode no(关闭保护模式,其他服务器可访问) replicaof &lt;masterip&gt; &lt;masterport&gt;(主服务器的ip和端口) masterauth &lt;masterauth&gt;(主服务器的访问密码) 3.2 阶段二：数据同步阶段工作流程 在salve初次连接master后，复制master中的所有数据到slave，将slave的数据库状态更新成master当前的数据库状态。 3.2.1 工作流程 步骤1：请求同步数据 步骤2：创建RDB同步数据 步骤3：恢复RDB同步数据 步骤4：请求部分同步数据 步骤5：恢复部分同步数据 至此，数据同步工作完成！ 状态： salve：具有了master端的全部数据，包括RDB过程接收的数据 master：保存slave当前数据同步的位置 总体：master和slave之间完成了数据克隆 3.2.2 数据同步阶段master说明 如果master数据量巨大，数据同步阶段应避开流量高峰期，避免造成master阻塞，影响业务正常执行 复制缓存区大小设置不合理，会导致数据溢出，使用增量复制时发现数据存在丢失情况，又得进行第二次全量复制，使得slave陷入死循环状态 repl-backlog-size 1mb master单机内存占用主机内存比例不应过大，建议**50%-70%**左右，留下部分内存用于执行bgsave命令和创建复制缓存区 3.2.3 数据同步阶段slave说明 为避免slave进行全量复制、增量复制时服务器响应阻塞或不同步，建议关闭此期间的对外服务 slave-serve-stable-data yes|no 尽量避免多个slave同时对master请求数据同步，因为发送的RDB文件增多，可能会对带宽造成巨大冲击，因为需要适量错峰 3. slave过多时，建议调整拓扑结构，又一主多从结构改为树状结构，中间的节点既是master，也是slave。但是也要主要深度，因为层级越深，slave与顶层的master间的数据同步延迟越大。 3.3 阶段三：命令传播阶段 当master数据库状态被修改后，导致主从服务器数据库状态不一致，此时需要主从数据同步到一致的状态，同步的动作成为命令传播。 master将接收到的数据变更命令发送给slave，slave接收命令后执行命令。 命令传播阶段的三个核心要素：服务器的运行id（run id）、复制积压缓冲区、复制偏移量 3.3.1 服务器的运行id（run id） 概念 服务器运行ID是每台服务器每次运行的身份识别码，一台服务器多次运行可生成多个运行ID 组成 由40位字符组成，是一个随机的十六进制字符 作用 被用于在服务器之间进行传输时识别身份 实现方式 运行ID在启动时自动生成，master在首次连接slave时，会把自己的运行ID发送给slave，salve保存此ID。可通过info Server命令查看节点运行ID。 3.3.2 复制积压缓冲区 概念 复制积压缓冲区，是一个先进先出（FIFO）的队列，用于存储服务器执行过的命令，每次传播命令，master都会将传播的命令记录下来，并存储在复制缓冲区。 由来 服务器启动时，如果开启由AOF或被连接成为master节点，即创建复制缓冲区。 作用 用于保存master收到的所有指令（仅影响数据变更的指令，如set、select）。 数据来源 当master接收到主客户端的指令时，除了将指令执行，会将该指令存储到缓冲区中。 注意事项 复制缓冲区默认存储空间大小为1M，由于存储空间大小是固定的，当空间满了之后，最先入队的元素会被弹出，以便新的元素入队。 3.3.3 复制偏移量 概念 一个数字，描述复制缓冲区中指令字节的位置 分类 master复制偏移量：记录发送给所有slave的指令字节对应的位置（多个） slave复制偏移量：记录slave接收master发送过来的指令字节对应的位置（一个） 数据来源 master端：发送一次记录一次 slave端：接收一次记录一次 作用 同步信息，对比master和slave的差异，slave断开后，恢复数据使用 3.4 主从复制完成流程 4. 主从复制常见的问题 4.1 心跳机制（重点） 进入命令传播阶段后，master与slave间需要进行信息交换，使用心跳机制来进行维护，实现双方连接保持在线。 master心跳： 指令：PING 周期：由 repl-ping-replica-period决定，默认10秒 作用：判断slave是否在线 查询：INFO relication 获取slave最后一次连接时间间隔，lag项维持在0或1是为正常 slave心跳： 指令：REPLCONF ACK {offset} 周期：1秒 作用1：汇报slave自己的复制的偏移量，获取最新的数据变更指令 作用2：判断master是否在线 注意事项： 当slave多数掉线，或者延迟过高时，master为保障数据稳定性，将拒绝写请求，防止发生脑裂问题 min-replicas-to-write 3 // 表示连接到master的最少slave数量 min-replicas-max-lag 10 // 表示连接到master的最少slave数量 slave数量和延迟由slave发送REPLCONF ACK命令做确认 4.2 频繁的全量复制 频繁的全量复制（1） 伴随着系统的运行，master的数据量越来越大，一旦master重启，run id将发生变化，会导致全部slave的全量复制操作。 内部优化调整方案： master内部创建master_replid变量，使用run id相同的策略生成，长度为41位，并发送给所有slave 在master关闭时执行shutdown save，进行RDB持久化，将run id与offset保存到RDB文件中 repl-id repl-offset 通过redis-check-rdb命令可以查看该信息 master重启后加载RDB文件，将RDB文件保存的relp-id和repl-offset加载到内存中，恢复数据 master_repl_id = repl master_repl_offset = repl-offset 通过info命令可以查看该信息 作用：本机保存上次run id，重启后恢复该值，使所有slave认为还是之前的master。 频繁的全量复制（2） 问题现象：网络环境不佳，出现网络中断，salve不提供服务 问题原因：复制缓冲区过小，断网后slave的offset越界，出发全量复制 最终结果：slave反复执行全量复制 解决方案：修改复制缓冲区大小：repl-backlog-size {size} 4.3 频繁的网络中断 频繁的网络中断（1） 问题现象：master的CPU占用过高或slave频繁断开连接 问题原因： slave每1秒发送REPLCONF ACK命令到master 当slave接到了慢查询时，会大量占用CPU性能 master每1秒调用复制定时函数，比对slave发现长时间没有进行响应 最终结果：master各种资源（输出缓冲区、带宽、连接等）被严重占用 解决方案：通过设置合理的超时时间，确认是否释放slave：repl-timeout。 频繁的网络中断（2） 问题现象：salve与master连接断开 问题原因： master发送ping指令频率较低 master设定超时时间较短 ping指令在网络中存在丢包 解决方案：提高ping指令发送的频率：repl-ping-slave-period（超时时间repl-time的时间至少是ping指令频度的5到10倍，否则slave很容易判定超时） 4.4 数据不一致 问题现象：多个slave获取相同数据不同步 问题原因：网络信息不同步，数据发送有延迟 解决方案： 优化主从间的网络环境，尽量部署在同一机房 监控主从节点延迟（通过offset）判断，如果slave延迟过大，在那时屏蔽程序对该salve的数据访问：slave-serve-stale-data yes|no （慎用） ","link":"http://mofish.pily.life/post/redis_learning_16/"},{"title":"计算机网络知识点(二)：常见的HTTP状态码（已迁移）","content":"🤗 稍微记录一下 1xx：Informational（信息性状态码） 接受的请求正在处理 100（Continue）：继续。客户端应继续其请求。 101（Switching Protocols）：切换协议。服务器根据客户端的请求切换协议。 2xx：Success（成功状态码）请求正常处理完毕 200（OK）：请求成功。 201（Created）：已创建。成功请求并创建了新的资源。 202（Accepted）：已接受，但未处理完成。 204（No Content）：请求成功，但没有返回任何内容。 205（Reset Content）：请求成功，用户终端（如浏览器）应该重置内容。 206（Partial Content）：成功处理了部分 GET 请求。 3xx：Redirection（重定向状态码）需要进行附加操作以完成请求 300（Multiple Choices）：多种选择，服务器可执行多种操作，服务器可根据请求者(user agent)选择一项操作，或提供操作列表共请求者选择。 301（Moved Permanently）：永久重定向，请求的资源已永久移动到新位置，服务器返回此响应时，会自动将请求转到新的位置。 302（Found）：临时重定向，请求的资源只是临时被移动，客户端应继续使用原有的位置来进行请求。但是如果请求方法不是 GET 的话，请求方法和消息主体会发生变化，因为一些旧客户端会错误地将请求方法转换为 GET。 303（See Other）：HTTP1.1新增的状态，是302的细化，303和302状态码有着相同的功能，但是303明确表示客户端应当采用get方法获取资源，比如，当使用post方法访问程序，其执行后的处理结果为希望客户端能以get方法重定向到另一个uri上去时。 307（Temporary Redirect）：临时重定向，HTTP1.1新增的状态，是302的细化，唯一区别是可确保请求方法和消息主体不会发生变化，但是对于GET请求来说，两者没有啥区别。 4xx：Client Error（客户端错误状态码）服务器无法处理请求 400（Bad Request）：客户端请求的语法错误，服务器无法理解。 401（Unauthorized）：请求要求用户的身份认证。 403（Forbidden）：服务器拒绝请求。 404（Not Found）：服务器找不到请求的网页。 405（Method Not Allowed）：禁用请求中指定的方法。 408（Request Time-out）：服务器等候请求时发生超时。 410（Gone）：客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码。 411（ Length Required）：服务器无法处理客户端发送的不带Content-Length的请求信息 5xx：Server Error（服务器错误状态码）服务器处理请求出错 500（Internal Server Error）： 服务器内部错误，无法完成请求。 501（Not Implemented）：服务器不具备完成请求的功能。例如，服务器无法识别请求方法时可能会返回此代码。 502（Bad Gateway）：服务器作为网关或代理，从上游服务器收到无效响应。 503（Service Unavailable）：服务器目前无法使用(由于超载或停机维护)。 504（Gateway Time-out）：充当网关或代理的服务器，未及时从远端服务器获取请求。 505（HTTP Version not supported）：服务器不支持请求中所用的 HTTP 协议版本。 ","link":"http://mofish.pily.life/post/network_learning_02/"},{"title":"Redis学习之路(十五)：Redis分布式锁","content":"✍️ 分布式应用进行逻辑处理时经常会遇到并发问题，为了防止该问题发生就要使用到分布式锁来限制程序的并发执行。 👌 而Redis分布式锁使用得非常广泛，也是面试需要考察得地方。 目录 一、基础分布式锁的演进 二、超时问题 三、可重入性 四、RedLock 基础分布式锁的演进 分布式锁本质要实现的目标就是在门口加一把锁，当别的进程也想要进入的时候，发现已经被锁住了，就只好放弃或者稍后再试。 加锁一般使用setnx(set if not exists) 指令，一次只允许一个客户端加锁，先来先加锁。待处理完毕后再调用** del 指令**来释放锁。 &gt; setnx lock:codehole true // 加锁 &gt; OK // 加锁成功 ... 开始处理业务逻辑 ... ...... ... 处理完毕 ... &gt; del lock:codehole // 解锁 &gt; (integer) 1 // 解锁成功 但是有个问题，如果处理业务过程中出现异常导致程序退出，那么del指令可能没有被调用的话，这样就会陷入死锁，锁永远得不到释放。 解决的方法就是给这个锁加上一个合适的过期时间，这样即使中间出现了异常，那么也能保证过期时间到了之后锁能自动释放 &gt; setnx lock:codehole true // 加锁 &gt; OK // 加锁成功 &gt; expire lock:codehole 3 // 设置3秒后自动解锁 ... 开始处理业务逻辑 ... ...... ... 处理完毕 ... &gt; del lock:codehole // 解锁 &gt; (integer) 1 // 解锁成功 但是以上的逻辑还是有问题。如果在setnx和expire之间，服务器进程突然挂了，就会导致expire得不到执行，从而造成死锁。这种问题的根本在于setnx和expire是两条指令，整个操作不具备原子性的。 为了解决这个问题，官方在 Redis2.8 版本中，给set指令加入了扩展参数，使得setnx和expire指令可以一起执行。 &gt; set lock:codehole true ex 3 nx (ex 3 设置过期时间为3秒，nx 只有键key不存在时才会设置key的值) ... 开始处理业务逻辑 ... ...... ... 处理完毕 ... &gt; del lock:codehole // 解锁 &gt; (integer) 1 // 解锁成功 超时问题 Redis 的分布式锁不能解决超时问题，如果在加锁和释放锁之间的逻辑处理执行的时间过长，以至于超出锁的时间限制。就会出现第一个线程持有的锁过期了，但是业务逻辑还没执行完，锁却自动释放了，从而导致第二个线程就提前重新持有了这把锁，最终的结果就是该逻辑处理的代码不能严格的串行执行。 为了避免这个问题，Redis分布式锁不建议用于较长时间的任务。 有个稍微安全一点的方法时将set指令的value参数设置为一个唯一的随机编号，释放锁时先判断这个编号和加锁时的是否一致，如果一致再删除key，这个为了确保当前线程占有的锁不被其它线程释放，除非这个锁是因为过期了而被自动释放的。 &lt;?php /** * 简单代码示例 */ class RedisMuteLock { private $redis = null; public function __construct() { $this-&gt;redis = new Redis(); $this-&gt;redis-&gt;connect('127.0.0.1', 6379); } public function lock($key, $value, $lockSecond = 20): bool { $set = $this-&gt;redis-&gt;set(&quot;Lock:{$key}&quot;, $value, ['NX', 'EX' =&gt; $lockSecond]); return (bool)$set; } public function unlock($key, $value) { $script = ' if redis.call(&quot;GET&quot;, KEYS[1]) == ARGV[1] then return redis.call(&quot;DEL&quot;, KEYS[1]) else return false end '; return $this-&gt;redis-&gt;eval($script, [&quot;Lock:{$key}&quot;, $value], 1); } } $redis = new RedisMuteLock(); $value = strtoupper(md5(uniqid(rand(), true ))); if ($redis-&gt;lock('Order', $value)) { echo '开始业务处理' . PHP_EOL; sleep(5); $del = $redis-&gt;unlock('Order', $value); echo '处理完毕，解锁成功' . PHP_EOL; }else { echo '访问量大，请稍后重试' . PHP_EOL; } 但是这种方案还有一个原子性的问题，就是匹配value和删除key不是一个原子操作，所以还是有可能出现匹配到但是没删除到的可能，这就可能需要用到Lua脚本来进行处理了，因为Lua脚本可以保证连续多个指令的原子性执行。 if redis.call(&quot;get&quot;, KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;, KEYS[1]) else return 0 end 但是这也不是一个完美的方案它只是相对安全一点，因为如果真的超时了，当前线程的逻辑没有执行完，其他线程也会乘虚而入。 可重入性 可重入锁指的是可重复可递归调用的锁，在外层使用锁之后，在内层仍然可以使用，如果没有可重入锁的支持，在第二次尝试获得锁时将会进入死锁状态。 RedLock 上面提到的一般都是redis单机方案的，如果是使用redis集群的话，可能就不适用了。 分布式锁的集群问题 如下图，在Sentinel集群中，当主节点挂掉时，从节点会取而代之，这个时候问题就来了，如果客户一在主节点中申请了一把锁，但是这把锁还没来得及同步到从节点，主节点突然挂掉了，然后从节点变成了主节点后内存没有这个锁，就导致了客户二能够成功的申请加锁，这就导致了系统中同样一把锁被两个客户同时持有。 不过这种不安全的情况也仅在主从发生failover下才会产生，而且持续时间极短，业务系统多数情况下可以容忍。 Redlock算法 为了使用 Redlock 需要提供多个 Redis 实例，这些实例之前相互独立，没有主从关系。同很多分布式算法一样， Redlock 也使用“大多数机制”。 · 加锁时，它会向过半节点发送 set(key, value, nx=True, ex吨以）指令，只要过半节点 set 成功，就认为加锁成功。释放锁时，需要向所有节点发送 del 指令。不过Redlock 算法还需要考虑出错重试、时钟漂移等很多细节问题，同时因为 Redlock 要向多个节点进行读写，意昧着其相比单实例 Redis 性能会下降一些。 https://blog.csdn.net/dabao87/article/details/113773942 Redlock使用场景 如果很在乎高可用，希望即使挂了一台Redis也完全不受影响，那么就可以考虑使用Redlock。 不过代价就是需要更多的Redis实例，性能上会有所下降，代码复杂度会提供，运维也需要特殊的对待。 ","link":"http://mofish.pily.life/post/redis_learning_15/"},{"title":"Redis学习之路(十四)：缓存穿透、击穿、雪崩","content":"👉 使用缓存的主要目是提升查询速度和保护数据库等稀缺资源不被占满导致整个服务异常 🖖 而缓存最常见的问题是缓存穿透、击穿和雪崩，在高并发下这三种情况都会有大量请求落到数据库，导致数据库资源占满，引起数据库故障。 缓存穿透 概念：访问一个不存在的key，缓存起不到作用，请求会穿透到DB，如果这些key的数量比较大时，DB可能会挂掉，或者连DB都不存在这个数据时，一直被穿透访问。 解决方案： 采用布隆过滤器，用于存储可能访问的key，过滤掉非法的key 访问key未在DB查询到值，也将空值写进缓存，但是可以设置较短的过期时间 接口层增加参数校验、用户校验、权限校验，过滤掉不合法的请求 缓存雪崩 概念：大量的key设置了相同的过期时间，导致在缓存的同一时刻全部失效，造成瞬间DB请求量大、压力骤增，引起雪崩 解决方法： 可以给缓存设置过期时间时加上一个随机值时间，使得每个key的过期时间分布开来，不会集中在同一时刻失效。 缓存击穿 概念：跟缓存雪崩有点类似，但是缓存击穿是针对某个非常热点的key的，这个key抗住了大并发，但是当这个key失效的瞬间，持续的大并发都跑到数据库去请求了，瞬间击穿，从而造成数据库崩掉 解决方案： 设置热点数据永不过期 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。 ","link":"http://mofish.pily.life/post/redis_learning_14/"},{"title":"Redis学习之路(十三)：redis和mysql数据一致性","content":"😵 你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题。 🤔 那么你如何解决一致性问题？ 一、谈谈一致性 强一致性：这种一致性级别最符合用户直觉，它要求系统写入什么，读出来的也会是什么，用户体验好，但是实现起来往往对系统的性能影响大。 弱一致性：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能达到一致性，但会尽可能的保证在某个时间级别后，数据能够达到一致性。 最终一致性：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能能够达到一个数据一致性的状态，单独提出来是因为它是数据一致性比较推崇的模型。 二、经典的缓存模式 为了减轻数据库压力，提升系统性能，我们一般会选择使用缓存，但是使用缓存也会导致数据不一致的问题，最经典的缓存+数据库读写的模式就是Cache-Aside Pattern。 Cache-Aside Pattern，即旁路缓存模式，它的提出是为了尽可能地解决缓存与数据库的数据不一致问题，也是最经典的缓存+数据库读写的模式。 读：读的时候，先读缓存，命中则直接返回，没有命中的时候就读数据库，然后写入缓存，同时返回数据 写：更新的时候，先更新数据库，再删除缓存 问题1：为什么该模式是删除缓存，而不是更新缓存？ 原因很简单，很多时候复杂的缓存场景不单单是直接从数据库取出来的数据，有可能是经过一系列的搜索、计算得出来的值， 而删除不是更新，其实是一个懒加载思想，因为缓存里面其实有很多是冷数据，所以我只需要再你需要用到缓存的时候再去计算写入缓存即可。 除此之外，还可以避免如果数据更新频率高（即写场景多，读场景少的情况），反复写入更新缓存的性能浪费的情况。 问题2：为什么写操作时是先操作数据库呢？为什么不先操作缓存呢？ 假设有A、B两个请求，请求A做更新操作，请求B做查询读取操作。 线程A发起一个写操作，第一步如果先操作缓存，把缓存删除掉 此时线程B同时发起了一个读操作，缓存已被删除，所以获取不到 线程B继续读数据库数据，读出来一个旧数据 线程B把旧数据写入缓存中 线程A写入DB最新的数据 这样子就会导致数据不一致了，缓存里面存的是旧数据，但是数据库已经更新了新的数据了，所以在写操作时，需要先操作数据库，而不是先操作缓存。 三、一致性方案 3.1 先更新数据库，再更新缓存（不建议使用） 这方案大家普遍反对的原因如下： （线程安全角度）同时有请求A和请求B进行更新操作 线程A更新了数据库 线程B更新了数据库 线程B更新了缓存 线程A更新了缓存 这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了数据不一致，因此不考虑。 业务场景角度 如果写场景比较多，那么数据压根还没读到，缓存就被频繁的更新，浪费性能。 如果写入数据库的值不是直接写入缓存，是需要经过复杂计算的话，那么使用懒加载是最合适的，无需每次写入都浪费性能计算 3.2 先删除缓存，再更新数据库（同问题2） 详情请看问题2：为什么写操作时是先操作数据库呢？为什么不先操作缓存呢？ 3.3 先更新数据库，再删除缓存 这种方案就是刚刚上面提到的Cache Aside Pattern： 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 但是无论是方法2还是方法3，都会遇到一下这种情况：假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作 * 1）缓存刚好失效 * 2）请求A查询数据库，得一个旧值 * 3）请求B将新值写入数据库 * 4）请求B删除缓存 * 5）请求A将查到的旧值写入缓存 如果将步骤3）和 4）互换就是方案2了 所以，如果发生了上述情况，确实会导致数据不一致的情况，但是这种情况概率比较低，因为一般来说读操作是比写操作快的，但是要解决上面的问题还有有一定的办法的，但是要牺牲性能。 四、特殊方案 4.1 延时双删除 先删除缓存 再更新数据库 再删除缓存 其中可在异步队列延时进行，其主要目的是为了确保读请求结束，写请求可以删除读请求可能带来的缓存脏数据。 而另外一个原因就是，现在基本都是读写分离，数据库之间数据同步也有一定的延迟，而使用延时双删除，也是为了确认在读写数据同步的情况下，删除掉读请求可能带来的不一致性。 但是这里还有一个问题就是如果删除缓存失败了怎么办？那就使用删除失败重试机制 4.2 删除失败重试机制 其实可以继续丢回队列里面重新进行删除操作。 4.3 读取biglog异步删除缓存 五、总结： 高并发场景下会遇到各种极端问题，并且造成不一致的主要原因是cache和db的更新不在一个原子操作上，即便先更新再删缓存一样存在数据不一致的问题，比如网络抖动、业务阻塞等等。双删的目的是尽量缩短不一致的窗口期，当然这个窗口期时长的接受程度取决于业务场景，适合中等体量的高并发。在这个问题上并没有一个完美的解决方案。 每个缓存尽量加上过期时间，确认数据的最终一致性 对于需要强一致性的数据，如果业务复杂度不高的情况下，尽量从数据库读取并校验后再进行业务处理，例如一些订单、金额的相关操作 一直实际业务用到缓存的都是弱一致性，存储一些基础数据、购物车数据、个人信息等等，因此基本的Cache Aside Pattern也可以满足，所以还是得看具体业务和接受程度 如果非要强一致性的话，可以并发写期间加锁，任何读操作不写入缓存或者缓存及数据库封装版本乐观锁，数据库操作版本加1，更新缓存时通过lua脚本，版本大于当前值才更新（但是一般读写缓存就是为了解决好并发，加锁的话可能会死得更惨） ","link":"http://mofish.pily.life/post/redis_learning_13/"},{"title":"计算机网络知识点(一)：在浏览器中输入url地址->显示主页的过程（已迁移）","content":"😏 一个页面从输入 URL 到页面加载显示完成，这个过程都发生了什么？ 🤠 这个也是经典面试题了，简单记录一下吧！ 简单的来说，分为一下几个过程： 浏览器通过 DNS 解析查找域名的 IP 地址 浏览器与目标服务器建立 TCP 连接（3次握手） 浏览器通过 HTTP 协议发送资源请求 服务端处理请求并返回 HTTP 报文 浏览解析渲染页面 关闭 TCP 连接（4次挥手） ","link":"http://mofish.pily.life/post/network_learning_01/"},{"title":"Redis学习之路(十二)：惰性删除和内存回收机制","content":"小小知识点，记录一下🤓 惰性删除 为什么要使用惰性删除？ 删除指令del会直接释放对象的内存，大部分情况该指令执行非常快，没有明显延迟，不过如果删除的key是一个非常大的对象，那么删除操作就会导致单线程卡顿。 因此 Redis 为了解决这个卡顿的问题，在4.0版本引入了unlink指令，它能对删除操作进行懒处理，丢给后台线程来异步回收内存。 &gt; unlink key &gt; ok 是否有线程安全问题？ 不会。当unlink指令发出后，它就已经被切断关系，丢到异步线程去处理了，因此在unlink的一瞬间，它就再也无法被主线程中的其它指令访问到了。 flush Redis 提供了flushdb和flushall指令，用来清空数据库，这也是及其缓慢的操作，同样可能会卡死线程，因此在4.0版本同样给这两个指令带来了异步化，在指令后增加async参数即可进行异步处理。 &gt; flushdb async &gt; ok 更多异步删除节点 除了del指令和flush操作之外，Redis在key的过期、LRU淘汰、rename指令过程中，也会实施回收内存。 此外，还有一种特殊的flush操作，其发生于正在进行全量同步的从节点中，在接受完整的rdb文件后，也需要将当前的内存一次性清空，以加载整个rdb文件的内容到内存。 Redis 4.0 为这些删除点也带来了异步删除机制，打开这些点需要额外的设置选项。 slave-lazy flush ：从节点接受完 rdb 文件后的 flush 操作 lazy ee-1 y-eviction ：内存达到 maxmemo可时进行淘汰。 lazy ee-lazy-expire key ：过期删除 la free- lazy-se er-del rename ：指令删除 destKey 内存回收机制 Redis 并不总是将空闲内存立即归还给操作系统。 如果当前 Redis 内存有 10GB ，当你删除了1GB key 后，再去观察内存，你会发现内存变化不会太大。 原因是操作系统是以页为单位来回收内存的，这个页上只要还有一个 key 在使用，那么它就不能被回收。 Redis 虽然删除了1GB key ，但是这些 key 分散到了很多页面中，每个页面都还有其他 key 存在，这就导致了内存不被立即回收。 不过，如果你执行 flushdb 然后再观察内存，会发现内存确实被回收了 。原因是所有的 key 都被干掉了，大部分之前使用的页面都完全干净了，就会立即被操作系统回收。 Redis 然无法保证立即回收已经删除的 key 的内存，但是它会重新使用那些尚未回收的空闲内存。这就好比电影院里虽然一拨观众走了，但是座位还在，下一拨观众来了，直接坐上就行，而操作系统回收内存就好比把座位也都给搬走了。 ","link":"http://mofish.pily.life/post/redis_learning_12/"},{"title":"Redis学习之路(十一)：过期删除策略和内存淘汰策略","content":"❓ 设置完一个键的过期时间后，到了这个时间，这个键还能获取到么？假如获取不到那这个键还占据着内存吗？ 😐 如何设置Redis的内存大小？当内存满了之后，Redis有哪些内存淘汰策略？我们又该如何选择？ 目录 一、过期删除策略 1.1 过期删除方式 1.2 redis使用的过期删除策略 1.3 定时扫描规则 1.4 从节点的key过期后是怎么处理的？ 二、内存淘汰策略 2.1 相关参数解释 2.2 淘汰方式 2.3 近似LRU算法 2.4 近似LRU算法的优化 2.5 go实现LRU算法 2.6 LFU算法 一、过期删除策略 在 redis 内部，每当我们设置一个 key 的过期时间时，redis 就会将该 key 带上过期时间存放到一个过期字典中，以后会定时遍历这个字典来删除到期的 key。 处理定时遍历外，它还会使用 惰性删除策略来删除过期的 key，所谓 惰性删除策略就是在客户端访问这个 key 的时候，redis 对这个 key 的过期时间进行检查，如果过期了就立即删除。 如果说定时删除是集中处理，那么惰性删除就是零散处理。 1.1 过期删除方式 通常删除某个key，我们有如下三种方式进行处理： 定时删除 描述：在设置某个 key 的过期时间的同时，创建一个定时器，让定时器在该过期时间到来时，立即执行删除操作 优点：对内存最友好的，能够保证内存的key一旦过期就能立即从内存中删除 缺点：对CPU最不友好，需要维护定时器，在过期key较多的情况下，对服务器的响应时间和吞吐量造成影响 总结：用处理器性能换取存储空间 （拿时间换空间） 惰性删除 描述：当客户端访问该 key 时再去检查是否过期 优点：对CPU友好，只有当使用该 key 时才会进行检查，其它用不到的 key 就不用浪费时间进行检查 缺点：对内存不友好，因为当一个 key 过期后，如果一直没使用，就一直删除不了，内存就得不到释放。 总结：用存储空间换取处理器性能（拿空间换时间） 定期删除 描述：每隔一段时间就对一些 key 进行检查，删除里面过期的key 优点：可以通过限制删除操作执行的时长和频率来减少删除操作对 CPU 的影响。另外定期删除，也能有效释放过期键占用的内存 缺点：需要控制执行时长和频率，频率太高就会变得像定时删除，频率太低又会变为惰性删除 总结：周期性抽查存储空间 （随机抽查，重点抽查） 1.2 redis使用的过期删除策略 redis 使用的过期删除策略是：惰性删除 + 定期删除。 # redis.conf hz 10 // 定期删除 redis 配置，每秒运行的次数，默认每秒运行10次。 1.3 定期扫描规则 redis 默认每秒进行10次过期扫描，过期扫描不会遍历过期字典中所有的key，而是采用一种简单的贪心策略，步骤如下： 1）从过期字典中随机选出20个key 2）删除这20个key中已经过期的key 3）如果过期的key的比例超过1/4，则重复步骤1） 同时，为了保证过期扫描不会出现循环过度，导致线程卡死的现象，算法还增加了扫描时间上线，默认不会超过25ms。 注意： 开发的时候一定要注意key的过期时间，如果有大批量key过期，尽量每个key设置一个随机范围，避免大量key同时过期，从而卡顿。 因为当大量key同时过期时，redis会持续扫描过期字典多次，直到过期的key变得稀疏，这会导致线上读写请求出现明显卡顿，导致卡顿的另外一个原因时内存管理器需要频繁回收内存页，这也会产生一下的CPU消耗。 虽然有设置扫描时间为25ms的限制，但是如果客户端超时时间设置得比较短，如10ms，就会出现大量的链接因为超时而关闭，从而造成业务异常。 1.4 从节点的key过期后是怎么处理的？ 从节点不会进行过期扫描，从节点对过期的处理是被动的。主节点在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从节点，从节点通过执行这条 del 指令来删除过期的key。 因为指令同步是异步进行的，所以如果主节点过期的 key 的 del 指令没有及时同步的从节点的话，就会出现主从数据不一致的情况，主节点没有的数据从节点还存在。 二、内存淘汰策略（逐出算法） 我们看到，通过过期删除策略，对于某些冷 key 或永远用不到的 key，并且多次定期删除也没有选定并删除，那么这些 key 同样会一直驻留在内存中，又或者在 redis 中存入大量的 key，这些操作可能都会导致 redis 内存不够用，这时候就需要 redis 的内存淘汰策略了。 2.1 相关参数解释 最大可使用内存，默认值为0，表示不限制，生产环境根据需求设定，通常设置在50%以上 maxmemory 每次选取待删除数据的个数，选取数据时不会全库扫描，而是随机获取 maxmemory-samples 删除策略，达到最大内存后，对被挑选出来的数据进行删除的策略 maxmemory-policy 2.2 淘汰方式 volatile（从设置了过期时间的key中查找server.db[i].expires） volatile-lru：挑选最近最少使用的数据淘汰 volatile-lfu：挑选最近使用次数最少的数据淘汰 volatile-ttl：挑选将要过期的数据淘汰 volatile-random：任意选择数据淘汰 allkeys（检测全库的数据server.db[i].dict） allkeys-lru：挑选最近最少使用的数据淘汰 allkeys-lfu：挑选最近使用次数最少的数据淘汰 allkeys-random：任意选择数据淘汰 no-enviction no-enviction：禁止驱逐数据（redis4.0中默认策略），会引发错误OOM（out of memory） 2.3 近似LRU算法 redis 使用的是一种近似LRU 算法，它跟常规的 LRU 算法不太一样。近似 LRU 算法通过随机采样法淘汰数据，每次随机出5（默认）个key，从里面淘汰掉最近最少使用的key。 可以通过maxmemory-samples参数修改采样数量： 例：maxmemory-samples 10 maxmenory-samples配置的越大，淘汰的结果越接近于严格的LRU算法 redis 为了实现近似LRU算法，给每个 key 增加一个长度为24bit的额外的字段，用于存储最后一次被访问的时间戳。 之所以不适用标准的LRU算法，是因为其需要消耗大量的内存，需要对现有的数据结构进行较大的改造（标准LRU算法需要额外维护一个链表） 而近似LRU算法很简单，只需要在现有的数据基础上是由随机采样法来淘汰元素即可 大大的节省了内存占用以及提升了性能 2.4 近似LRU算法的优化 Redis3.0对近似LRU算法进行了一些优化。新算法会维护一个候选池（大小为16），池中的数据根据访问时间进行排序，第一次随机选取的key都会放入池中，随后每次随机选取的key只有在访问时间小于池中最小的时间才会放入池中，直到候选池被放满。当放满后，如果有新的key需要放入，则将池中最后访问时间最大（最近被访问）的移除。 当需要淘汰的时候，则直接从池中选取最近访问时间最小（最久没被访问）的key淘汰掉就行。 2.5 go实现LRU算法 package main import &quot;fmt&quot; // Node 定义节点接口 type Node struct { Key string Value string pre *Node next *Node } var head *Node var end *Node type LRUCache struct { limit int HashMap map[string]*Node } // GetLRUCache 初始化LRUCache func GetLRUCache(limit int) *LRUCache { lruCache := LRUCache{limit: limit} lruCache.HashMap = make(map[string]*Node, limit) return &amp;lruCache } //获取值 func (l *LRUCache) get(key string) string { // 判断这个key是否在map中，在的话ok=true，不在的话ok=false if v,ok := l.HashMap[key];ok { l.refreshNode(v) return v.Value } else { return &quot;&quot; } } //插入新的值 func (l *LRUCache) put(key, value string) { if v,ok := l.HashMap[key];!ok { if len(l.HashMap) &gt;= l.limit { oldkey := l.removeNode(head) delete(l.HashMap, oldkey) } node := Node{Key: key, Value: value} l.addNode(&amp;node) l.HashMap[key] = &amp;node } else { v.Value = value l.refreshNode(v) } } // 刷新node节点的位置，放到最后 func (l *LRUCache) refreshNode(node *Node) { if node == end { return } l.removeNode(node) l.addNode(node) } //移除节点关系 func (l *LRUCache) removeNode(node *Node) string{ if node == end { end = end.pre } else if node == head { head = head.next } else { node.pre.next = node.next node.next.pre = node.pre } return node.Key } // 向队尾添加节点 func (l *LRUCache) addNode(node *Node) { if end != nil { end.next = node node.pre = end node.next = nil } end = node if head == nil { head = node } } func main() { lruCache := GetLRUCache(5) // 定义一个map长度为5的LRU缓存池 lruCache.put(&quot;001&quot;, &quot;用户1信息&quot;) lruCache.put(&quot;002&quot;, &quot;用户1信息&quot;) lruCache.put(&quot;003&quot;, &quot;用户1信息&quot;) lruCache.put(&quot;004&quot;, &quot;用户1信息&quot;) lruCache.put(&quot;005&quot;, &quot;用户1信息&quot;) fmt.Println(lruCache.HashMap) lruCache.get(&quot;002&quot;) // 获取了002，此时排序：001 003 004 005 002 fmt.Println(lruCache.HashMap) lruCache.put(&quot;004&quot;, &quot;用户2信息更新&quot;) // 更新了004，此时排序：001 003 005 002 004 lruCache.put(&quot;006&quot;, &quot;用户3信息插入&quot;) // 插入了006，此时排序：003 005 002 004 006 fmt.Println(lruCache.get(&quot;001&quot;)) // 获取001，为空，因为已被删除 fmt.Println(lruCache.get(&quot;003&quot;)) // 获取了003，此时排序： 005 002 004 006 003 lruCache.put(&quot;007&quot;, &quot;用户4信息插入&quot;) // 插入了007，此时排序： 002 004 006 003 007 fmt.Println(lruCache.get(&quot;005&quot;)) // 获取005，为空，因为已被删除 fmt.Println(lruCache.HashMap) } 2.6 LFU算法 LFU算法（Least Frequently Used）是redis4.0里面新加的一种淘汰策略，它的核心思想是根据 key 的最近被访问的频率进行淘汰，很少被访问的优先被淘汰，被访问多的则被留下来。 LFU算法能更好的表示一个 key 被访问的热度。假如使用的是LRU算法，一个 key 很久没有被访问，只是刚刚好偶尔被访问了一次，那么它就被认为是热点数据，不会被淘汰，而有些 key 是经常被访问的，只是最近比较少被访问，那么就有可能被误以为是冷数据而被淘汰。 ","link":"http://mofish.pily.life/post/redis_learning_11/"},{"title":"Redis学习之路(十)：持久化存储","content":"❓ 为了防止数据的意外丢失，确保数据安全性，因此需要对数据进行持久化存储。 😱 利用永久性存储介质讲数据进行保存，在特定的时间将保存的数据进行恢复的工作机制称为持久化。 目录 一、持久化过程保存的是什么？ 二、RDB 三、AOF 四、RDB和AOF的区别 五、混合持久化 一、持久化过程保存的是什么？ 将当前数据状态进行保存，快照形式，存储数据结果，存储格式简单，关注点在数据 将数据的操作过程进行保存，日志形式，存储操作过程，存储格式复杂，关注点在数据的操作过程 二、RDB RDB 是一种以快照形式进行全量备份的持久化机制之一，快照是内存数据数据的二进制序列化形式，在存储上非常紧凑。 服务器中每X小时执行bgsave备份，并将RDB文件拷贝到远程机器中，用于灾难恢复。 2.1 快照原理 我们都知道 Redis 是单线程程序，这个线程需要同时负责多个客户端套接字的并发读写操作和内存数据结构的逻辑读写。 在服务线上请求的同时， Redis 还需要进行内存快照，内存快照要求 Redis 必须进行文件IO操作，可文件IO操作不能使用多路复用API，这意味着单线程再服务线上请求的同时，还要进行文件IO操作，而文件IO操作会严重拖累服务器的性能。 除此之外，还有一个重要问题就是 Redis 需要一边持久化，一边响应客户端的请求。可持久化的同时，内存数据结构还在改变，可能持久化还没完成，某个key就被删除掉了，这怎么办？ 为了解决这个问题 ， Redis使用操作系统的多进程 COW （copy on Write），写时拷贝机制来实现快照持久化。 2.2 写时拷贝 Redis 在持久化时会调用 glibc 的函数 fork 产生一个子进程，快照持久化完全交给子进程来处理，父进程继续处理客户端请求。 子进程刚刚产生时，它和父进程共享内存里面的代码段和数据段，这时你可以把父子进程想象成一个连体婴儿，它们在共享身体，所以为了节约内存资源，尽可能让它们共享起来。 子进程做数据持久化，不会修改现有的内存数据结构，它只对数据结构进行遍历读取，然后序列化写到磁盘中。但是父进程不一样，它必须持续服务客户端请求，然后对内存数据结构进行不间断的修改。 这个时候就会使用操作系统的 COW 机制来进行数据段页面的分离，如下图： 数据段是由很多操作系统的页面组合而成，当父进程对其中一个页面的数据进行修改时，会将被共享的页面复制一份出来，然后对这个复制的页面进行修改。这时子进程相应的页面是没有变化的，还是进程产生时那一瞬间的数据。 随着父进程修改操作的持续进行，越来越多的共享页面被分离出来，内存就会持续增长，但是也不会超过原有数据内存的2倍大小。另外，Redis 实例里冷数据占的比例往往是比较高的，所以很少会出现所有的页面都被分离的情况。 最后，子进程因为数据没有变化，它能看到的内存里的数据再进程产生的一瞬间就凝固了，再也不会改变，这也是为什么的 RDB 的持久化方式叫作 ‘快照’ 的原因，接下来子进程就可以安心地遍历数据，进行序列化写磁盘了。 2.3 save相关命令和配置 2.3.1 相关命令 save指令 作用：手动执行一次保存操作 注意：会阻塞当前Redis服务器，直到当前RDB过程完成为止，有可能会造成长时间阻塞，线上环境不建议使用 bgsave指令 作用：手动启动后台保存操作，但不是立即执行 注意： bgsave命令是针对save阻塞问题做的优化。Redis内部所有涉及到RDB操作都采用bgsave的方式，save命令可以放弃使用。 2.3.2 相关配置 dbfilename dump.rdb 说明：设置本地数据库文件名，默认值为dunp.rdb 建议：通常设置为dump端口号.rdb dir 说明：设置存储.rdb文件的路径 rdbcompression yes 说明：设置存储至本地数据库时是否压缩数据，默认为yes，采用LZF压缩 建议：通常默认为开启状态，如果设置为no，可以节省CPU处理时间，但是会使存储文件增大 rdbchecksum yes 说明：设置是否进行rdb文件格式校验，该校验过程在写文件和读文件过程均进行 建议：通常默认为开启状态，如果设置为no，可以节省读写时间，但是存在一定的数据损坏风险 stop-writes-on-bgsave-error yes 说明：后台存储过程中如果出现错误现象，是否停止保存操作 save second changes 说明：满足限定时间范围内key的变化数量达到指定数量即进行持久化 参数： second：监控时间范围 changes：监控key的变化量 建议： 根据实际业务情况进行设置，频率过高或过低都会出现性能和备份不及时的问题 多个配置时，尽量是互补关系，不要设置成包含关系，以防重复触发增加性能压力 save配置启动后执行的是bgsave操作 2.4 触发方式 全量复制 在主从复制中详解 服务器运行过程中重启 debug reload 关闭服务器时指定保存数据 shutdown save 2.5 优缺点 优点： RDB是一个紧凑压缩的二进制文件，存储效率较高 RDB内部存储的是redis在某个时间点的数据快照，非常适合用于数据备份，全量复制等场景 RDB恢复数据的速度要比AOF快很多 缺点： RDB方式无论是执行指令还是利用配置，无法做到实时持久化，具有较大的可能性丢失数据 bgsave指令每次运行要执行fork操作创建子进程，要牺牲掉一些性能 Redis的众多版本中未进行RDB文件格式的版本统一，有可能出现各版本服务之间数据格式无法兼容现象 基于快照的思想，每次读写都是全量数据，当数据量巨大时，效率非常低 三、AOF 为了解决RDB存储的弊端，AOF（append only file）持久化来了，以独立日志的方式记录每次写命令，重启时再重新执行aof文件中的命令达到恢复数据的目的，与RDB相比可以简单描述为AOF记录了数据产生的过程。 不写全数据，仅记录部分数据 降低区分数据是否改变的难度，改记录数据为记录操作过程 对所有操作均进行记录，排除丢失数据的风险 AOF主要作用是解决了数据持久化的实时性 3.1 AOF原理 AOF日志存储的是 Redis 服务器的顺序指令序列，并且只记录对内存进行修改的指令记录，在恢复数据时顺序执行日志中的所有指令——也就是“重放”，以此来恢复 Redis 当前实例的内存数据结构的状态。 Redis 会在收到客户端修改指令后，进行参数检验、逻辑处理，如果没有问题，就立即将该指令文本存储到AOF日志中，也就是说，先执行指令再将日志落盘。 3.2 三种策略(appendfsync) always（每次） 每次写入操作均同步到AOF文件中，数据零误差，性能较低，不建议使用 everysec（每秒） 每秒将缓冲区中的指令同步到AOF文件中，数据准确性较高，性能较高，建议使用，也是默认配置 在系统突然宕机的情况下丢失1秒内的数据 no（系统控制） 由操作系统控制每次同步到AOF文件的周期，整体过程不可控 3.3 相关配置 appendonly yes|no 作用：是否开启AOF持久化功能，默认为不开启状态 appendfsync always | everysec | no 作用：AOF写数据策略 appendfilename filename 作用：AOF持久化文件名，默认文件名为appendonly.aof，建议配置为appendonly-端口号.aof dir 作用：AOF持久化文件保存路径，与RDB持久化文件保持一致即可 3.4 AOF重写 随着命令不断写入AOF日志，文件会越来越大，为了解决这个问题，Redis引入了AOF重写机制来压缩文件体积。 AOF文件重写是将Redis进程内的数据转化为写命令同步到新AOF文件的过程，简单说就是将对同一数据的若干个命令执行结果转化成最终结果数据对应的指令进行记录。 3.4.1 重写作用 减低磁盘占用量，提高磁盘利用率 提高持久化效率，降低持久化写时间，提高IO性能 降低数据恢复用时，提高数据恢复效率 3.4.2 重写规则 进程内已超时的数据不再写入文件 忽略无效指令，重写时使用进程内数据直接生成，这样新的AOF文件只保留最终数据的写入命令 对同一数据的多条写命令合并为一条命令 如lpush list1 a、lpush list1 b、 lpush list1 c 可以转化为：lpush list1 a b c 为防止数据量过大造成客户端缓冲区溢出，对list、set、hash、zset等类型，每条指令最多写入64个元素 3.4.3 两种重写方式 手动重写 bgrewriteaof 自动重写 auto-aof-rewrite-min-size size auto-aof-rewrite-percentage percentage 3.4.4 AOF自动重写方式 自动重写触发条件设置 auto-aof-rewrite-min-size size auto-aof-rewrite-percentage percentage 自动重写触发比对参数（ 运行指令info Persistence获取具体信息 ） aof_current_size(代表当前AOF文件空间) aof_base_size)(上一次重写后AOF文件空间) 自动重写触发条件 aof_current_size &gt; auto-aof-rewrite-min-size (aof_current_size−aof_base_size)÷aof_base_size&gt;=(aof\\_current\\_size-aof\\_base\\_size) \\div aof\\_base\\_size&gt;=(aof_current_size−aof_base_size)÷aof_base_size&gt;=auto-aof-rewrite-percentage 3.4.5 AOF重写流程 1). 重写条件出发，执行bgrewriteaof命令 2). 父进程 fork 子进程进行重写，fork 子进程的同时父进程阻塞，fork完毕后，父进程继续接受指令 3). 子进程在创建新的AOF文件的同时，父进程继续接收write指令，并且继续存到aof_buf缓存中和 aof_rewirte_buf缓存中，所以父进程继续往旧的AOF文件中备份，同时也要往新的AOF文件中备份。 4). 子进程根据内存快照，按照命令合并规则写入到新的AOF文件 5.1). 子进程写完新的AOF文件后，向父进程发信号，父进程更新统计信息，具体可以通过info persistence查看 5.2). 父进程把AOF重写缓冲区（aof_rewrite_buf）的数据写入到新的AOF文件，这样就保证了新AOF文件所保存的数据库状态和服务器当前状态一致 5.3). 使用新的AOF文件替换老文件，完成AOF重写 四、RDB和AOF的区别 RDB和AOF的选择实际上是在做一种权衡，每种都有利弊 如不能承受分钟以内的数据丢失，对业务数据非常敏感的，建有选用AOF 如能承受数分钟以内的数据丢失，且追求大数据集的恢复速度，建议选用RDB 灾难恢复选用RDB 双保险策略，同时开启RDB和AOF，采用混合持久化方案，重启后，Redis优先使用AOF来恢复数据，降低丢失数据的风险 五、混合持久化 重启 redis 时，很少使用RDB来恢复内存状态，因为会丢失大量数据，我们通常都是用AOF日志来重放，但是重放AOF日志相较于RDB来说要慢很多，这样在 redis 实例很大的时候，启动需要花费很长的时间。 redis 4.0 为了解决这个问题，新增了一个混合持久化选项。如下图所示，将 RDB 文件的内容和增量 AOF 日志文件存一起。这里的 AOF 日志不再是全量的日志，而是自持久化开始到持久化结束的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小。 ","link":"http://mofish.pily.life/post/redis_learning_10/"},{"title":"Redis学习之路(九)：为什么 Redis 单线程却能支撑高并发?","content":"经典面试题了这是😇 为什么Redis是单线程的 因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。 这里的单线程可以理解为：执行命令仍然是单线程顺序执行。 其实Redis 4.0 开始就有多线程的概念了，比如 Redis 通过多线程方式在后台删除对象、以及通过 Redis 模块实现的阻塞命令等。 单线程的优劣势 单进程单线程优势 代码更清晰，处理逻辑更简单 不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗 不存在多进程或者多线程导致的切换而消耗CPU 单进程单线程弊端 无法发挥多核CPU性能，不过可以通过在单机开多个Redis实例来完善； 为什么CPU不是瓶颈 redis是基于内存的，因此减少了cpu将数据从磁盘复制到内存的时间 redis是单线程的，因此减少了多线程切换和恢复上下文的时间 redis是单线程的，因此多核cpu和单核cpu对于redis来说没有太大影响，单个线程的执行使用一个cpu即可 只是相对其它的数据库来说，redis没有受到cpu的太多限制，也并不是丝毫没有关系。 redis官网说，因为cpu不是redis的瓶颈，且单线程简单，因此redis采用的单线程。内存大小和网络带宽才有可能是redis的瓶颈。 Redis 6.0 为什么要引入多线程呢？ 既然Redis 的瓶颈并不在 CPU，而在内存和网络。 内存不够的话，可以加内存或者做数据结构优化和其他优化等，但网络的性能优化才是大头，网络 IO 的读写在 Redis 整个执行期间占用了大部分的 CPU 时间，如果把网络处理这部分做成多线程处理方式，那对整个 Redis 的性能会有很大的提升。 6.0因为的多线程针对的是网络IO，并非是多线程执行redis命令，执行命令仍然是单线程顺序执行。 Redis 6.0开启多线程后，是否会存在线程并发安全问题？ 从实现机制可以看出，Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行。 所以我们不需要去考虑控制 Key、Lua、事务，LPUSH/LPOP 等等的并发及线程安全问题。 什么是IO多路复用技术 在了解I/O多路复用之前，我们可以先了解一下传统的I/O模型 阻塞I/O模型 (无法处理高并发) 最传统的一种 I/O 模型，即在读写数据过程中会发生阻塞现象。 当用户线程发出 I/O 请求之后，内核回去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态 (block)，用户线程交出CPU。 当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才能解除阻塞状态。 **非阻塞I/O模型 **(过度浪费CPU资源) 当用户线程发起一个 read 操作后，并不需要等待，而是马上得到一个结果。如果结果是一个 error 时，它就知道数据还没准备好，于是它可以再次发送 read 操作。 一旦内核中的数据准备好了，并且再次收到用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。 所以事实上，在非阻塞 I/O 模型中，用户线程需要不断地询问内核数据是否就绪，也就是说非阻塞 I/O 不会交出CPU，而会一直占用 CPU，从而导致 CPU 占用率非常高。 信号驱动 I/O 模型 (一般用于UDP中) 在信号驱动 I/O 模型中，当用户线程发起一个 I/O 请求操作，会给对应的 socket 注册一个信号函数，然后用户线程会继续执行。 当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用 I/O 读写操作来进行实际的 I/O 请求操作。 这个一般用于 UDP 中，对 TCP 套接口几乎是没用的，原因是该信号产生得过于频繁，并且该信号的出现并没有告诉我们发生了什么事情。 I/O 多路复用模型 该模式是目前使用得比较多的 I/O 模型，在该模型中，会有一个线程不断去轮循多个 socket 的状态，只有当 socket 真正有读写事件时，才真正调用实际的 I/O 读写操作。 这里的 I/O 通常指网络 I/O，多路指多个 Socket 链接，复用指操作系统进行运算调度的最小单位线程。整体意思也就是多个网络 I/O 复用一个或少量的线程来处理 Socket。 因为 I/O 多路复用模型中，只需要一个线程就可以管理多个 socket，系统不需要建立和维护新的进程或者线程，并且只有真正有 socket 读写事件进行时，才会使用 I/O 资源，所以大大的减少了资源占用。 其实采用多线程+阻塞 I/O 也可以达到类似效果，但是由于在多线程 + 阻塞 I/O中，每个 socket 对应一个线程，这样会造成很大的资源占用， 尤其是对于长连接来说，线程的资源一直不会释放，如果连接较多的话，会造成性能上的瓶颈。 而 I/O 多路复用模型，通过一个线程就可以管理多个 socket，只有当 socket 真正有读写事件发生才会占用资源来进行实际的读写操作。 因此 I/O 多路复用比较适合连接数较多的情况。 另外，I/O 多路复用为比非阻塞 I/O 模型效率高是因为在非阻塞 I/O 中，不断地询问 socket 状态是通过用户线程去进行的， 而在 I/O 多路复用中，轮询每个 socket 状态是在内核进行的，效率比用户线程高很多 对于多路复用 I/O 模型来说，一旦事件响应太慢，那么就会导致后续的事件迟迟得不到处理，并且会影响新的事件轮询。 这就是说，如果 Redis 每条命令执行如果占用大量时间，就会造成其他线程阻塞，对于 Redis 这种高性能服务是致命的，所以 Redis 是面向高速执行的数据库。 异步 I/O 模型 异步 I/O 模型是最理想的 I/O 模型，当用户线程发起 read 操作后，立刻可以去做其它事情，另一方面，从内核的角度看，当它收到一个 asynchronous read 之后，它会立刻返回，说明 read 请求已经成功发起了，因此不会对用户线程产生任何阻塞(block)。 然后内核会等待数据准备完成，然后将数据拷贝到用户线程，当一切处理完成后，内核会给用户线程发送一个信号告诉它 read 操作完成了。 也就是说，用户线程完全不需要关心实际的整个 I/O 操作是如何进行的，只需要先发起一个请求，当接收内核返回了成功信号时，可以直接去使用数据了。 各种 I/O 模型的对比 需要注意的是，除了异步 I/O 外，其它的四种模型其实都可以归类为阻塞式 I/O 模型，从图中我们可以看出前四种模型的第二阶段都是相同的，都会阻塞。 而阻塞 I/O 模型和 I/O 多路复用模式是两个阶段都阻塞的，那区别在哪呢？ 区别就在于，虽然第一阶段都阻塞，但是阻塞 I/O 模型如果接收更多的连接，就必须创建更多的线程；而 I/O 多路复用模型在第一阶段大量的连接通通都可以直接注册到 selector 复用器上，同时是要单个或者少量的线程来循环处理这些连接事件就可以了，一旦达到“就绪”的条件，就可以立即执行真正的 I/O 操作。 这就是 I/O 多路复用模型和阻塞 I/O 模型最大的不同，也是 I/O 多路复用模型的精髓所在。 I/O 多路复用技术的实现 Redis的I/O多路复用模式使用的是 Reactor设计模式 的方式来实现 文件描述符(file descriptor)： Linux系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符。 可以理解文件描述符是一个索引，这样，要操作文件的时候，我们直接找到索引就可以对其进行操作了。 我们将这个索引叫做文件描述符，简称 fd。 如上图对 Redis 的 I/O 多路复用模型进行说明一下： 一个 socket 客户端与服务端连接时，会生成对应一个套接字描述符（套接字描述符是文件描述符的一种），每一个 socket 网络连接其实都对应一个文件描述符。 多个客户端与服务端连接时，Redis 使用 「I/O 多路复用程序」将客户端 socket 对应的 fd 注册到监听列表（一个队列）中。当客户端执行accept、read、write和close等操作命令时，「I/O 多路复用程序」会将命令封装称一个事件，并绑定到对应的 fd 上。 当 socket 有文件事件产生时，I/O 多路复用模块就会将那些产生了事件的套接字 fd 传送给文件事件分派器。 「文件事件分派器」接收到「I/O多路复用」程序传来的套接字 fd 后，并根据套接字产生的事件类型，将套接字派发给相应的时间处理器来进行处理相关命令操作。 例如：以Redis的I/O多路复用程序 epoll函数为例 多个客户端连接服务端时，Redis会将客户端 socket 对应的 fd 注册进epoll，然后epoll同时监听多个文件描述符fd是否有数据到来， 如果有数据来了就通知事件处理器赶紧处理，这样就不会存在服务端一直等待某个客户端给数据的情形。 #（I/O多路复用程序函数有select、poll、epoll、kqueue） 整个「文件事件处理器」是在单线程上运行的，但是通过「I/O多路复用模块」的引入，实现了同时对多个 fd 读写的监控，当其中一个client端达到就绪的状态，文件事件处理器就马上执行，从而不会出现 I/O 堵塞的问题，提高了网络通信的性能。 I/O 多路复用有多种实现模式：select、 poll、 epoll select poll epoll 操作方式 遍历 遍历 回调 底层实现 数组 链表 红黑树 IO效率 每次调用都进行线性遍历，时间复杂度为O(n) 每次调用都进行线性遍历，时间复杂度为O(n) 事件通知方式，每当fd就绪，系统注册的回调函数就会被调用，将就绪fd放到readyList里面，时间复杂度O(1) 最大连接数 1024（x86）或2048（x64） 无上限 无上限 fd拷贝 每次调用select，都需要把fd集合从用户态拷贝到内核态 每次调用poll，都需要把fd集合从用户态拷贝到内核态 调用epoll_ctl时拷贝进内核并保存，之后每次epoll_wait不拷贝 select 函数 每次调用select，都需要把 fd_set 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大； 同时每次调用select都需要在内核遍历传递进来的所有fd，如果fd_set集合很大时，那这个开销也很大； 为了减少数据拷贝带来的性能损坏，内核对被监控的 fd_set 集合大小做了限制，大小不可改变(限制为1024)。 poll 函数 poll的机制与select类似，管理多个描述符也是进行轮询，根据描述符的状态进行处理； poll改变了文件描述符集合的描述方式，使用了 pollfd 结构而不是select的 fd_set 结构，使得poll没有最大文件描述符数量的限制，解决了select的问题3，但是并没有解决问题1、2的性能问题。 epoll 函数 为处理大批量文件描述符而作了改进的poll，是Linux下 I/O多路复用 接口select/poll的增强版本但是只能再Linux下工作； 同样没有描述符数量的限制； 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll； 使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次； epoll可以理解为 event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的； epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。 epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。 水平触发 (LT) : 默认工作模式，只要这个fd还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作； 边缘触发 (ET) : 又称高速模式，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无 论fd中是否还有数据可读，所以在ET模式下，read一个fd的时候一定要把它的buffer读光，也就是说一直读到read的返回值小于请求值，或者遇到EAGAIN错误。 如果采用EPOLLLT模式的话，系统中一旦有大量你不需要读写的就绪文件描述符，它们每次调用epoll_wait都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率.。而采用EPOLLET这种边沿触发模式的话，当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符 为什么要用IO多路复用技术(Redis默认使用epoll) 首先采用普通的同步阻塞I/O，那么Redis可能会在一个客户端上长期阻塞。该客户端可能长期没有数据到达，而Redis需要处理多个客户端的通信，当其他客户端有请求到达时，Redis则无法处理了，这显然是无法接受的。 如果使用同步非阻塞I/O，那么就需要不断轮循客户端，那么这种频繁的轮循会很浪费CPU资源，如果轮循不频繁，那么可能就会出现数据不能实时获取的问题。 如果使用 异步IO模型，线程的创建和频繁的上下文切换会浪费更多的资源。其次Redis本身就是单进程单线程的模式工作，多线程等待多个客户端显然与其系统思想不符。 因此，代入到Redis来说，Redis 是单线程架构，所有的命令操作都是先进入队列，然后一个一个按照顺序线性执行的，但是由于读写操作等待用户输入或输出都是阻塞的，所以 I/O 操作在一般情况下往往不能直接返回，这会导致某一文件的 I/O 阻塞导致整个进程无法对其它客户提供服务，而采用 I/O 多路复用技术就是为了解决这个问题。 它最大的优势在于可以在一次阻塞中监听多个文件描述符（FD），即可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗）。 综上，Redis 采用 I/O 多路复用技术（epoll）是因为 Redis 是单线程架构，是为了避免网络 I/O 读写操作阻塞整个进程。 单线程却能支撑搞并发总结： 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； Redis中的数据结构是专门进行设计的，简单、高效，如压缩表、跳表等等； 采用I/O多路复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗）； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； Redis基于Reactor模式开发了自己的网络事件处理器，效率比较高。 ","link":"http://mofish.pily.life/post/redis_learning_09/"},{"title":"Redis学习之路(八)：Stream","content":"😚 Stream 是 Redis 5.0 版本新增加的数据结构，用于弥补 List 和 Pub/Sub 来实现消息队列功能的不足。 👏 其提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。 目录 简述 消息ID和消息内容 增删改查 独立消费 创建消费组 消费 PEL如何避免消息丢失 Stream的高可用 简述 Stream 是一个新的强大的支持多播的可持久化消息队列，其结构如下图所示： 它有一个消息链表，将所有加入的信息都串起来，每个消息都有一个唯一的ID和对应的内容。 此外，消息是持久化的，Redis重启后，内容还在，不会丢失。 每个 Stream 都有唯一的名称，就是其对应的 Key，在我们首次使用 xadd 指令追加消息时自动创建。 每个 Stream 都可以挂多个消费组（Consumer Group），每个消费组都会有个游标 last_delivered_id 在Stream 数组之上往前移动，表示当前消费组已经消费到哪条信息了。 另外每个消费组都有一个 Stream 内唯一的名称，消费者不会自动创建，它需要单独的指令 xgroup create 进行创建，需要指定从 Stream 的某个消息 ID 开始消费，这个 ID 用来初始化 last_deliverer_id 变量。 每个消费组的状态都是独立的，互不影响，也就是说同一份Stream内部的消息会被每个消费组消费到。 除此之外，同一个消费组可以挂接多个消费者（Consumer），这些消费者之间是竞争关系，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动，每个消费者有一个组内唯一名称。 我们从图中观察到，消费者内部会有一个状态变量pending_ids，它记录了当前已经被客户端读取，但是还没有 ask 的消息，这个 pending_ids 变量在 Redis 官方被称为 PEL，也就是 Pending Entries List，是一个核心的数据结构，它用来确认客户端至少消费了消息一次，而不会再网络传输的中途丢失了而没被处理。 消息ID和消息内容 消息ID的形式是 timestampInMillis-sequence，例如1527846880572-5，它表示当前的消息再毫米时间戳 1527846880572 时产生，并且是该毫秒内产生的第 5 条消息。 消息ID可以由服务器自动生产，也可以由客户端自己指定，但是形式必须是**“整数-整数”**，而且后面加入的ID必须大于前面的消息ID。 消息内容就是键值对，没啥特别的。 增删改查 xadd：向 Stream 追加消息 # * 号表示服务器自动生成ID，后面顺序跟着 key 和 value # 名字叫 dog，年龄 27 岁 127.0.0.1:6379&gt; xadd codehole * name dog age 27 &quot;1632301447116-0&quot; 127.0.0.1:6379&gt; xadd codehole * name pily age 18 &quot;1632301478393-0&quot; 127.0.0.1:6379&gt; xadd codehole * name ultraman age 99 &quot;1632301500592-0&quot; xlen：获取 Stream 消息长度 127.0.0.1:6379&gt; xlen codehole (integer) 3 xrange：获取 Stream 中的消息列表，会自动过滤已经删除的消息 # - 表示最小值 + 表示最大值 127.0.0.1:6379&gt; xrange codehole - + 1) 1) &quot;1632301447116-0&quot; 2) 1) &quot;name&quot; 2) &quot;moyuxing&quot; 3) &quot;age&quot; 4) &quot;27&quot; 2) 1) &quot;1632301478393-0&quot; 2) 1) &quot;name&quot; 2) &quot;pily&quot; 3) &quot;age&quot; 4) &quot;18&quot; 3) 1) &quot;1632301500592-0&quot; 2) 1) &quot;name&quot; 2) &quot;ultraman&quot; 3) &quot;age&quot; 4) &quot;99&quot; # 指定最小消息 ID 的列表 127.0.0.1:6379&gt; xrange codehole 1632301478393-0 + 1) 1) &quot;1632301478393-0&quot; 1) 1) &quot;name&quot; 1) &quot;pily&quot; 2) &quot;age&quot; 3) &quot;18&quot; 2) 1) &quot;1632301500592-0&quot; 1) 1) &quot;name&quot; 1) &quot;ultraman&quot; 2) &quot;age&quot; 3) &quot;99&quot; # 指定最大消息 ID 的列表 127.0.0.1:6379&gt; xrange codehole - 1632301478393-0 1) 1) &quot;1632301447116-0&quot; 2) 1) &quot;name&quot; 2) &quot;moyuxing&quot; 3) &quot;age&quot; 4) &quot;27&quot; 2) 1) &quot;1632301478393-0&quot; 2) 1) &quot;name&quot; 2) &quot;pily&quot; 3) &quot;age&quot; 4) &quot;18&quot; xdel：从 Stream 中删除消息 127.0.0.1:6379&gt; xdel codehole 1632301478393-0 (integer) 1 127.0.0.1:6379&gt; xlen codehole (integer) 2 # 删除的消息没了 127.0.0.1:6379&gt; xrange codehole - + 1) 1) &quot;1632301447116-0&quot; 2) 1) &quot;name&quot; 2) &quot;moyuxing&quot; 3) &quot;age&quot; 4) &quot;27&quot; 2) 1) &quot;1632301500592-0&quot; 2) 1) &quot;name&quot; 2) &quot;ultraman&quot; 3) &quot;age&quot; 4) &quot;99&quot; del：删除整个 Stream 消息列表中的所有消息 # 删除整个Stream 127.0.0.1:6379&gt; del codehole (integer) 1 127.0.0.1:6379&gt; xlen codehole (integer) 0 独立消费 我们可以在不定义消费组的情况下进行 Stream 消息的独立消费，当 Stream 没有新的消息时，甚至可以阻塞等待。 Redis 设计了一个单独的消费指令xread，可以将 Stream 当成普通的消息队列（list）来使用。 # 从 Stream 头部读取两条消息（重新插入前面三条信息） 127.0.0.1:6379&gt; xread count 2 streams codehole 0-0 1) 1) &quot;codehole&quot; 2) 1) 1) &quot;1632304533303-0&quot; 2) 1) &quot;name&quot; 2) &quot;moyuxing&quot; 3) &quot;age&quot; 4) &quot;27&quot; 2) 1) &quot;1632304538029-0&quot; 2) 1) &quot;name&quot; 2) &quot;pily&quot; 3) &quot;age&quot; 4) &quot;18&quot; # 从 Stream 尾部读取一条新的消息，$ 表明只要 Stream 中的新数据 # （毫无疑问，这里不会返回任何数据，因为没有新数据进来） 127.0.0.1:6379&gt; xread count 1 streams codehole $ (nil) # 从尾部阻塞等待新消息到来，下面的指令会堵住，直到新消息到来 127.0.0.1:6379&gt; xread block 0 count 1 streams codehole $ # 新打开一个窗口，通过这个窗口往 Stream 里插入一条信息 127.0.0.1:6379&gt; xadd codehole * name xiaohong age 16 &quot;1632381201794-0&quot; # 再切到第一个窗口，我们可以看到阻塞解除了，并返回了新的消息内容和等待时间 127.0.0.1:6379&gt; xread block 0 count 1 streams codehole $ 1) 1) &quot;codehole&quot; 2) 1) 1) &quot;1632381201794-0&quot; 2) 1) &quot;name&quot; 2) &quot;xiaohong&quot; 3) &quot;age&quot; 4) &quot;16&quot; (1.84s) 客户端如果想要使用 xread 进行顺序消费，那么一定要记住当前消费到哪里，也就是返回的消息ID。下次再继续调用 xread 时，可继续消费后续的信息，避免重复消费。 block 为 0 时表示永远阻塞，直到消息到来；block 为 1000 表示阻塞 1s，如果 1s 内没有任何消息到来，则返回 nil。 127.0.0.1:6379&gt; xread block 1000 count 1 streams codehole $ (nil) (1.08s) 创建消费组 Stream 通过 xgroup create 指令创建消费组，如下图所示： 注意：创建消费组需要提供起始信息 ID 参数用来初始化 last_delivered_id 变量。 # 表示从头开始消费 127.0.0.1:6379&gt; xgroup create codehole cg1 0-0 OK # $ 表示从尾部开始消费，只接受新消息，当前 Stream 消息会全部忽略 127.0.0.1:6379&gt; xgroup create codehole cg2 $ OK # 获取 Stream 信息 127.0.0.1:6379&gt; xinfo stream codehole 1) &quot;length&quot; 2) (integer) 3 # 共 3 个消息 3) &quot;radix-tree-keys&quot; 4) (integer) 1 5) &quot;radix-tree-nodes&quot; 6) (integer) 2 7) &quot;groups&quot; 8) (integer) 2 # 共 2 个消费组 9) &quot;last-generated-id&quot; 10) &quot;1632385551651-0&quot; 11) &quot;first-entry&quot; # 第 1 个消息 12) 1) &quot;1632385524922-0&quot; 2) 1) &quot;name&quot; 2) &quot;pily&quot; 3) &quot;age&quot; 4) &quot;18&quot; 13) &quot;last-entry&quot; # 做后一个消息 14) 1) &quot;1632385551651-0&quot; 2) 1) &quot;name&quot; 2) &quot;john&quot; 3) &quot;age&quot; 4) &quot;88&quot; # 获取 Stream 消费组信息 1) 1) &quot;name&quot; 2) &quot;cg1&quot; 3) &quot;consumers&quot; 4) (integer) 0 # 该消费者没有消费者 5) &quot;pending&quot; 6) (integer) 0 # 该消费者没有正在处理的消息 7) &quot;last-delivered-id&quot; 8) &quot;0-0&quot; 2) 1) &quot;name&quot; 2) &quot;cg2&quot; 3) &quot;consumers&quot; 4) (integer) 0 5) &quot;pending&quot; 6) (integer) 0 7) &quot;last-delivered-id&quot; 8) &quot;1632385551651-0&quot; 消费 Stream 提供了 xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消费者名称和起始消息ID。 它同 xread 一样，也可以阻塞等待新信息，读到新信息后，对应的消息ID就会进入消费者的 PEL （正在处理的消息）结构里，客户端处理完毕后使用 xack 指令通知服务器后，表示该消息已处理完毕，该消息 ID 就会从 PEL 中移除。 # &gt; 号表示从当前消费组的 last_delivered_id 后面开始读 # 每当消费者读取一条消息， last_delivered_id 变量就会前进 127.0.0.1:6379&gt; xreadgroup Group cg1 c1 count 1 streams codehole &gt; 1) 1) &quot;codehole&quot; 2) 1) 1) &quot;1632388256718-0&quot; 2) 1) &quot;name&quot; 2) &quot;pily&quot; 3) &quot;age&quot; 4) &quot;18&quot; 127.0.0.1:6379&gt; xreadgroup Group cg1 c1 count 1 streams codehole &gt; 1) 1) &quot;codehole&quot; 2) 1) 1) &quot;1632388264858-0&quot; 2) 1) &quot;name&quot; 2) &quot;ultraman&quot; 3) &quot;age&quot; 4) &quot;22&quot; 127.0.0.1:6379&gt; xreadgroup Group cg1 c1 count 2 streams codehole &gt; 1) 1) &quot;codehole&quot; 2) 1) 1) &quot;1632388271050-0&quot; 2) 1) &quot;name&quot; 2) &quot;john&quot; 3) &quot;age&quot; 4) &quot;88&quot; # 再继续读取就没有消息了 127.0.0.1:6379&gt; xreadgroup Group cg1 c1 count 1 streams codehole &gt; (nil) # 阻塞等待接收消息 127.0.0.1:6379&gt; xreadgroup group cg1 c1 block 0 count 1 streams codehole &gt; # 开启另外一个窗口，往里面塞消息 127.0.0.1:6379&gt; xadd codehole * name marry age 24 &quot;1632388420051-0&quot; # 回到前一个窗口，发现阻塞解除，收到了新的消息 127.0.0.1:6379&gt; xreadgroup group cg1 c1 block 0 count 1 streams codehole &gt; 1) 1) &quot;codehole&quot; 2) 1) 1) &quot;1632388420051-0&quot; 2) 1) &quot;name&quot; 2) &quot;marry&quot; 3) &quot;age&quot; 4) &quot;24&quot; (12.63s) # 127.0.0.1:6379&gt; xinfo groups codehole 1) 1) &quot;name&quot; 2) &quot;cg1&quot; 3) &quot;consumers&quot; 4) (integer) 1 # 1 个消费者 5) &quot;pending&quot; 6) (integer) 4 # 共 4 条待处理信息还没有 ack 7) &quot;last-delivered-id&quot; 8) &quot;1632388420051-0&quot; 2) 1) &quot;name&quot; 2) &quot;cg2&quot; 3) &quot;consumers&quot; 4) (integer) 0 # 消费组 cg2 没有变化，因为前面都在操作 cg1 5) &quot;pending&quot; 6) (integer) 0 7) &quot;last-delivered-id&quot; 8) &quot;1632388271050-0&quot; # 如果同一个消费组有多个消费者，则可以通过 xinfo cunsumers 指令观察每个消费者的状态 127.0.0.1:6379&gt; xinfo consumers codehole cg1 1) 1) &quot;name&quot; 2) &quot;c1&quot; 3) &quot;pending&quot; 4) (integer) 4 # 共 4 条信息待处理 5) &quot;idle&quot; 6) (integer) 524136 # 空闲了多少ms没有读取消息了 # 接下来我们 ack 一条信息 127.0.0.1:6379&gt; xack codehole cg1 1632388256718-0 (integer) 1 # 待处理的信息变成 3 条 127.0.0.1:6379&gt; xinfo consumers codehole cg1 1) 1) &quot;name&quot; 2) &quot;c1&quot; 3) &quot;pending&quot; 4) (integer) 3 5) &quot;idle&quot; 6) (integer) 655723 # 把所有的信息都 ack 掉 127.0.0.1:6379&gt; xack codehole cg1 1632388420051-0 1632388264858-0 1632388271050-0 (integer) 3 # PEL 空了，没有待处理消息了 127.0.0.1:6379&gt; xinfo consumers codehole cg1 1) 1) &quot;name&quot; 2) &quot;c1&quot; 3) &quot;pending&quot; 4) (integer) 0 5) &quot;idle&quot; 6) (integer) 775588 PEL如何避免消息丢失 当客户端消费者读取 Stream 消息时，在 Redis 服务器将消息回复给客户端的过程中，如果客户端突然断开了连接，那个这个消息客户还没被客户端收到就丢失了。 不过没有关系，PEL 里已经保存了发出去的消息ID，待客户端重新连接上之后，可以再次收到PEL 中的消息ID。 Stream的高可用 Stream 的高可用是建立在主从复制基础上的，它和其它数据结构的复制机制没有区别，也就是说再 Sentinel 和 Cluster 集群环境下，Stream 是可以支持高可用的。 不过鉴于 Redis 的指令复制是异步的，在 failover 发生时，Redis 可能会丢失极小部分数据，这一点 Redis 的其他数据接口也是一样的。 故障failover表现在一个master分片故障后，slave接管master的过程。 小结 Stream 的消费模型借鉴了 Kafka 的消费分组的概念，弥补了 Redis PubSub 不能持久化消息的缺陷。 Stream 又不用于Kafka 的消息可以分 Partition，而 Stream 不行。 如果非要分 Partition 的话， 得在客户端做，提供不同的 Stream 名称，对消息进行 hash 取模来选择往哪个 Stream 里塞。 ","link":"http://mofish.pily.life/post/redis_learning_08/"},{"title":"数据结构学习之路(四)：ZipList压缩链表","content":"😵本来这篇是写在《Redis学习之路(一)：5大基础数据类型之Hash》里面的，但是因为篇幅可能过长， 而且关于数据结构的想单独拧出来整理，所以就抽出来写，那么下面就继续学习ZipList😗 基本概念 ZipList是一个经过特殊编码的双向链表，它不存储指向上一个链表节点和指向下一个链表节点的指针， 而是存储上一个节点长度和当前节点长度，通过牺牲部分读写性能来缓存高效的内存空间利用率，是一种时间换空间的思想😄 具体来说，就是ZipList不像❌普通双向链表一样，用地址指针把每一项链接起来，而是通过把表中每一项存放在前后连续的地址空间内，一个ziplist整体占用一大块内存。 另外，ZipList为了在细节上节省内存，对于值的存储采用了变长的编码方式，即大的整数是就用多一些字节存储，小的整数就用少一些字节来存储，接下来我们会就此讨论其细节😣 数据结构 好的，我们继续，从宏观上看，ZipList的内存结构如下： 对其各个部分在内存上是前后相邻的，它们分别的含义如下 属性 长度 描述 zlbytes 32bit 表示ziplist占用的字节总数（也包括&lt;zlbytes&gt;本身占用的4个字节） zltail 32bit 表示ziplist表中最后一项（entry）在ZipList中的偏移字节数。&lt;zltail&gt;的存在，使得我们可以很方便地找到最后一项（不用遍历整个ZipList），从而可以在ZipList尾端快速地执行push或pop操作。 zllen 16bit 表示ziplist中数据项（entry）的个数。zllen字段因为只有16bit，所以可以表达的最大值为 2^16 - 1。 这里需要特别注意的是，如果ziplist中数据项个数超过了16bit能表达的最大值，ziplist仍然可以来表示。那怎么表示呢？这里做了这样的规定：如果&lt;zllen&gt;小于等于2^16 - 2（也就是不等于2^16 - 1），那么&lt;zllen&gt;就表示ziplist中数据项的个数；否则，也就是&lt;zllen&gt;等于16bit全为1的情况，那么&lt;zllen&gt;就不表示数据项个数了，这时候要想知道ziplist中数据项总数，那么必须对ziplist从头到尾遍历各个数据项，才能计数出来。 entry 不定 表示真正存放数据的数据项，长度不定。一个数据项（entry）也有它自己的内部结构，这个稍后再解释。 zlend 8bit ziplist最后1个字节，是一个结束标记，值固定等于255。 entry ZipList为了支持双向遍历，所以才会有&lt;zltail&gt;这个字段用来快速定位最后一个元素，然后进行反向遍历。 但是我们知道ZipList是一块连续的内存，&lt;entry&gt;的大小是不确定的，那么我们要反向遍历的话得怎么找到上一个节点开始的位置呢？ 下面我们来看一下&lt;entry&gt;的结构： 属性 描述 prevlen 表示前entry的字节长度。这个字段的用处是为了让ziplist能够从后向前遍历（从后一项的位置，只需向前偏移prevlen个字节，就找到了前一项）。这个字段采用变长编码。 encoding 元素类型编码 data 元素内容 a. prevlen 为了节省内存，根据上一个节点的长度，prelen可以将其分为两类： entry的前8位小于254，则这8位就表示上一个节点的长度 entry的前8位等于254，则意味着上一个节点的长度无法用8位表示，后面32位才是真实的prevlen。 不用255是因为255已经被zlend用作分界值，用于判断ziplist是否到达尾部。 b. encoding + data 根据当前节点存储的数据类型及长度，可以将entry节点分为9个类型，其中整数节点分为6类，字符串节点分为3类。 整数节点（6类）： 整数节点的encoding的长度为8位，其中高2位用来区分整数节点和字符串节点（高2位为11时是整数节点），低6位用来区分整数节点的类型： 值得注意的是 最后一种encoding是存储整数012的节点的encoding，它没有额外的data部分，encoding的高4位表示这个类型，低4位就是它的data。这种类型的节点的encoding大小介于ZIP_INT_24B与ZIP_INT_8B之间（113），但是为了表示整数0，取出低四位xxxx之后会将其-1作为实际的data值（0~12）。在函数zipLoadInteger中，我们可以看到这种类型节点的取值方法： ...... } else if (encoding &gt;= ZIP_INT_IMM_MIN &amp;&amp; encoding &lt;= ZIP_INT_IMM_MAX) { ret = (encoding &amp; ZIP_INT_IMM_MASK)-1; } ...... 字符串节点（3类）： 当data小于63字节时(2^6)，节点存为上图的第一种类型，高2位为00，低6位表示data的长度。 当data小于16383字节时(2^14)，节点存为上图的第二种类型，高2位为01，后续14位表示data的长度。 当data小于4294967296字节时(2^32)，节点存为上图的第二种类型，高2位为10，下一字节起连续32位表示data的长度。 注意： 不同于整数节点encoding永远是8位，字符串节点的encoding可以有8位、16位、40位三种长度。 相同encoding类型的整数节点 data长度是固定的，但是相同encoding类型的字符串节点，data长度取决于encoding后半部分的值。 总结 ziplist是为节省内存空间而生的。 ziplist是一个为Redis专门提供的底层数据结构之一，本身可以有序也可以无序。当作为hash的底层实现时，节点之间没有顺序；当作为zset的底层实现时，节点之间会按照大小顺序排列。 ","link":"http://mofish.pily.life/post/data_structure_04/"},{"title":"数据结构学习之路(三)：SDS简单动态字符串","content":"👉简单动态字符串SDS是Redis中String类型的底层数据结构 👉Redis的实现方法并没用采用传统的C语言中的字符串表示，而是通过改进后自己定义了一种叫做简单动态字符串（simple dynamic string, 简称SDS）的抽象类型。 此外SDS在兼容C语言标准字符串极其处理函数外，还在此基础上保证了二进制安全，下面我们将详细的介绍SDS的实现。 PS❗️：二进制安全通俗地讲，C 语言中，用 &quot;\\0&quot; 表示字符串的结束， 如果字符串中本身就有 &quot;\\0&quot; 字符，字符串就会被截断，即非二进制安全；若通过某种机制，保证读写字符串时不损害其内容，则是二进制安全。 数据结构 在Redis中，SDS根据字符串长短定义了5种数据接口，分别叫 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64，其结构体如下。 sds.h //定义了一个char 指针 typedef char *sds; /* Note: sdshdr5 is never used, we just access the flags byte directly. * However is here to document the layout of type 5 SDS strings. */ struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 低三位存类型, 高五位存长度 */ char buf[]; /* 柔性数组，存放实际内容 */ }; struct __attribute__ ((__packed__)) sdshdr8 { //buf 已经使用的长度 uint8_t len; /* 已使用长度，用 1 字节存 */ //buf 分配的长度，等于buf[]的总长度-1，因为buf有包括一个/0的结束符 uint8_t alloc; /* 总长度，用 1 字节存 */ //只有3位有效位，因为类型的表示就是0到4，所有这个8位的flags 有5位没有被用到 unsigned char flags; /* 低三位存类型, 高五位预留 */ //实际的字符串存在这里 char buf[]; /* 柔性数组，存放实际内容 */ }; // 与上面的变化只有len和alloc， 就是长度不同而已 struct __attribute__ ((__packed__)) sdshdr16 { uint16_t len; /* 已使用长度，用 2 字节存 */ uint16_t alloc; /* 总长度，用 2 字节存 */ unsigned char flags; /* 低三位存类型, 高五位预留 */ char buf[]; }; // 与上面的变化只有len和alloc， 就是长度不同而已 struct __attribute__ ((__packed__)) sdshdr32 { uint32_t len; /* 已使用长度，用 4 字节存 */ uint32_t alloc; /* 总长度，用 4 字节存 */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; // 与上面的变化只有len和alloc， 就是长度不同而已 struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* 已使用长度，用 8 字节存 */ uint64_t alloc; /* 总长度，用 8 字节存 */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; 这五种结构体中的字段定义如下👇： len：表示 buf 中已占用的字节数； alloc：表示 buf 中已分配的字节数，记录的是 buf 分配的总长度； flags：标识当前结构体的类型，低三位存类型, 高五位预留； buf：柔性数组，真正存储字符串的数据空间。 之所以分成这个多不同类型的结构体，其优点是🤙🤙 不同长度的字符串用不同的的类型表示，节约内存 有单独的统计变量len和alloc，可以很方便的得到字符串长度 SDS 对上层暴露的指针是直接指向柔性数组 buf 的，兼容 C 语言处理字符串的各种函数 由于有长度统计变量 len 的存在，读写字符串时不依赖 &quot;\\0&quot; 终止符，保证了二进制安全 下面来详细看一下这几种 sdshdr 的结构，sdshdr8、sdshdr16、sdshdr32 和 sdshdr64 的结构相同 sdshdr16结构如下所示： sdshdr5与上述的不同，做了进一步的压缩，用 flags 中预留的 5 位存长度，把 len 和 alloc 也省掉了，其结构如下所示： 在 Redis 的源代码中，对类型的宏定义如下： #define SDS_TYPE_5 0 #define SDS_TYPE_8 1 #define SDS_TYPE_16 2 #define SDS_TYPE_32 3 #define SDS_TYPE_64 4 源码中的 attribute((packed)) 需要重点关注。一般情况下，结构体会按其所有变量大小的最小公倍数做字节对齐，而用 packed 修饰后，结构体则变为按 1 字节对齐。 以 sdshdr32 为例，修饰前按 4 字节对齐，大小为 12 字节；修饰后按 1 字节对齐，注意 buf 是个 char 类型的柔性数组，地址连续，始终在 flags 之后。packed 修饰前后示意图如下所示： 这样的好处有两个： 1️⃣节省内存，例如 sdshdr32 可节省 3 个字节； 2️⃣SDS 返回给上层的，不是结构体首地址，而是指向内容的 buf 指针 s。修饰后，无论是 sdshdr8、sdshdr16 还是 sdshdr32，都能通过 s[-1] 快速找到 flags，因为此时按 1 字节对齐。若没有 packed 的修饰，还需要对不同结构做处理，实现更复杂。 两大特点 空间预分配 字符串拼接是字符串中最常用的操作，在SDS中具体的函数是sdscatsds： sds sdscatsds(sds s, const sds t) { return sdscatlen(s, t, sdslen(t)); } 但是sdscatsds只是暴露给上层的方法，其最终调用的函数是sdscatlen： /* 将指针 t 的内容和指针 s 的内容拼在一起，该操作是二进制安全的 */ sds sdscatlen(sds s, const void *t, size_t len) { size_t curlen = sdslen(s); s = sdsMakeRoomFor(s,len); // 扩容检查，如需扩容则返回扩容好的新的字符串 if (s == NULL) return NULL; memcpy(s+curlen, t, len); // 直接拼接，保证了二进制安全 sdssetlen(s, curlen+len); s[curlen+len] = '\\0'; // 加上结束符以保持兼容 return s; } 由于其中可能涉及 SDS 的扩容，sdscatlen 中调用 sdsMakeRoomFor 对待拼接的字符串 s 容量做了检查，若无须扩容则直接返回 s；若需要扩容，返回的则是扩容好的新字符串。 容量计算规则 ▪️ 扩容后长度小于 1MB：按新长度的 2 倍扩容； ▪️ 扩容后长度超过 1MB：按新长度加上 1MB 扩容。 扩容对象的选择 ▪️ SDS 类型未改变：通过 realloc 扩大柔性数组； ▪️ SDS 类型发生变化：开辟新内存，拼接完内容后，释放旧指针。 ❗️❗️通过空间预分配策略， Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。 惰性空间释放 Redis 通过 sdsfree 函数释放字符串占的内存，该方法通过对 s 的偏移，可定位到 SDS 结构的首部，取出长度 len，然后调用 s_free 进行内存的释放： void sdsfree(sds s) { if (s == NULL) return; s_free((char*)s-sdsHdrSize(s[-1])); // 定位到 SDS 首部，直接释放内存 } ❗️❗️为了优化性能（减少申请内存的开销），SDS 提供了不直接释放内存，而是通过重置统计值达到清空目的方法：sdsclear： void sdsclear(sds s) { sdssetlen(s, 0); // 统计值 len 清零 s[0] = '\\0'; // 清空 buf } 该方法仅将 SDS 的 len 归零，此处已存在的 buf 并没有真正被清除，新的数据可以覆盖写，而不用重新申请内存。 ","link":"http://mofish.pily.life/post/data_structure_03/"},{"title":"数据结构学习之路(二)：哈希表","content":"👏 哈希表也叫散列表，这种数据结构提供了Key-Value 的映射关系，只要给出一个 Key ，就可以高效查找到它所匹配的 Value ，时间复杂度接近于 O(1)。 👻 其实哈希表是基于数组的一种存储方式，它主要由哈希函数和数组构成，当要存储一个数据的时候，首先用一个函数计算数据的地址，然后再将数据存进指定地址位置的数组里面。这个函数就是哈希函数，而这个数组就是哈希表。 哈希表的优势 相比于简单的数组以及链表，它能够根据元素本身在第一时间，也就是时间复杂度为0（1）内找到该元素的位置。这使得它在查询和删除、插入上会比数组和链表要快很多。 时间复杂度在平均情况下，搜索、插入、删除都是O(1)；但在最差情况下，即有哈希冲突时，会退化成O(n) 常见哈希算法 直接定址法 取关键字或关键字的某个线性函数值为散列地址。 即 f(key) = key 或 f(key) = a * key + b，其中 a 和 b 为常数。 除留余数法 取关键字被某个不大于散列表长度 m 的数 p 求余，得到的作为散列地址 即 f(key) = key % p，p &lt; m，这是最为常见的一种哈希算法。 数学分析法 当关键字的位数大于地址的位数，对关键字的各位分布进行分析，选出分布均匀的任意几位作为散列地址。 仅适用于所有关键字都已知的情况下，根据实际应用确定要选取的部分，尽量避免发生冲突。 平方取中法 先计算出关键字值的平方，然后取平方值中间几位作为散列地址。 随机分布的关键字，得到的散列地址也是随机分布的。 随机数法 选择一个随机函数，把关键字的随机函数值作为它的哈希值。 通常当关键字的长度不等时用这种方法。 什么是哈希冲突 简单的说哈希冲突就是指哈希函数算出来的地址被别的元素占用了，而好的哈希函数会尽量避免哈希冲突。 解决哈希冲突的办法有开放定址法（发生冲突，继续寻找下一块未被占用的存储地址），再散列函数法，链地址法。 哈希冲突的解决方法 开放寻址法 开发地址法的做法是，当冲突发生时，使用某种探测算法在散列表中寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到。按照探测序列的方法，一般将开放地址法区分为线性探查法、二次探查法、双重散列法等。 这里为了更好的展示三种方法的效果，我们用以一个模为8的哈希表为例，采用除留余数法，往表中插入三个关键字分别为26，35，36的记录，分别除8取模后，在表中的位置如下： 这个时候插入42，那么正常应该在地址为2的位置里，但因为关键字30已经占据了位置，所以就需要解决这个地址冲突的情况，接下来就介绍三种探测方法的原理，并展示效果图。 1). 线性探查法 函数：fi = (f(key) + i) % m，0 &lt;= i &lt;= m-1 解释：探查时从地址 d 开始，首先探查 T[d]，然后依次探查 T[d+1]，…，直到 T[m-1]，此后又循环到 T[0]，T[1]，…，直到探查到有空余的地址或者到 T[d-1]为止。 效果：插入42时，探查到地址2的位置已经被占据，接着下一个地址3，地址4，直到空位置的地址5，所以39应放入地址为5的位置。 缺点：需要不断处理冲突，无论是存入还是査找效率都会大大降低。 2). 二次探查法 函数：fi = (f(key) + di) % m，0 &lt;= i &lt;= m-1 解释：探查时从地址 d 开始，首先探查 T[d]，然后依次探查 T[d+di]，di 为增量序列12，22，32，……，q2， 且q≤1/2 (m-1)，直到探查到 有空余地址或者到 T[d-1]为止。 效果：所以插入42时，探查到地址2被占据，就会探查T[2+12]也就是地址3的位置，被占据后接着探查到地址T[2+12+22]=T[2+1+4]=7，然后插入。 缺点：无法探查到整个散列空间。 3). 双重散列法等 函数：fi = (f(key) + i*g(key)) % m，1 &lt;= i &lt;= m-1，其中 f(key) 和 g(key) 是两个不同的哈希函数，m 为哈希表的长度。 解释：双哈希函数探测法，先用第一个函数 f(key) 对关键码计算哈希地址，一旦产生地址冲突，再用第二个函数 g(key) 确定移动的步长因子，最后通过步长因子序列由探测函数寻找空的哈希地址。 效果：比如，f(key)=a 时产生地址冲突，就计算g(key)=b，则探测的地址序列为 f1=(a+b) mod m，f2=(a+2b) mod m，……，fm-1=(a+(m-1)b) % m，假设 b 为 3，那么关键字42应放在 “5” 的位置。 链地址法 前面我们谈到了散列冲突处理的开放定址法，它的思路就是一旦发生了冲突，就去寻找下一个空的散列地址。那么，有冲突就非要换地方呢，我们直接就在原地处理行不行呢？ 可以的，于是我们就有了链地址法，即将哈希值相同的数值链接在同一个链表中。 链地址法的优势： a. 链地址法处理冲突简单，且无堆积现象，即非同义词决不会发生冲突，因此平均查找长度较短； b. 链地址法中各链表上的结点空间是动态申请 c. 开放定址法为减少冲突，要求装填因子α较小，故当结点规模较大时会浪费很多空间。 而 链地址法中可取α≥1，且结点较大时， 链地址法中增加的指针域可忽略不计，因此节省空间； d. 在用链地址法构造的散列表中，删除结点的操作易于实现。只要简单地删去链表上相应的结点即可。 而对开放地址法构造的散列表，删除结点不能简单地将被删，只能将结点的空间置为空，否则将截 断在它之后填人散列表的同义词结点的查找路径。 链地址法的缺点： a. 指针需要额外的空间（也可忽略不计），故当结点规模较小时，开放定址法较为节省空间， 而若将节省的指针空间用来扩大散列表的规模，可使装填因子变小， 这又减少了开放定址法中的冲突，从而提高平均查找速度。 再哈希法 再哈希法其实很简单，就是再使用哈希函数去散列一个输入的时候，输出是同一个位置就再次哈希，直至不发生冲突位置 缺点：每次冲突都要重新哈希，计算时间增加。 Redis的数据底层原理的相关知识点 Redis中，Hash、ZSet和Set三种数据结构在满足一定条件下使用的底层结构都是字典(dict)，而字典数据结构的精华就是 hashtable，跟Java的HashMap几乎是一样的，都是通过分桶的方式解决hash冲突，即链地址法。 ZSet比较特殊，当满足一定条件下，它使用的是一个复合结构，一方面它需要一个hash结构来存储value和score的对应关系，另一方面需要提供按照score排序的功能，还需要能够指定score的范围来获取value列表的功能，所有就需要另外一个结构”跳跃列表“ struct zset { dict *dict; // all values value =&gt; score zskiplist *zsl; } Redis里面Set的结构底层实现也是字典，只不过所有的value都是Null 第一维是数组，第二维是链表，数组中存储的是第二维链表的第一个元素指针： 渐进式rehash 字典结构内部包含两个 hashtable，通常情况下只有一个 hashtable 是有值的，但是在字典扩容时，需要分配新的 hashtable，然后进行渐进式搬迁。 这时候两个 hashtable 存储的分别是新旧的 hashtable，待搬迁结束后，旧的会被删除，新的则取而代之。 为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0]里面的所有键值对全部 rehash 到 ht[1]， 而是分多次、渐进式地将 ht[0]里面的键值对慢慢地 rehash 到 ht[1]。 步骤： 为ht[1]分配空间，让dict字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash工作正式开始。 在rehash进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在 rehashidx索引(table[rehashidx]桶上的链表)上的所有键值对rehash到ht[1]上，当rehash工作完成之后，将rehashidx属性的值增一，表示下一次要迁移链表所在桶的位置。 随着字典操作的不断执行，最终在某个时间点上，ht[0]的所有桶对应的键值对都会被rehash至ht[1]，这时程序将rehashidx属性的值设为-1，表示rehash操作已完成。 注意点： （1）删除和查找：在进行渐进式rehash的过程中，字典会同时使用ht[0]和ht[1]两个哈希表，所以在渐进式rehash进行期间，字典的删除、查找、更新等操作会在两个哈希表上进行。比如说，要在字典里面查找一个键的话，程序会先在ht[0]里面进行查找，如果没找到的话，就会继续到ht[1]里面进行查找，诸如此类。 （2）新增数据：在渐进式 rehash 执行期间，新添加到字典的键值对一律会被保存到ht[1]里面，而ht[0]则不再进行任何添加操作。这一措施保证了ht[0]包含的键值对数量会只减不增，并随着rehash操作的执行而最终变成空表。 扩容和缩容条件 扩容 正常情况下，当hash表中的元素个数等于第一维数组的长度时，就会开始扩容，扩容的新数组是原数组的大小的2倍。 不过如果Redis正在做bgsave，为了减少内存页的过多分离（Copy On Write），Redis尽量不去扩容（dict_can_resize）。但是如果元素个数达到第一维数组长度的5倍（dict_force_resize_ratio），就会执行强制扩容。 缩容 当hash表因为元素被删除主键变得稀疏时，Redis会对其进行缩容来减少 hash 表第一维数组的空间占用。缩容的条件是元素个数低于数组长度的10%。缩容不会考虑Redis是否正在做bgsave。 扩容时考虑 BGSAVE 是因为，扩容需要申请额外的很多内存，且会重新链接链表（如果会冲突的话）, 这样会造成很多内存碎片，也会占用更多的内存，造成系统的压力。 而缩容过程中，由于申请的内存比较小，同时会释放掉一些已经使用的内存，不会增大系统的压力。因此不用考虑是否在进行 BGSAVE 操作。 ","link":"http://mofish.pily.life/post/data_structure_02/"},{"title":"数据结构学习之路(一)：数据结构基础","content":"💻 在复习 Redis 的同时，数据结构也要提上日程了，该来还是得来得，以前拉下的最终还是得偿还的😭 👉 所以接下来就陆续记录一下数据结构的一下复习知识点吧，包括各种二叉树、B树、跳跃表等等的！！ 数据结构基础 常用的数据结构又很多，但大多数都以数组或链表作为存储方式，数组和链表可以被看作数据存储的“物理结构” 那什么是数据存储的“物理结构”呢？ 如果把数据接口比作活生生的人，那么物理结构就是人的血肉和骨骼，看得见摸得着，实实在在存在。 如果物质层面的人体比作数据存储的物理结构，那么精神层面的人格则是数据存储的逻辑结构，它是抽象的概念，是依赖于物理结构的存在。 线性结构 非线性结构 逻辑结构 顺序表、栈、队列 树图 顺序存储结构 链式存储结构 物理结构 数组 链表 我们常用的两种数据结构：栈和队列，这两者都属于逻辑结构，它们的物理实现既可以利用数组，也可以利用链表来完成。 而后面学到的二叉树，也是一种逻辑结构，同样地，二叉树也可以依托物理上的数组或链表来实现。 数组 数组是由有限个相同类型的变量所组成的有序集合，它的物理存储方式是顺序存储，访问方式是随机访问。 利用下标查找数组元素的时间复杂度是O(1),中间插入、删除数组元素的时间复杂度是O(n)。 注意：PHP 的数组是一种非常强大灵活的数据类型，我们可以使用 PHP 中的数组轻易的实现集合、栈、列表、字典等多种数据结构。 链表 链表是一种链式数据结构，由若干节点组成，每个节点包含指向下一个节点的指针。 链表的物理存储方式是随机存储，访问方式是顺序访问。 查找链表节点的时间复杂度是O(n)，中间插入、删除节点的时间复杂度是O(1)。 栈 栈是一个线性逻辑结构，可以用数组或者链表实现，其包含入栈和出栈操作，遵循先入后出的原则(FILO)。 队列 队列也是一种线性逻辑结构，也是可以用数组或者链表实现，其包含入队和出队操作，遵循先入先出的原则(FIFO)。 哈希表 哈希表也叫散列表，是存储 Key-Value 映射的集合。对于某一个 Key，哈希表可以在接近O(1)的时间内进行读写操作。 哈希表通过哈希函数实现 Key 和数组下标的转换，通过开发寻址法和链表法来解决哈希冲突。 ","link":"http://mofish.pily.life/post/data_structure_01/"},{"title":"Redis学习之路(七)：Scan命令","content":"😵 在平时线上 Redis 维护工作中，有时候需要从 Redis 实例的成千上万个 key 找到特定前缀的 key 列表来手动处理数据。 🤔 除了简单粗暴的 keys 命令，还有什么方法可以找出满足特定前缀的 key 列表呢？ 目录 大海捞针scan scan的基本用法 字典的结构 scan遍历顺序 字典扩容 对比扩容、缩容前后的遍历顺序 渐进式Rehash 更多的scan命令 大key扫描 大海捞针scan Redis 提供了一个简单粗暴的指令 keys 用来列出所有满足特定正则字符串规则的 key： 127.0.0.1:6379&gt; keys * 1) &quot;name&quot; 127.0.0.1:6379&gt; keys n*me 1) &quot;name&quot; 127.0.0.1:6379&gt; keys nam* 1) &quot;name&quot; 这个指令非常简单，只需提供一个简单的正则字符串即可，但是有两个很明显的缺点： 没有offset、limit参数，一次性查出所有满足条件的key，万一实例中有几百万个 key ，那屏幕就一直刷屏了； keys 算法是遍历算法，复杂度是 O(n) ,如果实例中有百、千万级别以上的 key，这个指令就会导致 Redis 服务卡顿，所有读写 Redis 的其它指令都会被延后甚至会超时报错， 因为 Redis 是单线程程序，顺序执行所有指令， 其它指令必须等到当前的 keys 指令执行完了才可以继续。 面对这两个显著的缺点该怎么办呢？ Redis 为了解决这个问题，在 2.8 版本加入了scan指令，相较于keys 具备了以下特点： 复杂度虽然也是 O(n) ，但它是通过游标分布进行的，不会阻塞线程； 提供 limit 参数，可以控制每次返回结果的最大条数，limit 只是一个 hint，返回的结果可多可少； 同 keys 一样，它也提供模式匹配功能； 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数； 返回的结果可能会有重复，需要客户端去重，这点非常重要； 遍历的过程中如果有数据修改，改动厚的数据能不能遍历到是不确定的； 单次返回的结果是空并不意味着遍历结束，需要看返回的游标值是否为零。 scan的基本用法 在使用之前，先让 Redis 插入 10000 条测试数据。 for ($i=0; $i &lt; 10000; $i++) { RedisTools::set('keys' . $i, $i); } 现在 Redis 中现有10000条数据，接下来我们找出已 keys99 开头的 key 列表。 scan 提供了三个参数，第一个是 coursor 整数值，第二个是 key 的正则模式，第三个是遍历的 limit hint。 第一次遍历时， cursor 值为0，然后将返回结果中第一个整数值作为下一次遍历的 cursor ，一直遍历到返回的 cursor 值为0时结束。 127.0.0.1:6379&gt; scan 0 match hlytms:keys99* count 1000 1) &quot;13720&quot; 2) 1) &quot;hlytms:keys9904&quot; 2) &quot;hlytms:keys9942&quot; 3) &quot;hlytms:keys9918&quot; 4) &quot;hlytms:keys9927&quot; 5) &quot;hlytms:keys9920&quot; 6) &quot;hlytms:keys9905&quot; 7) &quot;hlytms:keys9966&quot; 8) &quot;hlytms:keys9936&quot; 9) &quot;hlytms:keys9982&quot; 10) &quot;hlytms:keys9906&quot; 11) &quot;hlytms:keys9943&quot; 127.0.0.1:6379&gt; scan 13720 match hlytms:keys99* count 1000 1) &quot;5324&quot; 2) 1) &quot;hlytms:keys9968&quot; 2) &quot;hlytms:keys9974&quot; 3) &quot;hlytms:keys9934&quot; 4) &quot;hlytms:keys9994&quot; 5) &quot;hlytms:keys9997&quot; 6) &quot;hlytms:keys9969&quot; 7) &quot;hlytms:keys9978&quot; 8) &quot;hlytms:keys9973&quot; 9) &quot;hlytms:keys9964&quot; 10) &quot;hlytms:keys9944&quot; 11) &quot;hlytms:keys9947&quot; 12) &quot;hlytms:keys9900&quot; 127.0.0.1:6379&gt; scan 5324 match hlytms:keys99* count 1000 1) &quot;15282&quot; 2) 1) &quot;hlytms:keys9935&quot; 2) &quot;hlytms:keys9976&quot; 3) &quot;hlytms:keys9913&quot; 4) &quot;hlytms:keys9902&quot; 5) &quot;hlytms:keys993&quot; 6) &quot;hlytms:keys997&quot; 7) &quot;hlytms:keys9977&quot; 8) &quot;hlytms:keys9972&quot; 9) &quot;hlytms:keys9970&quot; 10) &quot;hlytms:keys9990&quot; 11) &quot;hlytms:keys9961&quot; 127.0.0.1:6379&gt; scan 15282 match hlytms:keys99* count 1000 1) &quot;8550&quot; 2) 1) &quot;hlytms:keys991&quot; 2) &quot;hlytms:keys9957&quot; 3) &quot;hlytms:keys9950&quot; 4) &quot;hlytms:keys9992&quot; 5) &quot;hlytms:keys9998&quot; 6) &quot;hlytms:keys9987&quot; 7) &quot;hlytms:keys9925&quot; 8) &quot;hlytms:keys9995&quot; 9) &quot;hlytms:keys9965&quot; 从上面的过程可以看出，虽然提供的 limit 是1000,但是返回的结果却只有10个左右。因为这个 limit 不是限定返回结果的数量，而是限定服务器单次遍历的字典槽数量(约等于)。 如果将 limit 设置为10,你会发现返回结果是空的，但是游标值不为0，意味着遍历还没结束。 127.0.0.1:6379&gt; scan 15282 match hlytms:keys99* count 10 1) &quot;6258&quot; 2) (empty list or set) 字典的结构 在 Redis 中所有的 key 都存储再一个很大的字典中，如下图所示，它是一维数组 + 二维链表的结构。 第一维数组的大小总是2n2^{n}2n(n &gt;= 0)，扩容一次数组，大小空间加倍，也就是2n+12^{n+1}2n+1。 scan 指令返回的游标就是第一维数组的位置索引，我们将这个位置索引称为槽（slot）。 如果不考虑字典的扩容缩容，直接按数组下标挨个遍历就行了。limit 参数就表示需要遍历的槽位数，之所以返回的结果可能多可能少，是因为不是所有的槽位上都会挂接链表，有些槽位可能是空的，还有些槽位商挂接的链表上的元素可能会有多个。 每一次遍历都会将 limit 数量的槽位上挂接的所有链表元素进行模式匹配过滤后，一次性返回给客户端。 scan遍历顺序 scan 的遍历顺序非常特别。它不是从第一维数组的第0位一直遍历到末尾，而是采用了高位进位加法来遍历。 之所以使用这样特殊的方式进行遍历，是考虑到字典的扩容和缩容时避免槽位的遍历重复和遗漏。 首先我们看下图，该图呈现了普通加法和高位进位加法的区别： 从图中可以看出高位进位加法从左边加，进位往右边移动，同普通加法正好相反。但是最终它们都会遍历所有的槽位并且没有重复。 字典扩容 HashMap有扩容的概念，当 LoadFactor 达到阀值时，需要重新分配一个新的2倍大小的数组，然后将所有元素全部 rehash 挂到新的数组下面。 rehash 就是讲元素的 hash 值对数组长度进行取模运算，因为长度变了，所以每个元素接的槽位可能也会发生变化。又因为数组的长度是2的n次方，所以取模运算等价于位与操作。 a mod 8 = a &amp; (8 - 1) = a &amp; 7 a mod 16 = a &amp; (16 - 1) = a &amp; 15 a mod 32 = a &amp; (32 - 1) = a &amp; 31 这里的7、15、31称为字典的 mask 值，mask 的作用就是保留 hash 值的低位，高位都被设置为 0 。 接下来我们看看 rehash 前后元素的槽位的变化。 假设当前的字典的数组长度由 8 位扩展到 16 位，那么 3 号槽位的 011 将会被 rehash 到 3 号槽位和11 号槽位，也就是说该槽位链表中大约有一半的元素还是 3 号槽位，其它的元素会放到 11 号槽位，11 这个数字的二进制是 1011,就是对 3 的二进制 011 增加了一个高位 1。 抽象一点说，假设开始槽位的二进制数是 xxx ，那么该槽位中的元素将被 rehash 到 0xxx 和 1xxx (xxx + 1)中。 如果字典长度由 16 位扩容到 32 位，那么对于二进制槽位 xxxx 中的元素将被 rehash 到 0xxxx 和 1xxxx (xxxx + 16) 中。 对比扩容、缩容前后的遍历顺序 观察下图，我们会发现采用高位进位加法的遍历顺序， rehash 后的槽位再遍历顺序上是相邻的。 假设当前要遍历 110 这个位置，那么扩容后，当前槽位上所有的元素对应的新槽位是 0110 和 1110，这时我们可以直接从 0110 这个槽位开始往后继续遍历， 0110 槽位之前的所有槽位都是已经遍历过了，这样就可以避免扩容后对已经遍历过的槽位进行重复遍历。 另外对于缩容来说，假设当前即将遍历 110 这个位置，那么缩容后，当前槽位的有元素对应的新槽位是 10，也就是去掉槽位二进制的最高位。这时我们可以从 10 这个槽位继续往后遍历，其之前的所有槽位已经遍历过了，可避免重复遍历。 但是！！！缩容有个不太一样的地方，他会对 010 这个槽位上的元素进行重复遍历，因为缩容后 10 槽位的元素是 010 和 110 上挂接的元素的融合。 渐进式Rehash Redis 在 HashMap 扩容时采用了“渐进式rehash”的方式，主要是为了解决扩容时如果元素特别多，那么一次性将旧数组下挂接的元素全部转移到新数组下面，导致线程出现卡顿的现象。 “渐进式rehash” 它会同时保留旧数组和新数组，然后在定时任务中以后后续对 hash 的指令操作中渐渐地间旧数组中挂接的元素迁移到新数组商。 这意味着要操作处于 rehash 中的字典，需要同时访问新旧两个数组结构。如果旧数组下面找不到元素，还需要去新数组下面寻找。 scan 也需要考虑这个问题，对于 rehash 中的字典，它需要同时扫描新旧槽位，然后将结果融合后返回给客户端。 更多的scan命令 scan 指令是一系列指令，除了可以遍历所有的 key 之外，还可以对指定的容器集合进行遍历。比如 zscan 遍历 zset 集合元素， hscan 遍历 hash 字典的元素， sscan遍历 set 集合的元素。 它们的原理同 scan 类似，因为 hash 底层就是字典， set 也是一个特殊的 hash （所有的 value 指向同一个元素） , zset 内部也使用了字典来存储所有的元素内容，所以这里不再赘述。 大key扫描 在日常开发中，如果一个 key 太大，那么无论是迁移、扩容、删除时可能都会造成卡顿现象，因此在平时的业务开发中，尽量避免大 key 的产生。 那么如何定位大 key 呢？我们需要用到 scan 指令。 redis-cli -h 127.0.1 -p 6379 --bigkeys 如果你担心这个指令会大幅度提升 Redis 的 ops 导致线上报警，还可以增加一个休眠参数。 redis-cli -h 127.0.0.1 -p 6379 --bigkeys -i 0.1 该指令每隔 100 条 sacn 指令就会休眠 0.1s，ops 就不会剧烈抬升，但是扫描时间会变长。 ","link":"http://mofish.pily.life/post/redis_learning_07/"},{"title":"浅谈控制反转与依赖注入（已迁移）","content":"😵 控制反转和依赖注入这两个东西，究竟是啥玩意，有时候你看了很多文章还是一头雾水，似懂非懂的感觉！ 😝 最近在知乎看到一个挺生动的描述，这个转载记录一下。 第一章：小明和他的手机 从前有个人叫小明 小明有三大爱好，抽烟，喝酒…… 咳咳，不好意思，走错片场了。应该是逛知乎、玩王者农药和抢微信红包 我们用一段简单的伪代码，来制造一个这样的小明 class Ming extends Person { private $_name; private $_age; function read() { //逛知乎 } function play() { //玩农药 } function grab() { //抢红包 } } 但是，小明作为一个人类，没有办法仅靠自己就能实现以上的功能，他必须依赖一部手机，所以他买了一台iphone6，接下来我们来制造一个iphone6 class iPhone6 extends Iphone { function read($user=&quot;某人&quot;) { echo $user.&quot;打开了知乎然后编了一个故事 \\n&quot;; } function play($user=&quot;某人&quot;) { echo $user.&quot;打开了王者农药并送起了人头 \\n&quot;; } function grab($user=&quot;某人&quot;) { echo $user.&quot;开始抢红包却只抢不发 \\n&quot;; } } 小明非常珍惜自己的新手机，每天把它牢牢控制在手心里，所以小明变成了这个样子 class Ming extends Person { private $_name; private $_age; public function __construct() { $this-&gt;_name = '小明'; $this-&gt;_age = 26; } function read() { //…… 省略若干代码 (new iPhone6())-&gt;read($this-&gt;_name); //逛知乎 } function play() { //…… 省略若干代码 (new iPhone6())-&gt;play($this-&gt;_name);//玩农药 } function grab() { //…… 省略若干代码 (new iPhone6())-&gt;grab($this-&gt;_name);//抢红包 } } 今天是周六，小明不用上班，于是他起床，并依次逛起了知乎，玩王者农药，并抢了个红包。 $ming = new Ming(); //小明起床 $ming-&gt;read(); $ming-&gt;play(); $ming-&gt;grab(); 这个时候，我们可以在命令行里看到输出如下 小明打开了知乎然后编了一个故事 小明打开了王者农药并送起了人头 小明开始抢红包却只抢不发 这一天，小明过得很充实，他觉得自己是世界上最幸福的人。 第二章： 小明的快乐与忧伤 小明和他的手机曾一起度过了一段美好的时光，一到空闲时刻，他就抱着手机，逛知乎，刷微博，玩游戏，他觉得自己根本不需要女朋友，只要有手机在身边，就满足了。 可谁能想到，一次次地系统更新彻底打碎了他的梦想，他的手机变得越来越卡顿，电池的使用寿命也越来越短，一直到某一天的寒风中，他的手机终于耐不住寒冷，头也不回地关了机。 小明很忧伤，他意识到，自己要换手机了。 为了能获得更好的使用体验，小明一咬牙，剁手了一台iphoneX，这部手机铃声很大，电量很足，还能双卡双待，小明很喜欢，但是他遇到一个问题，就是他之前过度依赖了原来那一部iPhone6，他们之间已经深深耦合在一起了，如果要换手机，他就要拿起刀来改造自己，把自己体内所有方法中的iphone6 都换成 iphoneX。 经历了漫长的改造过程，小明终于把代码中的 iphone6 全部换成了 iphoneX。虽然很辛苦，但是小明觉得他是快乐的。 于是小明开开心心地带着手机去上班了，并在回来的路上被小偷偷走了。为了应急，小明只好重新使用那部刚刚被遗弃的iphone6，但是一想到那漫长的改造过程，小明的心里就说不出的委屈，他觉得自己过于依赖手机了，为什么每次手机出什么问题他都要去改造他自己，这不仅仅是过度耦合，简直是本末倒置，他向天空大喊，我不要再控制我的手机了。 天空中的造物主，也就是作为程序员的我，听到了他的呐喊，我告诉他，你不用再控制你的手机了，交给我来管理，把控制权交给我。这就叫做控制反转。 第三章：造物主的智慧 小明听到了我的话，他既高兴，又有一点害怕，他跪下来磕了几个头，虔诚地说到：“原来您就是传说中的造物主，巴格梅克上神。我听到您刚刚说了 控制反转 四个字，就是把手机的控制权从我的手里交给你，但这只是您的想法，是一种思想罢了，要用什么办法才能实现控制反转，又可以让我继续使用手机呢？” “呵“，身为造物主的我在表现完不屑以后，扔下了四个大字，“依赖注入！” 接下来，伟大的我开始对小明进行惨无人道的改造，如下 作者：胡小国 链接：https://zhuanlan.zhihu.com/p/33492169 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 class Ming extends Person { private $_name; private $_age; private $_phone; //将手机作为自己的成员变量 public function __construct($phone) { $this-&gt;_name = '小明'; $this-&gt;_age = 26; $this-&gt;_phone = $phone; echo &quot;小明起床了 \\n&quot;; } function read() { //…… 省略若干代码 $this-&gt;_phone-&gt;read($this-&gt;_name); //逛知乎 } function play() { //…… 省略若干代码 $this-&gt;_phone-&gt;play($this-&gt;_name);//玩农药 } function grab() { //…… 省略若干代码 $this-&gt;_phone-&gt;grab($this-&gt;_name);//抢红包 } } 接下来，我们来模拟运行小明的一天 $phone = new IphoneX(); //创建一个iphoneX的实例 if($phone-&gt;isBroken()){//如果iphone不可用，则使用旧版手机 $phone = new Iphone6(); } $ming = new Ming($phone);//小明不用关心是什么手机，他只要玩就行了。 $ming-&gt;read(); $ming-&gt;play(); $ming-&gt;grab(); 我们先看一下iphoneX 是否可以使用，如果不可以使用，则直接换成iphone6,然后唤醒小明，并把手机塞到他的手里，换句话说，把他所依赖的手机直接注入到他的身上，他不需要关心自己拿的是什么手机，他只要直接使用就可以了。 这就是依赖注入。 第四章：小明的感悟 小明的生活开始变得简单了起来，而他把省出来的时间都用来写笔记了，他在笔记本上这样写到 我曾经有很强的控制欲，过度依赖于我的手机，导致我和手机之间耦合程度太高，只要手机出现一点点问题，我都要改造我自己，这实在是既浪费时间又容易出问题。自从我把控制权交给了造物主，他每天在唤醒我以前，就已经替我选好了手机，我只要按照平时一样玩手机就可以了，根本不用关心是什么手机。即便手机出了问题，也可以由造物主直接搞定，不需要再改造我自己了，我现在买了七部手机，都交给了造物主，每天换一部，美滋滋！ 我也从其中获得了这样的感悟： 如果一个类A 的功能实现需要借助于类B，那么就称类B是类A的依赖，如果在类A的内部去实例化类B，那么两者之间会出现较高的耦合，一旦类B出现了问题，类A也需要进行改造，如果这样的情况较多，每个类之间都有很多依赖，那么就会出现牵一发而动全身的情况，程序会极难维护，并且很容易出现问题。要解决这个问题，就要把A类对B类的控制权抽离出来，交给一个第三方去做，把控制权反转给第三方，就称作控制反转（IOC Inversion Of Control）。控制反转是一种思想，是能够解决问题的一种可能的结果，而依赖注入（Dependency Injection）就是其最典型的实现方法。由第三方（我们称作IOC容器）来控制依赖，把他通过构造函数、属性或者工厂模式等方法，注入到类A内，这样就极大程度的对类A和类B进行了解耦。 第五章 小明的困惑 有一天，小明发现自己在想阅读知乎的时候，读到了这样一行文字。 作者：胡小国 链接：https://zhuanlan.zhihu.com/p/33492169 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 ","link":"http://mofish.pily.life/post/php_knowledge_01/"},{"title":"Redis学习之路(六)：管道Pipeline","content":"⏩ 一直以来，很多同学都对Redis 管道有一个误解，以为这是Redis服务器提供的一种特别的技术，从而可以加速Redis的存取效率，但是实际上并不是。 🆗 这个技术本质上是由客户端提供的，与服务器并有没有什么直接的关系，下面我们来整理一下知识点。 目录 Redis的消息交互 管道简述 压力测试 深入理解管道本质 总结 Redis的消息交互🤣 当我们使用客户端对Redis进行一次操作时，步骤如下： 客户端将请求传送给服务器 服务器处理完毕后 将响应回复给客户端 按照这样描述，每个命令执行的时间 = 客户端发送时间 + 服务器处理和返回时间 + 一个网络来回的时间 其中一个网络来回的时间是不固定的，它的决定因素有很多，不如客户端到服务端要经过多少跳、网络是否拥堵等等。 但是这个时间的量级也是最大的，也就是说一个命令的完成时间长度很多程度商取决于网络开销，即使服务器每秒可处理10万个请求，但是网络开销如果需要250ms时，1s也只能处理4个请求了。 如上图所示：客户端是经历了一读一写一读一写四个操作才完整执行两条指令。 管道简述🤣 为了解决这个问题，Redis在很早就支持了管道技术。也就是说客户端可以一次发送多条命令，不用逐条等待命令的返回值，而是到最后一起读取返回结果，这样就只需要一次网络开销，速度也就得到了明显的提示。 这便是管道操作的本质，服务器根本没有任务区别对待，还是走着接收一条信息、执行一条信息、回复一条信息的正常流程。 客户端则通过对管道中的指令列表改变读写顺序就可以大幅节省IO时间。 Ps：当然，管道中的命令数量并不是越多越好，毕竟该操作会消耗一定内存，太多命令会把内存撑爆。 压力测试🤣 接下来我们实践一下管道的力量，我门使用Redis自带的一个压力测试工具redis-benchmark。 首先我们对一个普通的set指令进行压测，QPS大概10w/s。 ultraman@ultraman-PC:~$ redis-benchmark -t set -q SET: 125628.14 requests per second 接下来我们加个管道选项P参数，它表示单个管道内并行的请求数量，当P=2时，QPS提升到了24w/s。 ultraman@ultraman-PC:~$ redis-benchmark -t set -P 2 -q SET: 240963.86 requests per second 再看看当P=24时，QPS到达了270w/s。 ultraman@ultraman-PC:~$ redis-benchmark -t set -P 24 -q SET: 2777778.00 requests per second 当然，P不是无限大的，当你提升到一直数值时，会发现QPS上不去了，是因为这里CPU处理能力已经到顶了，无法再提升了。 深入理解管道本质🤣 接下来，我们深入分析一下请求交互的流程，以下是完整的请求交互流程图： 这里用文字来仔细描述一遍流程 客户端进程调用 write 将消息写到操作系统内核为套接字分配的发送缓冲区 send buffer 中。 客户端操作系统内核将发送缓冲区的内容发送到网卡，网卡硬件将数据通过“网际路由”送到服务器网卡。 服务器操作系统内核将网卡的数据放到内核为套接字分配的接收缓冲区 recv beffer 中。 服务器进程调用 read 从接收缓冲区中取出消息进行处理。 服务器进程调用 write 将响应消息写到内核为套接字分配的发送缓冲区 send buffer 中。 服务器操作系统内核将发生缓冲区的内容发送到网卡，网卡将数据通过“网际路由”送到客户端网卡。 客户端操作系统内核将网卡的数据放到内核为套接字分配的接收缓冲区 recv buffer 中。 客户端进程调用 read 从接受缓冲区中取出消息返回给上层业务逻辑进行处理。 结束 其中步骤(5)(8)和(1)(4)是一样的，只不过方向返回来而已，一个是请求，一个是响应。 我们开始以为 write 操作是要等到对方收到信息后才返回的，但是实际上 write 操作只负责将数据写到本地操作系统内核的发送缓冲中就立即返回了，剩下的事就交给操作系统内核异步将数据发送到目标机器。 但是如果发送缓冲区满了，那么就需要等待缓冲区空出空闲空间来，这个就是写操作IO操作的真正耗时。 我们开始以为 read 操作是从目前机器拉取数据，但是实际上 read 操作只负责将数据从本地操作系统内核的接受缓冲区中取出来就完事了。 但是如果缓冲区是空的，那么就需要等待数据到来，这个就是 read 操作IO操作的真正耗时。 所以，对于 get 这样一个简单的请求来说，write 操作几乎没有耗时，直接写到发送缓冲区中就返回了，而 read 就比较耗时了，因为它要等待消息经过网络路由到目标机器处理后的响应消息，再回送到当前的内核读缓冲区才可以返回。这才是一个网络来回的真正开销。 而对于管道来说，连续的 write 操作根本没多少耗时，之后第一个 read 操作会等待一个网络的来回开销，然后所有的响应信息就都送回到内核的读缓冲区中了，后续的 read 操作直接就可以从缓冲区中拿到结果，瞬间就返回了。 总结🤣 这就是管道的本质，它并不是服务器的什么特性，而是客户端通过改变了读写的顺序带来的性能的巨大提升。 ","link":"http://mofish.pily.life/post/redis_learning_06/"},{"title":"Redis学习之路(五)：事务和PubSub","content":"😀由于这两个实际用的比较少，这里我们整理一下常见的问题即可，all right？ 一、事务 什么是事务？ 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 Redis事务的概念 Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的，它会将一个事务中的所有命令序列化，然后按顺序执行。 总的来说redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 Redis事务的三个阶段 事务开始 MULTI 命令入队 事务执行EXEC 事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中。 事务管理（ACID）概述 原子性（Atomicity） 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency） 事务前后数据的完整性必须保持一致 隔离性（Isolation） 多事务并发执行时，一个事务的执行不应影响其它事务的执行 持久性（Durability） 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响 Ps：Redis事务总是具有ACID中的一致性和隔离性，另外当其运行再AOF持久化模式下，并且appendsync选项的值为always时，事务也是具有持久性的。 Redis事务保证原子性吗，支持回滚吗？ 当事务中存在一个格式、语法错误的命令入队时，会报错，事务会被取消，所有命令都不会被执行 127.0.0.1:6379&gt; MULTI OK 127.0.0.1:6379&gt; set age 18 QUEUED 127.0.0.1:6379&gt; get (error) ERR wrong number of arguments for 'get' command 127.0.0.1:6379&gt; exec (error) EXECABORT Transaction discarded because of previous errors. 127.0.0.1:6379&gt; get age (nil) 当事务中某个命令语法没有问题，但是执行失败了，并不会影响其它命令的执行，即不具备原子性，不会进行回滚 127.0.0.1:6379&gt; MULTI OK 127.0.0.1:6379&gt; set sex man QUEUED 127.0.0.1:6379&gt; INCR sex QUEUED 127.0.0.1:6379&gt; exec 1) OK 2) (error) ERR value is not an integer or out of range 127.0.0.1:6379&gt; get sex &quot;man&quot; Redis 中的事务为什么不会滚？ 官网给出的解释有三个原因： 作者认为发生事务回滚的原因大部分是程序错误导致的，这种情况一般发生再开发和测试阶段，生产环境很少出现 对于逻辑性错误，比如本来应该自增1的，但是程序却写成自增2，那么这种错误也是无法通过事务回滚的 Redis追求的是简单高效，而传统事务的实现相对比较复杂，这和Redis的设计思想相违背。 二、PubSub(发布/订阅) 基本介绍 前面我们有讲过Redis消息队列的使用方法，但是没有提到Redis消息队列的不足之处，那就是它不支持消息的多播机制。 那什么是消息多播呢？ 消息多播就是允许生产者只生产一次消息，又中间件负责将消息复制到多个消息队列，每个消息队列又相应的消费组进行消费。 如果是普通的消息队列，就得将多个不同的消费组逻辑串联起来放在一个子系统中，进行连续消费。 PubSub 为了支持消息多播，Redis不能再依赖那5中基本数据类型了，它单独使用了一个模块来支持消息多播，这个模块的名字叫作PubSub，也就是发布者/订阅者模式。 下面我们有代码来简单演示一下： &lt;?php //设置php脚本执行时间 set_time_limit(0); //设置socket连接超时时间 ini_set('default_socket_timeout', -1); /** * 消费(订阅)者 * 消费者需要创建redis长连接，并且设置set_time_limit和default_socket_timeout，以确保阻塞获取消息过程php不超时，socket连接不超时 * Class Sub */ class Sub { /** * 连接Redis * DelayQueue constructor. */ public function __construct() { $this-&gt;redis = new Redis(); $this-&gt;redis-&gt;pconnect('127.0.0.1', 6379); $this-&gt;redis-&gt;auth('111111'); } /** * 订阅消息 */ public function subJob() { // 通过回调处理接收的信息，可同时订阅多个频道 $this-&gt;redis-&gt;psubscribe(['codehole'], function ($instance, $channelName, $message){ echo $channelName . '----&gt;' . $message; }); } } $sub = new Sub(); $sub-&gt;subJob(); &lt;?php /** * 生产（发布）者 * Class Pub */ class Pub { /** * 连接Redis * DelayQueue constructor. */ public function __construct() { $this-&gt;redis = new Redis(); $this-&gt;redis-&gt;connect('127.0.0.1', 6379); $this-&gt;redis-&gt;auth('111111'); } /** * 发布消息 * @return int */ public function publish() { return $this-&gt;redis-&gt;publish('codehole', 'hello world'); } } $pub = new Pub(); $pub-&gt;publish(); 注意： 1. 必须先启动消费者，然后再执行生产者 2. 消费者可以启动多个，同时接收同一个频道的消息，可用于不用的业务逻辑处理 PubSub的缺点 PubSub 的生产者传递过来一个消息， Redis 会直接找到相应的消费者传递过去。如果一个消费者都没有，那么消息会被直接丢弃。 如果开始有三个消费者，一个消费者突然挂掉了，生产者会继续发送消息，另外两个消费者可以持续收到消息，但是当挂掉的消费者重新连上的时候，在断连期间生产者发送的消息，对于这个消费者来说就是彻底丢失了。 如果Redis停机重启，PubSub的消息是不会持久化的，毕竟Redis宕机时就相当于一个消费者都没有了，所有的消息都会被直接丢弃。 正是因为这些缺点，所以在消息队列的领域基本找不到合适的使用场景。 注意：但是再Redis5.0的版本，Redis新增了一个叫Stream的数据结构，这个功能给Redis带来了持久化的消息队列功能，下一章节我们再来详细学习一下。 ","link":"http://mofish.pily.life/post/redis_learning_05/"},{"title":"Redis学习之路(四)：简单异步队列","content":"📖 对于异步队列，我们第一时间想到的就是Rabbitmq和Kafka这种消息队列中间件，毕竟这些中间件都是专业的消费队列，但是它们的复杂度和特性之多是需要花一些精力去理解的。 🤡 但是对于只有一组消费者的消息队列，我们可以取舍的使用Redis来作为一种消息队列，下面我们来看一下。 目录 基础知识 异步消息队列List 简单延时队列的实现Zset 注意 基础知识👇👇 Redis的list数据接口常用来作为异步消息队列使用，使用rpush/lpush操作入队列，使用lpop/rpop来出队列。 &gt; rpush notify-queue apple banana pear (integer) 3 &gt; llen notify-queue (integer) 3 &gt; lpop notify-queue &quot;apple&quot; &gt; llen notify-queue (integer) 2 &gt; lpop notify-queue &quot;banana&quot; &gt; llen notify-queue (integer) 1 &gt; lpop notify-queue &quot;pear&quot; &gt; llen notify-queue (integer) 0 &gt; lpop notify-queue (nil) 上面是rpush和lpop结合使用得例子。还可以使用 lpush 和 rpop 结合使用，效果是一 样的。这里不再赘述。 异步消息队列👇👇 客户通过队列的pop操作来获取消息，然后进行处理。处理完了再pop一个，如此循环，这便是作为队列消费者的客户端的生命周期。 可是如果队列空了怎么办？客户端就回陷入pop的死循环😇，不停的pop，从而造成了空轮循。空轮循不但拉高了客户端CPU消耗，Redis的QPS也会被拉高📈，从而降低📉了Redis的性能。 通常我们使用sleep方法来解决这个问题，让线程睡眠1秒或者0.5秒，就可以让客户端的CPU和Redis的QPS降下来📉📉。 阻塞读 用上面睡眠的方式虽然可以解决问题，但是又会导致另外一个问题，那就是睡眠会导致消息的延迟增大，不能及时的消费消息。 虽然缩短睡眠时间可以缓解，但是💢有没有更好的解决方案呢？当然也有，就是bpop/brpop。 这两个指令的前缀字符b代表的是blocking，也就是阻塞读。 阻塞读在队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消息的延迟几乎为零。 空闲连接自动断开 但是这里有个需要注意的点，就是空闲连接自动断开的问题，如果线程一直阻塞在那里，Redis的客户端连接就成了闲置连接，闲置过久后，服务器一般会主动断开连接❌，从而减少📉闲置资源的占用，而这个时候blpop/brpop会抛出异常❗️。 所以编写客户端消费者时注意要捕获异常，进行重新连接重试♻️。 简单延时队列的实现 延时队列就是带延时功能的消息队列，相较于普通队列，它可以在指定时间消费掉信息。 我们通过redis的有序集合zset来实现简单的延迟队列，将消息数据序列化，作为zset的value，把消息处理时间作为score，每次通过zRangeByScore获取一条消息进行处理。 应用场景 用户下单后，30分钟未支付，订单自动作废 支付后，24小时没有评论就自动五星好评 PHP简单代码实例 延时队列类： &lt;?php class DelayQueue { protected $prefix = 'delay_queue'; protected $redis = null; protected $key = ''; /** * 连接Redis * DelayQueue constructor. * @param $queue */ public function __construct($queue) { $this-&gt;key = $this-&gt;prefix . $queue; $this-&gt;redis = new Redis(); $this-&gt;redis-&gt;connect('127.0.0.1', 6379); $this-&gt;redis-&gt;auth('111111'); } /** * 删除任务 * @param $value * @return int */ public function delTask($value) : int { return $this-&gt;redis-&gt;zRem($this-&gt;key, $value); } /** * 添加任务 * @param $name * @param $time * @param $data * @return int */ public function addTask($name, $time, $data) : int { echo date('Y-m-d H:i:s') . ': 新增任务' . $name . PHP_EOL; return $this-&gt;redis-&gt;zAdd($this-&gt;key, $time, json_encode([ 'task_name' =&gt; $name, 'task_time' =&gt; $time, 'task_data' =&gt; $data ])); } /** * 获取任务 * @return array */ public function getTask() : array { return $this-&gt;redis-&gt;zRangeByScore($this-&gt;key, 0, time(), ['limit' =&gt; [0,1]]); } /** * 处理任务 * @return bool */ public function dealTask() : bool { // 每次只取一个任务 $task = $this-&gt;getTask(); if (empty($task)) { return false; } $task = current($task); // 有并发的问题，这里判断当前队列是否抢到到该任务 if ($this-&gt;delTask($task)) { $task = json_decode($task, true); echo date('Y-m-d H:i:s') . ' 订单：【' . $task['task_name'] . '】'. $task['task_data']['msg'] . PHP_EOL; return true; } return false; } /** * 初始化模拟插入任务 */ public function initTask() { $this-&gt;addTask('Close_S202108020001', time() + 30, ['msg' =&gt; '订单超时未支付，自动取消', 'order_id' =&gt; 'S202108020001']); //30s后处理 $this-&gt;addTask('Close_S202108020002', time() + 60, ['msg' =&gt; '订单超时未支付，自动取消', 'order_id' =&gt; 'S202108020002']); //60s后处理 $this-&gt;addTask('Close_S202108020003', time() + 90, ['msg' =&gt; '订单超时未支付，自动取消', 'order_id' =&gt; 'S202108020003']); //90s后处理 } } 消费脚本 &lt;?php include_once &quot;DelayQueue.php&quot;; set_time_limit(0); $dealQueue = new DelayQueue('Order:DelayQueue'); $dealQueue-&gt;initTask(); while (true) { $dealQueue-&gt;dealTask(); sleep(1); } 注意 Redis 作为消息队列为什么不能保证 100% 的可靠性？ 1.如果使用rpop获取任务的话，由于rpop没有阻塞功能，脚本需要轮循查看list队列是否有任务，造成资源浪费（可使用brpop，带有阻塞功能）； 2.基于zset设计的队列存在将 zrangebyscore和zrem的原子化操作的问题，会存在争抢任务，造成浪费，此外消息时间如果有误会造成任务顺序有误； 3.没有Ack机制，客户端取出信息后，处理过程中发生错误的情况就会造成任务丢失； 何时使用？ 需要异步处理任务，且业务逻辑比较简单的时候； 任务数量比较平稳，不会出现大量堆积的时候，不然容易崩了丢失任务； 使用时记得加上确实任务处理机制，即取出任务时，加入到正在处理任务队列中，待任务处理完再进行删除，方式客户端异常时任务丢失。 ","link":"http://mofish.pily.life/post/redis_learning_04/"},{"title":"Redis学习之路(三)：限流之Redis-Cell","content":"限流算法在分布式领域是一个经常被提起的话题 ，当系统的处理能力有限时💢，或者当活动火热访问量突然骤增时📈📈 如何阻止计划外的请求继续对系统施压，这时一个需要重视的问题🤠 除了上面所说的情况，限流还有一个同样常用的场景，那就是有目的的控制用户行为，避免垃圾请求，例如用户恶意刷接口、例如每日限制发帖数量等等。 一、简单限流策略 首先我们来看一个常见的、简单的限流策略。系统限定用户某个行为在指定时间里只能发生N次，那么如何使用Redis的数据结构来实现这个简单的限流功能呢？ a.过期时间内自增 原理：该方法是利用Redis的Incr自增命令和Expire命令，从第一次请求开始计算，如果在范围时间内，Incr次数达到最大限制次数的话📈📈，则限制🈲本次操作。 代码示例 &lt;?php class ActionLimit { private $redis; public function __construct() { // 连接Redis $redis = new Redis(); $redis-&gt;connect('127.0.0.1', '6379'); $redis-&gt;auth('xxxxxx'); $this-&gt;redis = $redis; } /** * 检查 * @param $userId(用户ID，唯一标识) * @param $action(请求表示) * @param $period(规定时间内,分钟) * @param $maxCount(最大请求次数) * @return bool */ public function checkActionLimit($userId, $action, $period, $maxCount) { $key = 'hits:' . $userId . ':' . $action; // 访问次数自增一 $hitCount = $this-&gt;redis-&gt;incr($key); if ($hitCount == 1) { $this-&gt;redis-&gt;expire($key, $period * 60); } echo &quot;hits:&quot; . $hitCount . PHP_EOL; if ($hitCount &gt; $maxCount) { return false; } return true; } } $actionLimit = new ActionLimit(); for ($i = 1; $i &lt; 10; $i++) { sleep(2); // 睡它两秒 // 一分钟最多只能评论5次 $str = date('Y-m-d H:i:s') . &quot; : 第{$i}次访问 &quot; ; $checkLimit = $actionLimit-&gt;checkActionLimit(10086, 'addComment', 1, 5); if (!$checkLimit) { $str .= '超过限制了'; } echo $str . PHP_EOL; } 执行结果： b.滑动时间窗口 除了上面这个简单的方法，在书中还看到过一个滑动事件窗口（定宽）的方案，就是使用ZSet数据结构的Score值，通过Score值圈出这个时间窗口， 因此，我们只需要保留该窗口内的数据，窗口外的数据都可以砍掉或者限制额外的数据进入窗口。 如下图所示，用一个ZSet结构记录用户的行为历史，每一个行为都会作为ZSet中的一个Key保存下来，同一个用户的同一种行为用一个ZSet记录。 代码示例 &lt;?php class Period { private $redis; public function __construct() { $redis = new Redis(); $redis-&gt;connect('127.0.0.1', 6379); $redis-&gt;auth('111111'); $this-&gt;redis = $redis; } public function checkIsUsing($userId, $actionKey, $period, $maxCount) { $key = &quot;hist:{$userId}-{$actionKey}&quot;; $nowTime = time(); // 移除滑动窗口内多余的数据 $this-&gt;redis-&gt;zRemRangeByScore($key, 0, $nowTime - $period); // 获取当前窗口内活动的数据数量 $currentCount = $this-&gt;redis-&gt;zCard($key); // 获取当前窗口内最早要过期的一条数据 $minActionTime = $this-&gt;redis-&gt;zRange($key, -$maxCount, -$maxCount); if ($currentCount &gt;= $maxCount) { // 如果当前窗口数据超过maxCount，则返回错误提示 $waitTime = $minActionTime[0] + $period - time(); echo &quot;{$period}秒内最多操作{$maxCount}次，请等待{$waitTime}秒&quot;.PHP_EOL; return false; }else { // 如果当前窗口数据没有超过maxCount，则插入数据 $this-&gt;redis-&gt;zAdd($key,$nowTime, $nowTime); // 加个过期时间，避免冷数据持续占用内存 $this-&gt;redis-&gt;expire($key, $period + 1); return true; } } } $period = new Period(); // 60秒内只能发表5次评论 if ($period-&gt;checkIsUsing(10086, 'addComment', 60, 5)) { echo '评论成功' . PHP_EOL; } 执行结果： 二、高级限流算法-漏斗限流 漏斗限流是最常用的限流方法之一，顾名思义，这个算法的灵感是来源于漏斗的结构。 如下图所示，漏斗的容量是有限的，如果当漏水的速度大于装水的速度时，一切正常；但是当漏水速度小于装水速度时，就会出现漏斗满了，水溢出来的情况。 而在程序设计中，漏斗的容量代表当前用户行为可以持续进行的数量，漏嘴的流水速率代表着系统允许改行为的最大频率，我们需要在程序中判断当前漏斗中是否有充足的容量来处理当前这次用户行为。 代码示例 &lt;?php class Funnel { private $capacity; // 漏斗容量 private $leakingRate; // 漏嘴流水速度 private $leftQuota; // 漏斗剩余空间 private $leakingTs; // 上一次漏水时间 public function __construct($capacity, $leakingRate) { $this-&gt;capacity = $capacity; $this-&gt;leakingRate = $leakingRate; $this-&gt;leftQuota = $capacity; $this-&gt;leakingTs = time(); } /** * 处理空间 */ public function makeSpace() { $nowTs = time(); // 距离上次漏水过去了多久 $deltaTs = $nowTs - $this-&gt;leakingTs; // 又可以腾出不少空间了 $deltaQuota = $deltaTs * $this-&gt;leakingRate; if ($deltaQuota &lt; 1) return; // 增加剩余空间 $this-&gt;leftQuota += $deltaQuota; // 记录漏水时间 $this-&gt;leakingTs = $nowTs; // 剩余空间不得高于容量 if ($this-&gt;leftQuota &gt; $this-&gt;capacity) { $this-&gt;leftQuota = $this-&gt;capacity; } } /** * 流水 * @param $quota 1 消耗一个容量 * @return bool */ public function watering($quota) { $this-&gt;makeSpace(); // 判断剩余空间是否足够 if ($this-&gt;leftQuota &gt;= $quota) { $this-&gt;leftQuota -= $quota; echo date('Y-m-d H:i:s') .': '. $this-&gt;leftQuota. PHP_EOL; return true; } echo date('Y-m-d H:i:s') .': '. '不够空间啦' . PHP_EOL; return false; } } $funnels = []; function isActionAllowed($userId, $action, $capacity, $leakingRate){ $key = &quot;hist:{$userId}_{$action}&quot;; global $funnels; if (isset($funnels[$key])) { $funnel = $funnels[$key]; } else { $funnel = new Funnel($capacity, $leakingRate); $funnels[$key] = $funnel; } return $funnel-&gt;watering(1); } for ($i = 0; $i &lt; 20; $i++) { isActionAllowed(10086, 'addComment', 15, '0.5'); sleep(1); } 流程图 不足 Funnel对象中，makeSpace方法都漏斗算法的核心，其在每次漏水前都会被调用以触发漏水，给漏斗腾空空间来， 能腾出多少空间取决于时间过去了多久以及流水的速度 ，而值得注意的是，漏斗的空间占用是一个常量，不会因为行为频率增加而增大。 从流程我们可以看到，对于漏斗的一系列操作我们无法保证整个过程的原子性，意味着需要进行适当的加锁，而一旦加锁，就意味着有加锁或解锁失败的可能，除此还得引入重试机制，导致性能下降，代码复杂度增加。 因为为了解决这个问题，我们可以使用本章的重点知识点Redis-Cell。 Redis-Cell Redis4.0提供了一个限流Redis模块，叫做Redis-Cell。该模块也使用了漏斗算法，并提供了原子的限流指令。 该模块只有一个指令cl.throttle，它参数我们下面说明一下： 从上图可以看到，这个指令是意思是：允许用户10086的回复行为频率为60秒最多30次（漏水速率），漏斗的初始容量为15。也就是说一开始可以连续回复15个帖子，然后才开始收漏水速率的影响。 注意：除此之外，我们可以看到漏水速率变成了2个参数，用两个参数相除的接口来表达漏水速率相对来说更加直观一些。 &gt; cl.throttle 10086:addComment 15 30 60 &gt; 1) (integet) 0 # 0 表示允许 1 表示拒绝 &gt; 2) (integet) 15 # 漏斗容量 capacity &gt; 3) (integet) 14 # 漏斗剩余空间 leftQuota &gt; 4) (integet) -1 # -1代表还有空间 如果被拒绝了，这里返回的是需要多长时间再试(单位秒) &gt; 5) (integet) 2 # 多长时间后，漏斗完全空出来(单位秒) 因此在执行限流指令时，如果被拒绝了，就需要丢弃或者重试，一般我们直接使用第4个参数，把重试剩余时间提示给用户即可，是非常方便的，所以以上就是Redis限流的一些用法啦，大家可以去尝试一下。 ","link":"http://mofish.pily.life/post/redis_learning_03/"},{"title":"Redis学习之路(二)：布隆过滤器","content":"📱 日常我们刷抖音的时候，它都会不停地给我们推荐新的内容，而每次推荐时都要去重， 😮如果我们每次去重时都要频繁地对数据库进行exists查询，当系统并发量高时，数据库是很难扛得住的 😕而当我们把这些历史记录都缓存起来呢？那内存可能直接爆掉了，随着时间的推荐，占用的空间会越来越大，那么有啥效率高、占用小的方法呢？ 👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇 没错，就是布隆过滤器！！👏👏 它就是专门用来解决这种去重问题的，它在起到去重作用的同时，在空间上还能大大节省， 虽然稍微有一点点不精确，有一点点误判概率，但是只要参数设置得合理，精确度还是能控制的👏👏 一、布隆过滤器是什么？🤷‍♂ 我们其实可以把它理解为一个不算非常精确的set结构，当它说某个值存在时，这个值不能不存在，但它说这个值不存在时，那么一定不存在。 举个现实生活中的例子，就是当一个人说认识你的时候，可能只是有个朋友跟你长得很像，误认为是你，当它说不认识你的时候，就真的不认识你了。 因此，在部分需要过滤大量数据，避免重复时，就可以用到布隆过滤器，在合理的参数设置下，可以精准的过滤掉重复数据。 注意：Redis官方提供的布隆过滤器到了Redis4.0提供了插件功能之后才正式登场。 二、布隆过滤器的基本用法✍️ 使用前记得确保版本&gt;4.0，并且安装了Rebloom插件噢😲 可参考：https://www.jianshu.com/p/2cc55cd1e13a bf.add和bf.madd，一个是添加一个元素，一个是批量添加元素 bf.exists和bf.exists，一个是查询单个元素，一个是批量查询 下面我们来用用看： 127.0.0.1:6379&gt; bf.add users user1 (integer) 1 127.0.0.1:6379&gt; bf.add users user2 (integer) 1 127.0.0.1:6379&gt; bf.add users user3 (integer) 1 127.0.0.1:6379&gt; bf.exists users user4 (integer) 0 127.0.0.1:6379&gt; bf.madd users user4 user5 user6 user7 1) (integer) 1 2) (integer) 1 3) (integer) 1 4) (integer) 1 127.0.0.1:6379&gt; bf.mexists users user4 user3 user8 1) (integer) 1 2) (integer) 1 3) (integer) 0 熟悉了基本命令后，我们尝试一下用脚本测试一下，大概的测试流程就是插入50000个元素，然后用另外不同的50000个元素去检测，看看是否有检测失败的。 如用不使用bf.reserve，默认的error_rate是0.01，默认的initial_size是100个元素 error_rate越低，需要的空间越大 initial_size表示预计方法的元素数量，当实际数量超出这个数值时，误判率会上升，因此需要提前设置一个较大的值 使用bf.reserve命令前，需要确保对应的key不存在，不然会报错 三、简单代码示例(PHP) &lt;?php $redis = new Redis(); $redis-&gt;connect('127.0.0.1', '6379'); $redis-&gt;auth('xxxxxxxx'); // 如果不使用bf.reserve，默认initial_size=100，error_rate=0.01 // 可在预估成员数量时，适当的调整initial_size // 调整前确保这个key是不存在的，不然会报错：ERR item exists $redis-&gt;rawCommand('del', 'users'); $redis-&gt;rawCommand('bf.reserve', 'users', 0.01, 50000); $exists = 0; for ($i = 1; $i &lt;= 100000; $i++) { // 前50000个插入 if ($i &lt;= 50000) { $redis-&gt;rawCommand('bf.add', 'users', $i); } else { $check = $redis-&gt;rawCommand('bf.exists', 'users', $i); $check != 1 ? : $exists++; } } echo $exists . PHP_EOL; echo round($exists / 50000, 3); 结果 误差数：304 误差率：0.006 如果把error_rate设为0.001的话，结果如下 结果 误差数：1 误差率：0 Ps：布隆过滤器的initial_size设置过大，会浪费存储空间，设置得过小，就会影响准确率，因此用户在使用之前一定要尽可能精确的估计元素数量，还需要加上一定的冗余空间以避免实际元素可能会意外高出估计值许多。 四、布隆过滤器的原理（位数组+hash算法） 每个布隆过滤器对应到Redis的数据结构里面就是一个大型的位数组和几个不一样的无偏hash函数，如下图所示的HASH1、HASH2、HASH3就是这样的hash函数。 而所谓的无偏就是能把元素的hash值算的比较均匀，让元素被hash映射到位数组中的位置比较随机。 添加元素(步骤) 使用多个hash函数对key进行hash计算，算的一个整数的索引值 然后对位数组长度进行取模运算得到一个位置，每个hash函数都会算的一个不同的位置 在把位数组的这几个位置都置为1，就完成了添加操作 查找元素 其实与添加一样，都是使用多个hash函数计算，找到位数组中对应的值，然后检查这几个位置是否都是1，如果有一个是0，那么说明这个值不存在。 但是如果都是1的话，也可能是因为其它的值存在所导致的。 因此如果这个位数组比较稀疏，判断正确的概率会大很多 如果这个位数组比较拥挤，那么判断正确的概率就会大大降低 所以当返回的结果说你不存在时就真的不存在，而说你存在时，可能不存在。 注意点 使用时不要让实际元素数量远大于初始化数量，当实际元素数量开始超出初始化数量时，应该对布隆过滤器进行重建，重新分配一个size更大的过滤器，再将所有历史元素批量添加进去。 因为error_rate不会因为数量刚一超出就急剧增加，这给我们重建过滤器提供了较为宽松的时间。 五、优缺点 优势：省空间+高性能 省空间 如果是真实存储一个URL(64B)的话，经过hash函数对应着一个数组位(比特位)，这样空间比例大约时64*8 : 1，即512 : 1，因此用来存储判断是否存在的数据来说，布隆过滤器的空间效率是极其高效的。 高性能 BloomFilter 不存在像链表查询一样，需要一个一个去遍历。反而会像数组一样，类似于直接取下标就可以找到所需要的结果。 不同的是，BloomFilter 需要过几个hash function，去查找下标。所以，综合来看，性能是很高的！ 综合时间和空间效率，在有很低的误识别率情况下，各方面都是远超其他算法的。 不足：有误判+不能删除元素 有误判 因为hash可能会存在冲突，从而导致不存在的元素可能也会被会判为存在。 因此使用Redis的布隆过滤器时，要合理的设置位数组大小和误判率，尽量把误判率控制在自己可以接受的范围内。 不能删除元素 不用元素通过hash函数使得对应位置都置为1了，所以如果把其置为0的话，会影响到其它元素，因此是绝对不能删除。 ","link":"http://mofish.pily.life/post/redis_learning_02/"},{"title":"硬啃设计模式(八)：代理模式","content":"🤤 设计模式之后更新的频率可能会放慢一点噢，可能会一个月2篇左右~ 😿 因为打算把更多精力放在整理其它更实用的知识点上，例如redis、elk、mq等等 目录 简述 概念 角色 应用场景 优缺点 UML图 代码示例 总结 简述👇👇 在有些情况下，一个客户不能或者不想直接访问另一个对象，这时需要找一个中介帮忙完成某项任务，这个中介就是代理对象。例如，购买火车票不一定要去火车站买，可以通过 12306 网站或者去火车票代售点买。又如找女朋友、找保姆、找工作等都可以通过找中介完成。 在软件设计中，使用代理模式的例子也很多，例如，要访问的远程对象比较大（如视频或大图像等），其下载要花很多时间。还有因为安全原因需要屏蔽客户端直接访问真实对象，如某单位的内部数据库等。 概念🗣🗣 由于某些原因需要给某对象提供一个代理以控制对该对象的访问， 这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 角色👦👧 抽象主题(Subject)类：通过接口或抽象类声明真实主题和代理对象实现的业务方法 实主题(Real Subject)类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 理(Proxy)类：提供了与真实主题相同的接口，其内部含有对真是主题的引用，它可以访问、控制或扩展真是主题的功能。 应用场景🏠🏠 当无法或不想直接引用某个对象或访问某个对象存在困难时，可以通过代理对象来间接访问。 使用代理模式主要有两个目的：保护目标对象、增强目标对象。 优缺点✔️❌ 优点🤤 1. 代理模式再客户端与目标对象之间起到一个中介作用和保护目标对象的作用 2. 代理对象可以扩展目标对象的功能 3. 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合性，增加了程序的可扩展性 缺点😭 1. 代理模式会造成系统设计中类的数量增加 2. 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢 3. 增加了系统的复杂度 代理模式UML图 装饰器模式代码实例 &lt;?php /** * 抽象主题类 * Interface GiveGift */ interface GiveGift { public function giveDoll(); public function giveFlowers(); public function giveChocolate(); } /** * 真实主题类 * Class Pursuit */ class Pursuit implements GiveGift { public $mm; public function __construct(SchoolGirl $mm) { $this-&gt;mm = $mm; } public function giveDoll() { echo '给' . $this-&gt;mm-&gt;name . '送一个娃娃' . PHP_EOL; } public function giveFlowers() { echo '给' . $this-&gt;mm-&gt;name . '送一束鲜花' . PHP_EOL; } public function giveChocolate() { echo '给' . $this-&gt;mm-&gt;name . '送一盒巧克力' . PHP_EOL; } } /** * 代理类 * Class Proxy */ class Proxy implements GiveGift { public $gg; public function __construct(SchoolGirl $mm, $proxyName, $pursuitName) { $this-&gt;gg = new Pursuit($mm); echo $proxyName . '替' . $pursuitName . PHP_EOL; } public function giveDoll() { $this-&gt;gg-&gt;giveDoll(); } public function giveFlowers() { $this-&gt;gg-&gt;giveFlowers(); } public function giveChocolate() { $this-&gt;gg-&gt;giveChocolate(); } } /** * Class SchoolGirl */ class SchoolGirl { public $name; public function __construct($name) { $this-&gt;name = $name; } } $Lisa = new SchoolGirl('Lisa'); $John = new Proxy($Lisa, 'Proxy', 'Pursuit'); $John-&gt;giveDoll(); $John-&gt;giveFlowers(); $John-&gt;giveChocolate(); 总结 在代码中，一般代理会被理解为代码增强，实际上就是在原代码逻辑前后增加一些代码逻辑，而使调用者无感知。 ","link":"http://mofish.pily.life/post/design_patterns_08/"},{"title":"硬啃设计模式(七)：装饰器模式","content":"😣 5.1快到啦！！！这个月都要补班，所以都是单休，最近工作也完成得比较快，超进度完成了！ 🙌 所以，忙里偷闲还不如闲里偷学呢哈哈哈哈哈，so，继续学习下一个设计模式：装饰器模式。 目录 简述 应用场景 概念 UML图 代码示例 与建造者模式的区别 总结 简述👇👇 通常情况下，扩展一个类的功能会使用继承的方式来实现。但是继承具有静态特征，耦合度高，并且随着扩展功能的增多，子类会很膨胀。 如果使用组合关系来创建一个包装对象(即装饰对象)来包裹真实对象，并在保持真实对象的类结构不变的前提下，为其提供额外的功能，这就是装饰器模式的目标。 生活中的例子 像大话设计模式中的例子，人打扮其实就是一个装饰的过程，本来我只穿了个裤衩的，然后后面想出门了，又需要穿衣服、裤子、戴个手表什么之类的， 又像我们平时吃手抓饼一样，我们可能会加鸡蛋、火腿、肉松等等，而无论加什么，它实际都是一个饼，而加料其实就是对饼的一个装饰。 应用场景🏠🏠 当需要给一个现有类添加附加职责，而又不能采用生成子类的方法进行扩充时；（注意，是现有的类） 当需要通过对现有得一组基本功能进行排列组合而产生非常多的功能时，采用继承关系很难实现，而采用装饰器模式即可很好实现； 当对象的功能要求可以动态地添加，也可以动态地撤销时。 概念🗣🗣 饰器模式属于结构型模式，指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式， 不使用继承而通过关联关系来调用现有类中的方法，达到复用的目的，并使得对象的行为可以灵活变化。 角色👦👧 抽象构件(Component)角色：定义一个抽象接口以规范准备接受附加责任的对象； 具体构件(ConcreteComponent)角色：实现抽象构件，通过装饰角色为其添加一些职责； 抽象装饰(Decorator)角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件功能； 具体装饰(ConcreteDecorator)角色：实现抽象装饰的相关方法，并给具体构件对象添加附件的职责。 注意： 如果只有一个具体构件角色的话，抽象构件角色可忽略，而抽象装饰角色可直接继承具体构件角色。 优缺点✔️❌ 优点🤤 1. 装饰器是继承的有力补充，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用 2. 通过使用不用装饰类及这些装饰类的排列组合，可以实现不同的效果 3. 装饰器模式完全遵守开闭原则 缺点😭 1. 装饰器模式会增加许多子类，过度使用会增加程序的复杂性 装饰器模式UML图 装饰器模式代码实例 Component.php 构建类 &lt;?php /** * Component * @Descript 抽象构件类 * 如果只有一个具体构件角色的话，该抽象类可忽略 * Interface Component */ //interface Component //{ // public function show(); //} /** * ConcreteComponent * @Descript 具体构件类 * Class People */ class People { private $name; public function __construct($name) { $this-&gt;name = $name; } public function show() { echo $this-&gt;name . '装扮：'; } } Decorator.php 装饰角色类 &lt;?php require_once &quot;./Component.php&quot;; /** * Decorator * @Description 抽象装饰类（服饰类） * Class Finery */ class Finery extends People { protected $people; public function __construct(People $people) { $this-&gt;people = $people; } /** * @Override */ public function show() { $this-&gt;people-&gt;show(); } } /** * Class TShirt */ class TShirt extends Finery { public function show() { parent::show(); echo '大T恤 '; } } /** * Class TShirt */ class BigTrouser extends Finery { public function show() { parent::show(); echo '跨裤 '; } } $people = new People('小明'); $tshirt = new TShirt($people); $bigTrouser = new BigTrouser($tshirt); $bigTrouser-&gt;show(); 运行结果： &gt; 小明装扮：大T恤 跨裤 与建造者模式的区别 从上面的例子我们可能会有疑问，这不跟建造者模式很类似吗？都是用小配件来组装对象，那二者有啥区别呢？ 过程稳定性的不同： 其实我们回看之前学习的建造者模式,就能明白，建造者模式要求建造的过程是稳定的，是相同的步骤生成不同类型的对象， 而装饰器模式的建造过程是不稳定的，我可以先穿裤子，再穿衣服，反之亦然， 模式的不同： 装饰器模式是一个结构型模式，主要是对原有对象做一个表面外部的装饰， 而建造者模式属于创建型模式，是对某个东西整体框架的建造以及内部稳定架构的组装 总结 装饰器模式是为已有功能动态地添加更多功能的一种方式，它把每个要装饰的功能放在单独的类中，并让这个类包装它所要的装饰对象， 因此，当需要执行特殊行为时，客户代码就可以在运行时根据需要有选择地、按顺序地使用装饰功能包装对象。 总的来说就是把类中的装饰功能从类中搬移去除，这样就可以简化原有的类，有效地把类的核心职责和装饰功能区分开。 但是如果过度使用的话，装饰器模式会增加许多子类，从而增加程序的复杂性。 ","link":"http://mofish.pily.life/post/design_patterns_07/"},{"title":"Redis学习之路(一)：5大基础数据类型","content":"🤺来，今天开始整理的是Redis的相关知识点，当然设计模式的也会继续同步更新 🗣🗣 所以，要抓紧时间巩固啦！虽然这些知识点以前已经整理过一遍，但是因为数据丢了，所以只能重新整理了，就当记忆加深吧！😤 Redis自身是一个Map，其中所有数据都是采用key-value的形式存储 而数据类型指的是存储数据的类型，也就是value的部分，key部分永远都是字符串 目录🔖🔖 String Hash List Set ZSet 一、String 📖📖类型描述 存储的数据：单个数据，最简单的数据存储类型，也是我们最常用到的类型 存储数据的格式：一个存储空间保存一个数据 存储内容：字符串、整数、浮点数 存储特点：二进制安全的，意思是可以包含任何数据，比如jpg图片或者序列化的对象等 🏠🏠常用场景 计数器（如：粉丝数） 查询频繁且处理逻辑复杂，但不必实时更新的数据（如：月活量、转码排队数） 乐观锁（incr 命令） 分布式锁（set 命令的 NX 参数） 💻💻常用命令 set 命令格式：set key value [EX seconds|PX milliseconds] [NX|XX] EX seconds: 将键的过期时间设置为seconds秒。 执行SET key value EX seconds的效果等同于执行SETEX key seconds value。 PX milliseconds ： 将键的过期时间设置为milliseconds毫秒。 执行SET key value PX milliseconds的效果等同于执行PSETEX key milliseconds value。 NX： 只在键不存在时， 才对键进行设置操作。 执行SET key value NX的效果等同于执行SETNX key value。 XX： 只在键已经存在时， 才对键进行设置操作。 因为 SET 命令可以通过参数来实现 SETNX 、 SETEX 以及 PSETEX 命令的效果， 所以 Redis 将来的版本可能会移除并废弃 SETNX 、 SETEX 和 PSETEX 这三个命令。 setnx 命令格式：set key value 只在键key不存在的情况下，将键key的值设置为value 若键key已经存在， 则SETNX命令不做任何动作 命令在设置成功时返回1，设置失败时返回0 setex 命令格式：set key seconds value 将键key的值设置为value， 并将键key的生存时间设置为seconds秒钟 如果键key已经存在， 那么SETEX命令将覆盖已有的值 命令在设置成功时返回OK get 命令格式：get key 返回与键key相关联的字符串值 如果键key不存在， 那么返回特殊值nil； 否则， 返回键key的值 如果键key的值并非字符串类型， 那么返回一个错误， 因为GET命令只能用于字符串值 append 命令格式：append key value 如果键 key 已经存在并且它的值是一个字符串， APPEND 命令将把 value 追加到键 key 现有值的末尾 如果 key 不存在， APPEND 就简单地将键 key 的值设为 value ， 就像执行 SET key value 一样 追加value之后， 返回键 key 的值的长度 strlen 命令格式：strlen key 返回键key储存的字符串值的长度 当键key不存在时， 命令返回0 当key储存的不是字符串值时， 返回一个错误 incr/decr 命令格式 incr/dect key 为键 key 储存的数字值加1/减1 如果键 key 不存在， 那么它的值会先被初始化为 0 ， 然后再执行 incr/decr 命令 如果键 key 储存的值不能被解释为数字， 那么将返回一个错误 incrby 、decrby 命令格式 incrby/decrby key decrement 与incr/dect类型，只不过增减的值不是1而是decrement步长 📠📠编码方式 int：当存储的字符串全是数字时，此时使用int方式存储 embstr：当存储的字符串长度小于44个字符时，使用该embstr方式存储 raw：当存储的字符串长度大于44个字符时，使用该raw方式存储 可以使用object encoding key查看key对应的encoding类型 对于embstr和raw这两种encoding类型，其存储方式还不太一样。对于embstr类型，它将RedisObject对象头和SDS对象在内存中地址是连在一起的，但对于raw类型，二者在内存地址不是连续的。 至于为什么是44字节，首先我们要了解到，所有的Redis对象都有下面这个头结构，共16字节： struct RedisObject { int4 type; // 4bits，不同的对象有不同的type int4 encoding; // 4bits，同一个类型的type会有不同的存储形式 int24 lru; // 24bits，记录LRU信息， int32 refcount; // 4bytes，每个对象都有引用计数器，当引用计数为0时，对象就会被销毁，内存被回收 void *ptr; // 8bytes，64-bit system，指向对象内容（body）的具体存储位置 } robj; 我们再来看SDS结构体的大小，在字符串比较小时，SDS对象头结构的大小是capacity+3，至少是3字节，意味着分配一个字符串的最小空间占用为19字节（16+3）。 struct SDS { int8 capacity; // 1bytes int8 len; // 1bytes int8 flags; // 1bytes bytes[] content; // 内联数组，长度为 capacity } 因为内存分配器jemalloc、tcmalloc等分配内存大小的单位都是2/4/8/16/32/64字节等，为了能容纳以恶完整的embstr对象，jemalloc至少会分配32字节空间，如果再稍微长一点就是64字节，超过64字节的话，Redis就认为它是一个大字符串，就不再适用embstr格式存储，而该使用raw形式。 那么继续回答上面的问题，为什么是44字节呢？因为除去头部的19个字节（16+3），还剩下45个字节，而SDS结构体中的content中的字符串是以NULL结尾的字符串,，占用1个字节，所以就只剩下44个字节了。 在介绍string类型的存储类型时，我们说到，对于embstr和raw两种类型其存储方式不一样，但ptr指针最后都指向一个SDS的结构。那什么是SDS呢？ ⛏⛏底层原理（embstr vs raw） SDS动态字符串 二、Hash 我们日常使用string类型来json_encode存储对象数据时，本能的会觉得好方便😁啥都不用管，一个字符串丢进去就行。 但是当我们需要频繁的获取并修改某个属性时，就需要不断的decode、encode😱，因此String类型就显得笨重了。 而Hash类型就很好的解决了这个问题，下面我们继续学习。 📖📖类型描述 新的存储需求：对一系列存储的数据进行编组，方便管理，典型应用存储对象信息 需要的存储结构：一个存储空间保存多个键值对数据 ❗️❗️注意事项 Hash类型下的Value只能存储字符串，不允许存储其它数据类型，不存在嵌套现象。 每个Hash可以存储232^3232^22 - 1个键值对 Hash类型十分贴近对象的数据存储形式，并且可以灵活添加删除对象属性，但切记不可滥用，更不要将其作为对象列表使用 hgetall 操作可以获取全部属性，如果内部Field过多，遍历整体数据效率就很会低，有可能成为数据访问瓶颈 对于同样是存储字符串，Hash与String的主要区别是什么呢？ Hash把所有相关的值聚集到一个Key中，节省内存空间并且减少了Key冲突 当需要批量获取值的时候，只需要使用一个命令，减少内存/IO/CPU的消耗 🏠🏠常用场景 购物车 以客户ID作为Key，每个客户创建一个购物车Hash存储结构 将商品的信息数量等字段作为Field，对应的信息对应 商品id_nums : 商品数量 商品id_name : 商品名称 商品id_descript : 商品描述 ...... 添加商品：追加新的Field和Value 更改数量：商品数量自增自减 浏览：遍历Hash 删除商品：删除Field 清空：删除Key 同一商品不同优惠卷的抢购 💻💻常用命令 hset 命令格式：hset key field value field不存在，新建成功时返回 1 field已经存在，成功覆盖时返回 0 hget 命令格式：hget key field hget 命令在默认情况下返回给定域的值 如果给定域不存在于哈希表中， 又或者给定的哈希表并不存在， 那么命令返回 nil hsetnx 命令格式：hsetnx key field value 设置成功时返回 1，域已存在导致设置为成功时返回 0 当且仅当域 field 尚未存在于哈希表的情况下， 将它的值设置为 value 如果哈希表 hash 不存在， 那么一个新的哈希表将被创建并执行 hsetnx 命令。 hexists 命令格式：hexists key field 检查给定域 field 是否存在于哈希表 hash 当中 hexists 命令在给定域存在时返回 1 ， 在给定域不存在时返回 0 hdel 命令格式：hdel key field [field ...] 删除哈希表 key 中的一个或多个指定域，不存在的域将被忽略 hincrby 命令格式：hincrby key field inctement 作用：为哈希表 key 中的域 field 的值加上增量 increment 增量也可以为负数，相当于对给定域进行减法操作 如果 key 不存在，一个新的哈希表被创建并执行 HINCRBY 命令 如果域 field 不存在，那么在执行命令前，域的值被初始化为 0 注意：对一个储存字符串值的域 field 执行 HINCRBY 命令将造成一个错误 hgetall 命令格式：hgetall key 返回哈希表 key 中，所有的域和值 ⛏⛏底层原理 Hash本身就是一个K-V结构，但是当存储Hash数据类型时，我们把它叫做内层的哈希，其底层可以使用两种数据结构实现的，分别是ZipList和HashTable。 当我们创建Hash表示默认存储结构，并不是HashTable，而是ZipList结构 hash_max_ziplist_entries和hash_max_ziplist_value值作为阀值 hash_max_ziplist_entries（默认512个键值对）表示一旦ziplist中元素数量超过该值，则需要转换为HashTable结构 hash_max_ziplist_value（默认64）表示一旦ziplist中数据长度大于该值，则需要转换为HashTable结构 底层原理详情： ZipList：压缩链表 HashTable：哈希表 三、List 📖📖类型描述 数据存储需求：存储多个数据，并对数据进入存储空间的顺序进行区分 需要的存储结构：一个存储空间保存多个数据，且通过数据可以体现进入顺序 常用场景 微信朋友圈点赞，要求按照点赞顺序显示点赞好友信息，如果取消点赞，移除对应好友信息 简单的异步队列，将需要延后处理的任务塞进list里面，按先后顺序取出处理 常用命令 lpush/rpush lpushx/rpushx lpop/rpop rpop/lpush lrem llen lindex linsert 底层原理 在3.2之后，redis新增了一种数据结构QuickList，用在列表的底层实现，而QuickList其实是Redis将链表和ZipList结合起来组成的。 如果只用链表的话，每个元素都需要附件指针空间，会导致浪费空间并且加重内存的碎片化。 而ZipList，即压缩列表，它将所有的元素彼此紧凑地挨在一起，存储在一块连续的内存中。 所以Redis讲两者结合起来组成了QuickList，也就是将多个ZipList使用双向指针串起来使用，即满足了快速的插入和删除性能，又不会出现太大的空间冗余。 底层原理详情： QuickList：快速列表 Set 类型描述 新的存储需求：存储大量的数据，在查询方面提供更高的效率 因此List是链表结构，无论是插入还是查询方面效率都不算特别高 需要的存储结构：能够保存大量的数据，高效的内部存储机制，便于查询 set类型：与hash存储结构完全相同，但是仅存储键，不存储值（nil），并且值是不允许重复的 常用场景 可以用来存储活动中中奖的用户ID、存储所需爬虫的网址、黑名单等，因为有去重功能，可以保证不会重复 应用于随机推荐类信息检索，例如热点歌单推荐，热点新闻推荐，热卖旅游线路，应用APP推荐， 大V推荐等 使用其交集、并集、差集等操作来实现共同好友、共同关注等功能 记录UP和IP访问量 常用命令 sadd smembers srem scard sismember srandmember spop sinter sunion sdiff sinterstore smove 底层原理 Set底层使用了Intset和Hashtable两种数据结构存储的，Intset我们可以理解为数组，HashTable就是普通的哈希表（key为set的值，value为null） Set的底层存储Intset和Hashtable是存在编码转换的，使用Intset存储必须满足下面两个条件，否则使用`Hashtable，条件如下： 结合对象保存的所有元素都是整数值 集合对象保存的元素数量不超过512个 底层原理详情： Intset：整数集合 HashTable：哈希表 Zset 类型描述 新的存储需求：数据排序有利于数据的有效展示，需要提供一种可以根据自身特征进行排序的方式 需要的存储结构：新的存储模型，可以保存可排序的数据 sorted_set类型：在set的存储结构基础上添加可排序字段 常用场景 粉丝列表，好友列表，根据亲密度排序 TOP10榜单、各种投票列表 带有权重的任务队列、带有指定执行时间的计划任务 常用命令 zadd zrange zrevrange zrem zrangebyscore zrevrangebyscore zremrangebyrank zremrangebyscore zcard zcount zinterstore zunionstore zrank/zrevrank zscore zincrby 底层原理 sorted set有两种实现方式，一种是ziplist压缩表，一种是zset(dict、skiplist)， 当sorted set中的元素个数小于128时(即元素对member score的个数，共256个元素)，使用ziplist， 当元素对中member长度小于64个字节时使用ziplist。 当然这两个可以通过配置文件修改zset-max-ziplist-entries、zset-max-ziplist-value。 底层原理详情： ZipList：压缩列表 HashTable+SkipList：跳跃表 当不满足ZipList的使用条件时，zset会使用一种复合结构来存储数据，即HashTable+SkipList， HashTable用来存储member到score的映射，这样就可以在O(1)时间内找到member对应的分数 SkipList按照从小到大的顺序存储分数，便于通过score范围来获取元素列表，并且SkipList每个元素的值都是[socre,value]对 ","link":"http://mofish.pily.life/post/redis_learning_01/"},{"title":"硬啃设计模式(六)：适配器模式","content":"😈 上班摸鱼更新中.........看到了千万不要告诉我领导😭，因为安排的工作都弄完了，而且今天也是周五！ 👏 从这篇开始，就为大家带来结构型模式的学习啦，这一章介绍的就是比较简单的适配器模式 简述👇👇 在日常生活中，我们常常遇到下面这些场景，外交官发言时需要一名翻译官为记者翻译，电脑要读取SD卡的内容时需要有个读卡器等等 而在程序设计中，需要开发的具有某种业务功能的组件在现有的组件库中已经存在， 但它们与当前系统的接口规范不兼容，而重新开发所需的成本又比较高的时候，就可以使用适配器模式来解决。 简单的说，就是需要的东西在面前，但却不能使用，而短时间又无法改造它，于是得想办法适配它。 概念👇👇 适配器模式(Adapter)，将一个类的接口转化成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的类可以一起工作 适配器模式分为类适配器模式和对象适配器模式，由于类适配器模式通过多重继承对一个类与另外一个接口进行匹配 而C#、Java等语言都不支持多重集成也就是一个类只有一个父类， 并且对象适配器模式使用频率更多，所以我们这里主要讲的是对象适配器模式。 应用场景👇👇 1️⃣以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致。 2️⃣使用第三方提供的组件，但组件接口定义和自己要求的接口定义不同。 主要角色👴👵 🕴目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口 💃适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口 🕺适配器（Adapter）类：它是转换器，通过继承或引用适配者对象，把适配者接口转换成目标接口，让客户按目标接口格式访问适配者 优缺点❌✔️ ✔️优点： 1️⃣客户端通过适配器可以透明地调用目标接口 2️⃣复用了现存的类，在不需要修改原有代码的前提下重用现有的适配者类 3️⃣将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题 4️⃣在很多业务场景中符合开闭原则 ❌缺点： 1️⃣适配器编写过程需要结合业务场景全面考虑，可能会增加系统的复杂性 2️⃣增加代码阅读难度，降低代码可读性，过多使用适配器会使系统代码变得凌乱 适配器模式UML图👇👇 适配器模式代码示例👇👇 Player.php &lt;?php /** * 球员抽象类 * Class player */ abstract class player { protected $name; public function __construct(string $name) { $this-&gt;name = $name; } // 进攻 public abstract function Attack(); // 防守 public abstract function Defense(); } /** * 前锋 * Class Forwards */ class Forwards extends player { public function Attack() { echo '前锋' . $this-&gt;name . '进攻' . PHP_EOL; } public function Defense() { echo '前锋' . $this-&gt;name . '防守' . PHP_EOL; } } /** * 后卫 * Class Guards */ class Guards extends player { public function Attack() { echo '后卫' . $this-&gt;name . '进攻' . PHP_EOL; } public function Defense() { echo '后卫' . $this-&gt;name . '防守' . PHP_EOL; } } /** * 中锋 * Class Center */ class Center extends player { public function Attack() { echo '中锋' . $this-&gt;name . '进攻' . PHP_EOL; } public function Defense() { echo '中锋' . $this-&gt;name . '防守' . PHP_EOL; } } Adapter.php &lt;?php include_once &quot;./Player.php&quot;; /** * 外国中锋(Adaptee适配者) * Class ForeignCenter */ class ForeignCenter { private $name; public function __construct(string $name) { $this-&gt;name = $name; } public function ForeignAttack() { echo '外籍中锋' . $this-&gt;name . '进攻' . PHP_EOL; } public function ForeignDefense() { echo '外籍中锋' . $this-&gt;name . '防守' . PHP_EOL; } } /** * 翻译官（Adepter适配器） * Class Translator */ class Translator extends player { private $foreignCenter; public function __construct(string $name) { $this-&gt;foreignCenter = new ForeignCenter($name); } public function Attack() { $this-&gt;foreignCenter-&gt;ForeignAttack(); } public function Defense() { $this-&gt;foreignCenter-&gt;ForeignDefense(); } } $forwards = new Forwards('詹姆斯'); $forwards-&gt;Attack(); $guards = new Guards('艾佛森'); $guards-&gt;Attack(); $center = new Translator('姚明'); $center-&gt;Attack(); $center-&gt;Defense(); 执行结果： &gt; php Adapter.php &gt; 前锋詹姆斯进攻 &gt; 后卫艾佛森进攻 &gt; 外籍中锋姚明进攻 &gt; 外籍中锋姚明防守 总结💯 前面也说过`适配器模式`的使用场景，这里再强调一遍，就是`适配器模式`尽量不要滥用，模式乱用不如不用， 如果在程序开发中，能够及早、提前的规划，事先预防接口不同、不匹配的问题， 在有小的接口不统一时能够及早发现，及时重构，不让问题继续扩大，而当碰到无法改变原有设计和代码的情况时，才考虑去适配， 这样就能做到事后控制不如事中控制，事中控制不如事前控制。 ","link":"http://mofish.pily.life/post/design_patterns_06/"},{"title":"硬啃设计模式：七大设计原则","content":"🙌这一章我们放松一下，先不学新的设计模式，我们来学一下简单的设计原则， 👉所以接下来我会给大家带来7️⃣中设计原则的相关知识概念，也是学习设计模式前需要重点理解的。 目录 简述 开闭原则 依赖倒置原则 单一职责原则 接口隔离原则 迪米特法则 里氏替换原则 合成复用原则 简述🗣🗣 这7中设计原则是软件设计模式中必须尽量遵循的原则，是设计模式的基础， 在实际的开发过程中，并不是一定要求所有代码都遵循设计原则， 而是要综合人力、时间、成本、质量等实际情况，在适当的场合遵循适当的设计原则，是一种平衡取舍。 各种原则要求的侧重点不同，我们先用一句话来归纳阔过，让大家有个印象之后再深入理解： 设计原则 一句话归纳 目的 开闭原则 对扩展开放，对修改关闭 降低维护带来的新风险 依赖倒置原则 高层不应该依赖低层，要面向接口编程 更利于代码结构的升级扩展 单一职责原则 一个类只干一件事，实现类要单一 便于理解，提高代码的可读性 接口隔离原则 一个接口只干一件事，接口要精简单一 功能解耦，高聚合、低耦合 迪米特法则 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 只和朋友交流，不和陌生人说话，减少代码臃肿 里氏替换原则 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 防止继承泛滥 合成复用原则 尽量使用组合或者聚合关系实现代码复用，少使用继承 降低代码耦合 实际上，这些原则的目的只有一个：降低对象之间的耦合，增加程序的可复用性、可扩展性和可维护性。 记忆口诀：访问加限制，函数要节俭，依赖不允许，动态加接口，父类要抽象，扩展不更改。 在程序设计时，我们应该将程序功能最小化，每个类只干一件事。若有类似功能基础之上添加新功能，则要合理使用继承。对于多方法的调用，要会运用接口，同时合理设置接口功能与数量。最后类与类之间做到低耦合高内聚。 1️⃣开闭原则👇👇 定义🥇 软件实体对扩展开放，对修改关闭，这就是开闭原则的经典定义🤓 这里的软件实体包括以下几个部分： 项目中划分出的模块 类与接口 方法 作用🥈 开闭原则是面向对象程序设计的终极目标，它使软件实体拥有一定的适应性和灵活性的同时具备稳定性和延续性。 具体来说，其作用如下： 提高代码稳定性 新增功能时不修改现在代码，而且在原有基础上进行扩展，测试时也只需要对扩展代码继续测试即可 提高代码可复用性 粒度越小，被复用的可能性就越大；在面向对象的程序设计中，根据原子和抽象编程可以提高代码的可复用性。 提高软件的可维护性 遵守开闭原则的软件，其稳定性高和延续性强，从而易于扩展和维护。 实现方法🥉 可以通过“抽象约束、封装变化”来实现开闭原则，即通过接口或者抽象类为软件实体定义一个相对稳定的抽象层，而将相同的可变因素封装在相同的具体实现类中。 因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生来的实现类来进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。 2️⃣依赖倒置原则👇👇 定义🥇 高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象。 其核心思想是：要面向接口编程，不要面向实现编程。 依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。 由于在软件设计中，细节具有多变性，而抽象层则相对稳定，因此以抽象为基础搭建起来的架构要比以细节为基础搭建起来的架构要稳定得多。这里的抽象指的是接口或者抽象类，而细节是指具体的实现类。 使用接口或者抽象类的目的是制定好规范和契约，而不去涉及任何具体的操作，把展现细节的任务交给它们的实现类去完成。 作用🥈 可以降低类间的耦合性 可以提高系统的稳定性 可以减少并行开发引起的风险 可以提高代码的可读性和可维护性 实现方法🥉 依赖倒置原则的目的是通过要面向接口的编程来降低类间的耦合性，所以我们在实际编程中只要遵循以下4点，就能在项目中满足这个规则。 每个类尽量提供接口或者是抽象类，或者两者都具备 变量的声明尽量是接口或者是抽象类 任何类都不应该直从具体类派生 使用类的继承时尽量遵循里氏替换原则 3️⃣单一职责原则👇👇 定义🥇 单一职责原则规定一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分 该原则提出对象不应该承担太多职责，如果一个对象承担了太多的职责，至少存在以下两个缺点： 一个职责的变化可能会削弱或者抑制这个类实现其他职责的能力； 当客户端需要该对象的某一个职责时，不得不将其他不需要的职责全都包含进来，从而造成冗余代码或代码的浪费。 作用🥈 单一职责原则的核心就是控制类的粒度大小、将对象解耦、提高其内聚性。如果遵循单一职责原则将有以下优点: 降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。 实现方法🥉 单一职责原则是最简单但又最难运用的原则，需要设计人员发现类的不同职责并将其分离，再封装到不同的类或模块中。 4️⃣接口隔离原则👇👇 定义🥇 接口隔离原则（Interface Segregation Principle，ISP）要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。 客户端不应该被迫依赖于它不使用的方法。 一个类对另一个类的依赖应该建立在最小的接口上。 以上两个类的含义是：要为各个类简历他们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 接口隔离原则和单一职责都是为了提高类的内聚性、降低它们之间的耦合性，体现了封装的思想，但两者是不同的 单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。 单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要约束接口，主要针对抽象和程序整体框架的构建。 作用🥈 接口隔离原则是为了约束接口、降低类对接口的依赖性，遵循接口隔离原则有以下 5 个优点: 将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 接口的粒度大小定义合理，能够保证系统的稳定性。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响。 实现方法🥉 在具体应用接口隔离原则时，应该根据以下几个规则来衡量： 接口尽量小，但是要有限度。一个接口只服务于一个子模块或业务逻辑。 为依赖接口的类定制服务。只提供调用者需要的方法，屏蔽不需要的方法。 了解环境，拒绝盲从。每个项目或产品都有选定的环境因素，环境不同，接口拆分的标准就不同深入了解业务逻辑 提高内聚，减少对外交互。使接口用最少的方法去完成最多的事情。 5️⃣迪米特法则👇👇 定义🥇 迪米特法则又叫做最少知识原则，其定义的通俗说法就是：只与你的直接朋友交谈，不跟陌生人说话。 其含义是：如果两个软件实体无需直接通信，那么就不应当发生直接的相互调用，可与i通过第三方转发改调用，其目的是降低类之间的耦合度，提高模块的相对独立性。 迪米特法则种的“朋友”指的是当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象和当前对象存在关联、聚合或者组合的关系，可以直接访问这些对象的方法。 作用🥈 迪米特法则要求限制软件实体之间通信的宽度和深度，正确的使用迪米特法则有以下两个有点： 降低类之间的耦合度，提高了模块的相对独立性 由于亲合度降低，从而提高了类的可复用率和系统的扩展性 但是，过度的使用迪米特法则会使系统产生大量的中介类，从而增加了系统的复杂性，使得模块之间的通信效率降低，所以使用时需要反复权衡，在确保高内聚和低耦合的同时，保证系统的结构清晰。 实现方法🥉 从迪米特法则的定义和特点可知，它强调以下两点： 从依赖者的角度来说，只依赖应该依赖的对象。 从被依赖者的角度说，只暴露应该暴露的方法。 所以，在运用迪米特法则时要注意以下 6 点： 在类的划分上，应该创建弱耦合的类。类与类之间的耦合越弱，越有利于复用。 在类的结构设计上，尽量降低类成员的访问权限 在类的设计上，优先考虑将一个类设置成不变类 在对其它类的引用上，讲引用其它对象的次数讲到最低 不暴露类的属性成员，而应该提供相应的访问器(set和get) 谨慎使用序列化功能 6️⃣里氏替换原则👇👇 定义🥇 继承必须确保超类所拥有的性质在子类种仍然成立。 里氏替换原则主要阐述了有关继承的一些原则，而且其原始继承复用的基础，它反映了基类与子类之间的关系，时对开闭原则的补充，是对实现抽象化的具体步骤的规范。 作用🥈 里氏替换原则的主要作用如下： 它是实现开闭原则的重要方式之一 它克服了继承中重写父类造成的可复用性变差的缺点 它是动作正确性的保证。即类的扩展不会给已有的系统引入新的错误，降低了代码出错的可能性 加强程序的健壮性，同时变更时可以做到非常耗的兼容性，提高程序的维护性、可扩展性，降低需求变更时引入的风险 实现方法🥉 里氏替换原则通俗来讲就是：子类可以扩展父类的功能，但不能改变父类原有的功能。 也就是说：子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。 根据上述理解，对里氏替换原则的定义可以总结如下： 子类可以实现父类的抽象方法 ，但不能覆盖父类的非抽象方法 子类中可以增加自己特有的方法 当子类的方法重载父类的方法时，方法的前置条件(即方法的输入参数)要比父类的方法更加宽松 当子类的方法实现父类的方法时（重写/重载或实现抽象方法），方法的后置条件（即方法的的输出/返回值）要比父类的方法更严格或相等 通过重写父类的方法来完成新的功能写起来虽然简单，但是整个继承体系的可复用性会比较差，特别是运用多态比较频繁时，程序运行出错的概率会非常大。 如果程序违背了里氏替换原则，则继承类的对象在基类出现的地方会出现运行错误。这时其修正方法是：取消原来的继承关系，重新设计它们之间的关系。 关于里氏替换原则的例子，最有名的是“正方形不是长方形”。当然，生活中也有很多类似的例子，例如，企鹅、鸵鸟和几维鸟从生物学的角度来划分，它们属于鸟类；但从类的继承关系来看，由于它们不能继承“鸟”会飞的功能，所以它们不能定义成“鸟”的子类。同样，由于“气球鱼”不会游泳，所以不能定义成“鱼”的子类；“玩具炮”炸不了敌人，所以不能定义成“炮”的子类等。 6️⃣合成复用原则👇👇 定义🥇 合成复用原则又叫组合/聚合复用原则，它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。 如果要使用继承关系，则必须验证遵守里氏替换原则。合成复用原则和里氏替换原则相辅相成，两者都是开闭原则的具体实现规范。 作用🥈 通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点: 继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化。 采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能，它有以下优点: 它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。 实现方法🥉 合成复用原则是通过将已有的对象纳入新对象中，作为新对象的成员对象来实现的，新对象可以调用已有对象的功能，从而达到复用。 ","link":"http://mofish.pily.life/post/design_patterns_principle/"},{"title":"硬啃设计模式(五)：建造者(生成器)模式","content":"😭 已经好久没更新了，懒惰的我，不可原谅👻 😤 其实最近在备考事业单位的考试，但是呢，感觉一个月的时间还是不够的，明年再战吧 👉👉 所以后续就会继续更新设计模式的学习啦，学习还是不能停的👏 简述👇👇 经过了前面几章的学习，今天我们带来的是`建造者`模式，又叫`生成器`模式，是最后一个要学习创建型模式😁 在平时我们的程序设计中，一些复杂的对象可能是由许多子部件，按照一定的步骤组合而成的，而现实中也有许多这样的例子， 如游戏中的不同角色，其性别、个性、体型、种族、样貌等等都有所差别，但是组成方式却大同小异， 又如现实中的房子，其装修的材料、家具、格局、设计风格都有所差别，但是建造方式却类似， 针对这些由多个不见构成的对象，各个部件可以灵活选择，但是创建步骤大同小异的情况线下，可以选择`建造者`模式。 概念👇👇 指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示，这样的设计模式被称为建造者模式。 它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。 它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 主要角色👇👇 🤵产品角色(Product)：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 👶抽象建造者(Builder)：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 👨具体建造者(Concrete Builder)：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 💂指挥者(Director)：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 优缺点 优点⭕️： 1️⃣封装性好，构建和表示分离 2️⃣扩展性好，各个具体的建造者相互独立，有利于系统的解耦 3️⃣客户端不必知道产品内部组成的细节，建造者可以对创建过程逐步细化，而不对其它模块产生任何影响，便于控制细节风险 缺点❌： 1️⃣产品的组成部分必须相同，这限制了其使用范围 2️⃣如果产品的内部变化复杂，如果产品内部发生变化，则建造者也要同步修改，后期维护成本较大 建造者(生成器)模式UML图👇👇 建造者(生成器)模式代码示例👇👇 实现类 Hamburger.php &lt;?php include_once &quot;./Builder.php&quot;; /** * 产品角色 * Class Product */ class Product { public $steps; public function add($step) { $this-&gt;steps[] = $step; } public function show() { foreach ($this-&gt;steps as $step) { echo $step . PHP_EOL; } } } /** * 指挥者 * Class Director */ class Director { public function build(HamburgerBuilder $builder) { $builder-&gt;buildStepA(); $builder-&gt;buildStepB(); return $builder-&gt;getResult(); } } $builder = new BanshaoHamburgerBuilder(); $director = (new Director())-&gt;build($builder); $director-&gt;show(); 建造者类 Builder.php &lt;?php include_once &quot;./Hamburger.php&quot;; /** * 汉堡包抽象建造者 * Interface HamburgerBuilder */ interface HamburgerBuilder { public function buildStepA(); public function buildStepB(); public function getResult(); } /** * 板烧鸡腿堡建造者 * Class BanshaoHamburgerBuilder */ class BanshaoHamburgerBuilder implements HamburgerBuilder { public $product; public function __construct() { $this-&gt;product = new Product(); } public function buildStepA() { $this-&gt;product-&gt;add('准备一块`板烧鸡腿`'); } public function buildStepB() { $this-&gt;product-&gt;add('用面包把`板烧鸡腿`夹住，加点生菜和沙拉酱'); } public function getResult() { $this-&gt;product-&gt;add('`板烧鸡腿`堡好了'); return $this-&gt;product; } } /** * 鳕鱼煲建造者 * Class XueyuHamburgerBuilder */ class XueyuHamburgerBuilder implements HamburgerBuilder { public $product; public function __construct() { $this-&gt;product = new Product(); } public function buildStepA() { $this-&gt;product-&gt;add('准备一块`鳕鱼`'); } public function buildStepB() { $this-&gt;product-&gt;add('用面包把`鳕鱼`夹住，加点生菜和沙拉酱'); } public function getResult() { $this-&gt;product-&gt;add('`鳕鱼`堡好了'); return $this-&gt;product; } } 运行结果： &gt; php Hamburger.php &gt; 准备一块`板烧鸡腿` &gt; 用面包把`板烧鸡腿`夹住，加点生菜和沙拉酱 &gt; `板烧鸡腿`堡好了 与工厂模式的区别❓❓ ⚪️建造者模式更加注重方法的调用顺序，工厂模式更注重创建对象 ⚫️粒度大小不同，厂方法模式创建的产品一般都是单一性质产品，而建造者模式创建的则是一个复合产品，它由各个部件复合而成，部件不同产品对象当然不同。 🔵关注重点不用，工厂模式只需要把对象创建出来即可，而建造者模式还需要关注该对象由哪些部件组成 🔴建造者模式根据建造过程中的顺序不一样，最终对象部件组成也不一样。 总结 So，经过上面的介绍，大家都已经基本了解建造者模式了，其实很多设计模式我们在平时工作的时候也会不自觉的用到，只是自己没发现。 但是当你接触了，学习了之后，下次遇到代码设计的问题时，就能够用更好的设计模式来处理，使得代码更加简洁易懂啦。 那么今天的学习到这里，后面就得开始学习另外的结构型模式和行为型模式咯，拜🖐 ","link":"http://mofish.pily.life/post/design_patterns_05/"},{"title":"硬啃设计模式(四)：原型模式","content":"👴👴 害，年后工作繁忙呀，都没事学习了，又得做新的项目了，只能硬挤一点时间出来整理😭 行吧😤 废话不多数，今天整理的是创建型模式中的第四种：原型模式👏👏👏 概念👇👇 原始模式属于`创建型模式`，是从一个对象，通过复制来创建一个相同或者类似的可定制的新对象，而不需要知道任何的创建细节。 值得注意的是，通过克隆方式拷贝出来的对象是一个新的对象，对其修改不会影响原型对象的属性。 使用场景👇👇 其实原型模式的原理就类似我们程序员中的`复制`和`粘贴`，有时候需要写重复代码，但是有部分要改动的时候，就会复制粘贴，然后改动。 例如下面简历的例子，通过深拷贝，在原本奖励的基础上重新修改工作经历，而不用重新填写用户的基本资料。 主要角色👇👇 🙈Prototype（抽象原型类）：它是声明克隆方法的接口，是所有具体原型类的公共父类，可以是抽象类、接口或者是具体实现类。 🙉ConcretePrototype（具体原型类）：它实现在抽象原型类中声明的克隆方法，在克隆方法中返回自己的一个克隆对象。 🙊Client（客户类）：使用原型对象的客户程序。 优缺点👇👇 优点⭕️： 1️⃣性能提高，规避了构造函数的约束 2️⃣辅助实现撤销操作 3️⃣简化对象的创建 缺点❌： 1️⃣要为每一个类都配置一个 clone 方法 2️⃣clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则 3️⃣当代码复杂，对象嵌套较多时，要实现深克隆较为复杂 “原型模式”UML图👇👇 “原型模式”代码实例 &lt;?php /** * 简历接口抽象类(抽象原型类) * Class CloneResume */ abstract Class CloneResume { public $name; public $sex; public $age; public $work; abstract function __clone(); } /** * 简历类(具体原型类) * Class Resume */ class Resume extends CloneResume { public function __construct($name) { $this-&gt;name = $name; $this-&gt;work = new WorkExperience(); } /** * 设置个人信息 * @param string $sex * @param string $age */ public function setPersonalInfo($sex, $age) { $this-&gt;sex = $sex; $this-&gt;age = $age; } /** * 设置工作经历 * @param string $workData * @param string $company */ public function setWorkExperience($workData, $company) { $this-&gt;work-&gt;setCompany($company); $this-&gt;work-&gt;setWorkDate($workData); } public function display() { echo $this-&gt;name . '-' . $this-&gt;sex . '-' . $this-&gt;age . PHP_EOL; echo '工作经历：' . $this-&gt;work-&gt;getCompany() . '-' . $this-&gt;work-&gt;getWorkDate() . PHP_EOL; } /** * 重新clone方法，实现深拷贝 */ public function __clone() { $this-&gt;work = clone $this-&gt;work; } } /** * 工作经历：一份简历可能包含多段工作经理 * Class WorkExperience */ class WorkExperience { private $workDate; // 工作时间 private $company; // 公司名称 /** * 设置工作时间 */ public function setWorkDate($workDate) { return $this-&gt;workDate = $workDate; } /** * 设置公司名称 */ public function setCompany($company) { return $this-&gt;company = $company; } /** * 获取工作时间 */ public function getWorkDate() { return $this-&gt;workDate; } /** * 获取公司名称 */ public function getCompany() { return $this-&gt;company; } } $resumeA = new Resume(&quot;小明&quot;); $resumeA-&gt;setPersonalInfo('男', '18'); $resumeA-&gt;setWorkExperience('2019-7-1至2020-7-1', '牛皮科技有限公司'); $resumeB = clone $resumeA; $resumeB-&gt;setWorkExperience('2020-8-1至2021-2-1', '瓜皮科技有限公司'); $resumeA-&gt;display(); $resumeB-&gt;display(); 图二：执行结果☝️☝️ 总结👇👇 👌至此，`原型模式`就介绍完毕啦，其实我们也可以把`原型模式`成为`克隆模式`，这样字面上的意思就更容易理解了，鸣人的`影分身`术都熟悉了吧😏 ","link":"http://mofish.pily.life/post/design_patterns_04/"},{"title":"硬啃设计模式(三)：工厂模式之抽象工厂模式","content":"🕺💃 哇~几天没见，甚是思念，又到了晚上学习的时候了哈哈哈😭哈。。哈😭😭 🗣🗣 今天为大家带来的时候抽象工厂模式，而且还会有抽象工厂模式的优化版噢🤤 概念👇👇 抽象工厂模式（Abstract Factory Pattern）隶属于设计模式中的创建型模式， 指当有多个抽象角色时使用的一种工厂模式，用于产品族的构建。 因此抽象工厂模式是工厂方法模式的升级版本， 工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用场景👇👇 1️⃣系统中有多个产品族，每个具体工厂创建同一族但是不同等级结构的产品 👉如：代码示例中有麦当劳和肯德基两个工厂，每个工厂包含了汉堡包和鸡块两个不同等级接口的产品组成的产品族 2️⃣系统一次只能消费其中某一族产品，即同族产品一起使用 👉如：调用示例中，实例化了McDonaldAbstractRestaurantFactory麦当劳餐厅工厂的话，只能使用麦当劳的产品 图一：产品等级和产品族☝️☝️ 主要角色👇👇 🏭抽象工厂（Abstract Factory）： 提供了创建产品的接口，它包含多个创建产品的方法， 如getHamburger()、getChickenNugget()，可以创建多个不同等级的产品。 🏭具体工厂（Concrete Factory）： 主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 🍔抽象产品（Product）： 定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品，如Hamburger、ChickenNugget。 🍔具体产品（Concrete Product）： 实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 优缺点👇👇 优点⭕️： 1️⃣可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理 2️⃣当需要产品族时，抽象工厂可以保证客户端始终只使用同一个产品的产品组 3️⃣抽象工厂增强了程序的可扩展性，当增加一个新的产品族时，不需要修改原代码，满足开闭原则。 缺点❌： 1️⃣当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改，此时不满足开闭原则 例如增加一个薯条产品时，所有的工厂类都要加一个获取薯条产品的方法，增加了系统的抽象性和理解难度。 Ps❗️❗️❗️❗️： 1️⃣根据优缺点来看，抽象工厂模式的扩展有一定的“开闭原则”倾斜性 2️⃣当系统中只存在一个等级结构的产品时，抽象工厂模式将退化到工厂方法模式 “抽象工厂模式”UML图👇👇 “抽象工厂模式”代码实例 AbstractFactory.php &lt;?php // 引入两个产品类 require_once &quot;../ChickenNugget.php&quot;; require_once &quot;../Hamburger.php&quot;; /** * 抽象工厂模式 * * 抽象餐厅工厂 * Interface AbstractRestaurantFactory */ interface AbstractRestaurantFactory { /** 汉堡包 */ public function getHamburger(); /** 鸡块 */ public function getChickenNugget(); } /** * 麦当劳餐厅 * Class McDonaldAbstractRestaurantFactory */ class McDonaldAbstractRestaurantFactory implements AbstractRestaurantFactory { /** 汉堡包 */ public function getHamburger() { return new McDonaldHamburger(); } /** 鸡块 */ public function getChickenNugget() { return new McDonaldChickenNugget(); } } /** * 肯德基餐厅 * Class KFCAbstractRestaurantFactory */ class KFCAbstractRestaurantFactory implements AbstractRestaurantFactory { /** 汉堡包 */ public function getHamburger() { return new KFCHamburger(); } /** 鸡块 */ public function getChickenNugget() { return new KFCChickenNugget(); } } // 麦当劳餐厅 $mcDonald = new McDonaldAbstractRestaurantFactory(); $mcDonaldHamburger = $mcDonald-&gt;getHamburger(); $mcDonaldChickenNugget = $mcDonald-&gt;getChickenNugget(); $mcDonaldHamburger-&gt;getName(); $mcDonaldChickenNugget-&gt;getName(); // 肯德基餐厅 $kfc = new KFCAbstractRestaurantFactory(); $kfcHamburger = $kfc-&gt;getHamburger(); $kfcChickenNugget = $kfc-&gt;getChickenNugget(); $kfcHamburger-&gt;getName(); $kfcChickenNugget-&gt;getName(); 图二：执行结果☝️☝️ 总结👇👇 👌至此，抽象工厂模式就介绍完毕，但是上面我们提到了，抽象工厂模式有一个缺点， 假如当我需要新增一个“薯条”产品时，那么我们处理要新增一个`抽象产品类`和两个`具体产品类`之外， 我们还需要修改所有的`工厂类`，为其提供一个获取“薯条”产品的方法，这样改动也太不友好了😭 👉另外还有一个问题，如果程序中有多个使用getHamburger()的地方， 那么如果每个使用的地方都需要new McDonaldAbstractRestaurantFactory()的话， 后续如果要改成new KFCAbstractRestaurantFactory()那不得找出每个地方都改掉？？想想就心痛💩💩 PS👇👇 想知道优化方法的话可以看看“大话设计模式-第15章 就不能不换DB吗？——抽象工厂模式“ 或者可以看看我的git代码示例噢😆 ","link":"http://mofish.pily.life/post/design_patterns_03/"},{"title":"硬啃设计模式(二)：工厂模式之工厂方法模式","content":"🙋‍♂ 来啦来啦！今天带来的时候工厂模式的第二种-工厂方法模式😻 👉 工厂方法模式该模式主要是为了弥补简单工厂模式每次新增角色时都需要修改工厂类的缺点。 概念👇👇 `工厂方法`模式属于“创建型模式”，该模式主要是为了弥补`简单工厂模式`的一个显著缺点🚫 就是类型的创建是依赖工厂类的，每次新增角色(Product)时，都必须对工厂类进行修改，使得该类职责过重🤦‍♀ 为了解决这个问题，就用了工厂方法模式🏭 创建一个工厂接口(interface Factory)和多个工厂实现类(class ProductFactory implements Factory) 这样一旦需要新增角色(Product)时，直接增加新的工厂实现类即可(ProductFactory)，无需再修改旧的代码了 ##“工厂方法模式”UML图👇👇 ##“工厂方法模式”代码实例👇👇 接下来，我们来看一下工厂接口和多个工厂实现类的创建实例： FactoryMethod.php &lt;?php include_once &quot;../Hamburger.php&quot;; /** * 工厂方法模式： * 汉堡包工厂接口类 * Interface RestaurantFactory */ interface HamburgerFactory { /** * 获取汉堡包实例 * @return mixed */ public function getHamburger(); } /** * 麦当劳餐厅工厂 * Class HamburgerRestaurantFactory */ class McDonaldHamburgerFactory implements HamburgerFactory { /** * 获取汉堡包餐厅实例 * @return mixed */ public function getHamburger() { return new McDonaldHamburger(); } } /** * KFC餐厅工厂 * Class PizzaRestaurantFactory */ class KFCHamburgerFactory implements HamburgerFactory { /** * 获取披萨餐厅实例 * @return mixed */ public function getHamburger() { return new KFCHamburger(); } } $mcDonaldRestaurantFactory = new McDonaldHamburgerFactory(); $mcDonaldRestaurant = $mcDonaldRestaurantFactory-&gt;getHamburger(); // 获取实例 $mcDonaldRestaurant-&gt;getName(); $mcDonaldRestaurant-&gt;getMaterial(); Hamburger.php &lt;?php /** * 抽象角色，所有汉堡包对象的父类 * 用于描述汉堡包的所有功能方法 * Interface Hamburger */ interface Hamburger { /** 汉堡包名称 **/ public function getName(); /** 汉堡包材料 **/ public function getMaterial(); } /** * 麦当劳汉堡包 * Class McDonaldHamburger */ class McDonaldHamburger implements Hamburger { /** 汉堡包名称 **/ public function getName() { echo '这是你的`板烧鸡腿堡`' . PHP_EOL; } /** 汉堡包材料 **/ public function getMaterial() { echo '两块面包、一颗生菜、一丢丢沙拉酱、最重要的是一块板烧鸡扒' . PHP_EOL; } } /** * 肯德基汉堡包 * Class KFCHamburger */ class KFCHamburger implements Hamburger { /** 汉堡包名称 **/ public function getName() { echo '这是你的`奥尔良鸡腿堡`' . PHP_EOL; } /** 汉堡包材料 **/ public function getMaterial() { echo '两块面包、一颗生菜、一丢丢沙拉酱、最重要的是一块奥尔良鸡扒' . PHP_EOL; } } 总结👇👇 经过上面的代码实例和类关系图，我们很好的了解了“工厂方法模式”的概念和结构😁 其实就是对“简单工厂模式”的一个优化，降低工厂类的职责⤵️⤵️ So，经过上一章的学习，这章也就非常好理解啦，那今天的学习就到这里，拜拜ヾ(•ω•`)o👋👋 ","link":"http://mofish.pily.life/post/design_patterns_02_02/"},{"title":"硬啃设计模式(二)：工厂模式之简单工厂模式","content":"👏 噢！我又来了，接下来第二章我们开始学习另外一个创建型模式-工厂模式，而其又分为三种 这三种工厂模式在接下来的三章中我们一一讲解，免得放在一章里面太长了，顺便分开写还可以水多几篇😈 ☝️ 简单工厂模式（本章内容） 1️⃣ 普通简单工厂模式 2️⃣ 多方法简单工厂模式 3️⃣ 静态方法简单工厂模式 ✌️ 工厂方法模式（可退化为简单工厂模式） 👌 抽象工厂模式 好滴👏👏那就让我们开始进入简单工厂模式的学习吧！ 概念👇👇 简单工厂模式属于“创建型模式”，通过专门定义一个工厂类来负责创建其他类的实例， 被创建的实例通常具有相同的父类 如下面的例子()： `Restaurant.php`中`HamburgerRestaurant`(汉堡包餐厅)和`PizzaRestaurant`(披萨餐厅)都实现了`Restaurant`类， 而且都`SimpleRestaurantFactory`(工厂类)来实例化它们。 具体分类👇👇 1️⃣工厂（Factory）角色：简单工厂模式的核心，它负责实现创建所有实例的内部逻辑。工厂类方法可被外部调用，用于创建对应的产品对象 2️⃣抽象（Product）角色：所有对象的父类，它负责描述所有实现类需要实现的功能方法 3️⃣具体产品（ConcreteProduct）角色：具体实例对象 优缺点👇👇 优点✔️ 用户使用时，可以直接根据自己所需传入对应的类型参数来创建所需实例， 而无需了解这些是如何创建的、如何组织的。有利于整个软件体系结构的优化。 缺点❌ 简单工厂模式的缺点也体现在工厂类(Factory)上，由于工厂类集中了所有实例的创建逻辑， 每次增加新的实例对象时，都修要修改工厂类，使得该类职责过重， 而且不符合开闭原则(不属于23中设计模式)，因此在“高内聚”和“扩展性”上面不是很好。 ##“普通简单工厂”UML图👇👇 “普通简单工厂”代码实例👇👇 Restaurant.php 定义了👮抽象角色（Restaurant）和👮‍♀产品角色（HamburgerRestaurant、PizzaRestaurant） &lt;?php /** * 抽象角色，所有对象的父类 * 它负责描述所有实现类需要实现的功能方法 * Interface Restaurant */ interface Restaurant { /** 下单 **/ public function placeOrder(); /** 备餐 **/ public function mealPreparation(); /** 上菜 **/ public function dishServe(); } /** * 具体实例对象：汉堡包餐厅 * Class HamburgerRestaurant */ class HamburgerRestaurant implements Restaurant { /** 下单 **/ public function placeOrder() { echo '欢迎光临，你点了一份`板烧鸡腿堡`套餐，请拿好票在右边候餐' . PHP_EOL; } /** 备餐 **/ public function mealPreparation() { echo '不好意思，你是78号，这是18号，你的已经在做了，在稍等一下' . PHP_EOL; } /** 上菜 **/ public function dishServe() { echo '78号！78号！板烧鸡腿堡套餐好了，请来取餐' . PHP_EOL; } } /** * 具体实例对象：披萨餐厅 * Class PizzaRestaurant */ class PizzaRestaurant implements Restaurant { /** 下单 **/ public function placeOrder() { echo '欢迎光临，你点了一份`日式照烧鳗鱼披萨`，请稍等......' . PHP_EOL; } /** 备餐 **/ public function mealPreparation() { echo '别催啦，很快就好了，披萨是这么久的，想快的话过去隔壁吃汉堡包吧！' . PHP_EOL; } /** 上菜 **/ public function dishServe() { echo '来啦来啦，B250座的披萨，久等了，快吃吧！' . PHP_EOL; } } SimpleRestaurantFactory.php 定义了简单餐厅工厂，以及获取产品实例化对象的方法 &lt;?php include_once &quot;./Restaurant.php&quot;; /** * 简单餐厅工厂 * Class SimpleRestaurantFactory */ class SimpleRestaurantFactory { public function getRestaurant(string $type) { switch ($type) { case 'hamburger': echo '你走进了汉堡包店' . PHP_EOL; return new HamburgerRestaurant(); case 'pizza': echo '你走进了披萨店' . PHP_EOL; return new PizzaRestaurant(); default: echo '请选择正确的餐厅' . PHP_EOL; return false; } } } $restaurant = new SimpleRestaurantFactory(); // pizza餐厅 $pizzaRestaurant = $restaurant-&gt;getRestaurant('pizza'); // 获取实例 $pizzaRestaurant-&gt;placeOrder(); // 下单 $pizzaRestaurant-&gt;mealPreparation(); // 备餐 $pizzaRestaurant-&gt;dishServe(); // 上菜 echo '感觉有点吃不饱要咋整......' . PHP_EOL; // hamburger餐厅 $hamburgerRestaurant = $restaurant-&gt;getRestaurant('hamburger'); $hamburgerRestaurant-&gt;placeOrder(); $hamburgerRestaurant-&gt;mealPreparation(); $hamburgerRestaurant-&gt;dishServe(); echo '溜了溜了，撑死了......' . PHP_EOL; 普通简单工厂还有另外两种形式👇👇 ☝️ 多方法简单工厂（MethodsRestaurantFactory.php）： 是对普通简单工厂模式的一种改进，在普通简单工厂模式中，如果传递的类型有误，则不能正确的创建对象， 而多方法简单工厂模式，则是在工厂类中提供多个工厂方法，对应的实例化不用的产品对象。 ✌️ 静态方法简单工厂（StaticRestaurantFactory.php） 进一步改进，对工厂方法置为静态方法，无需创建实例，直接调用方法即可。 ❗️多方法简单工厂（MethodsRestaurantFactory.php）示例： &lt;?php include_once &quot;./Restaurant.php&quot;; /** * 多方法简单工厂 * Class MethodsRestaurantFactory */ class MethodsRestaurantFactory { public function getHamburgerRestaurant() { echo '你走进了汉堡包店' . PHP_EOL; return new HamburgerRestaurant(); } public function getPizzaRestaurant() { echo '你走进了披萨店' . PHP_EOL; return new PizzaRestaurant(); } } $restaurant = new MethodsRestaurantFactory(); // pizza餐厅 $pizzaRestaurant = $restaurant-&gt;getPizzaRestaurant(); $pizzaRestaurant-&gt;placeOrder(); $pizzaRestaurant-&gt;mealPreparation(); $pizzaRestaurant-&gt;dishServe(); echo '感觉有点吃不饱要咋整......' . PHP_EOL; // hamburger餐厅 $hamburgerRestaurant = $restaurant-&gt;getHamburgerRestaurant(); $hamburgerRestaurant-&gt;placeOrder(); $hamburgerRestaurant-&gt;mealPreparation(); $hamburgerRestaurant-&gt;dishServe(); ❗️静态方法简单工厂（StaticRestaurantFactory.php）示例： &lt;?php include_once &quot;./Restaurant.php&quot;; /** * 静态方法简单工厂 * Class StaticRestaurantFactory */ class StaticRestaurantFactory { public static function getHamburgerRestaurant() { echo '你走进了汉堡包店' . PHP_EOL; return new HamburgerRestaurant(); } public static function getPizzaRestaurant() { echo '你走进了披萨店' . PHP_EOL; return new PizzaRestaurant(); } } // pizza餐厅 $pizzaRestaurant = StaticRestaurantFactory::getPizzaRestaurant(); $pizzaRestaurant-&gt;placeOrder(); $pizzaRestaurant-&gt;mealPreparation(); $pizzaRestaurant-&gt;dishServe(); echo '感觉有点吃不饱要咋整......' . PHP_EOL; // hamburger餐厅 $hamburgerRestaurant = StaticRestaurantFactory::getHamburgerRestaurant(); $hamburgerRestaurant-&gt;placeOrder(); $hamburgerRestaurant-&gt;mealPreparation(); $hamburgerRestaurant-&gt;dishServe(); 总结👇👇 这......就这？这有啥好总结的？这都不会吗？😤 感觉简单功能模式还是比较好理解的😁，定义一个抽象产品类，不同的具体产品都对其进行实现 再定义一个工厂对外提供实例化接口，从而让用户传递类型即可创建所需实例。 So，`简单工厂模式`就总结完啦，下一章我们来学习`工厂方法模式`，请大家坚持学习噢👏👏👏 ","link":"http://mofish.pily.life/post/design_patterns_02_01/"},{"title":"硬啃设计模式(一)：单例模式","content":"👏 今天开始就正式开始重新学习设计模式啦！！ 💯 也算是为了跳槽而开始复习吧，由于之前的博客内容不小心清空了😤，所以一切从0开始吧 好吧，别调皮了，我们继续学习吧。 概念👇👇 单例模式，是创建型模式中的一种。 整个应用中某个类只有一个对象实例的设计模式， 注意！是只有！一个对象实例✔️ 具体来说，作为对象的创建方式，单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统全局的提供这个实例。 它不会创建实例副本，而是会向单例类内部存储的实例返回一个引用。 特点👇👇 单例模式的主要特点是“三私一公” - 需要一个私有静态成员变量来保存类的唯一实例； - 构造函数必须声明为私有的，防止外部程序new一个对象从而失去单例的意义； - 克隆函数必须声明为私有的，防止对象被克隆； - 需要一个公共静态方法(通常命名为getInstance)来返回唯一实例的引用。 使用场景👇👇 如果每次操作数据库都要重新实例化，对程序和数据来说也增大了系统和内存的消耗。 而单例模式返回的唯一实例引用，可大大减少new操作的次数， 从而减少初始化连接数据库操作，可避免‘too many connections’的情况。 代码示例👇👇 class Singleton { /** * 私有静态成员变量，保存类的唯一实例 * @var */ private static $_instance = null; private $pdo; /** * 构造函数私有，防止外部实例化 * Singleton constructor. */ private function __construct() { try { echo '开始链接数据库......'. PHP_EOL; $this-&gt;pdo = new PDO('mysql:host=localhost;dbname=larabbs;port=3306;', 'root', ''); }catch (PDOException $exception) { trigger_error('数据库连接失败'.$exception-&gt;getMessage(), E_USER_ERROR); } } /** * 克隆函数私有，方式外部克隆对象 */ private function __clone() { trigger_error('禁止clone', E_USER_ERROR); } /** * 访问实例的公共静态方法 * @return Singleton|null */ public static function getInstance() { if (is_null(self::$_instance)){ echo '正在实例化' . PHP_EOL; self::$_instance = new self(); } return self::$_instance; } /** * 查询数据 */ public function fetchUser() { $result = $this-&gt;pdo-&gt;query(&quot;SELECT id,name,email from users LIMIT 1&quot;); print_r($result-&gt;fetchAll(PDO::FETCH_ASSOC)); } } echo '第一次获取实例化对象' . PHP_EOL; $instance1 = Singleton::getInstance(); echo 'instance1实例化完毕' . PHP_EOL; echo '---------------------------------' . PHP_EOL; echo '第二次获取实例化对象' . PHP_EOL; $instance2 = Singleton::getInstance(); echo 'instance2实例化完毕，无需重新实例化，直接获取第一次实例化的对象即可' . PHP_EOL; $instance1-&gt;fetchUser(); 总结👇👇 这......就这？这有啥好总结的？这都不会吗？ 不会的话就从头再看一遍，google、百度一下，多看看别人的例子，自己写一下就OK了。 那单例模式就算复习完毕了，下一章我们就学习`工厂模式`，溜了溜了~ ","link":"http://mofish.pily.life/post/design_patterns_01/"},{"title":"个人声明","content":"👏 欢迎光临 ！ ✍️ 下面的所有文章都仅作个人学习记录用途。 ","link":"http://mofish.pily.life/post/hello-gridea/"}]}